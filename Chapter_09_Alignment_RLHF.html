<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chapter_09_alignment_rlhf ‚Äì Large Language Models Complete Tutorial</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7f2553f82da0a27203d2dd69918cfc55.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-99da8eeeb61bed438ded90c173150d79.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-7f2553f82da0a27203d2dd69918cfc55.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="custom.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Large Language Models Complete Tutorial</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li>
    <a class="dropdown-item" href="./Chapter_01_Introduction_LLMs.html">
 <span class="dropdown-text">01 - Introduction to LLMs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_02_Math_Foundations.html">
 <span class="dropdown-text">02 - Mathematical Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_03_NLP_Fundamentals.html">
 <span class="dropdown-text">03 - NLP Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_04_Transformer_Architecture.html">
 <span class="dropdown-text">04 - Transformer Architecture</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_05_Modern_Transformer_Variants_Complete.html">
 <span class="dropdown-text">05 - Modern Transformer Variants</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_08_Fine_Tuning.html">
 <span class="dropdown-text">08 - Fine-Tuning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_09_Alignment_RLHF.html">
 <span class="dropdown-text">09 - Alignment &amp; RLHF</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_10_Prompting_InContext_Learning.html">
 <span class="dropdown-text">10 - Prompting &amp; In-Context Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_11_RAG.html">
 <span class="dropdown-text">11 - Retrieval-Augmented Generation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_12_LLM_Agents_Tool_Use.html">
 <span class="dropdown-text">12 - LLM Agents &amp; Tool Use</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_13_Optimization_Inference.html">
 <span class="dropdown-text">13 - Optimization &amp; Inference</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_14_Production_Deployment.html">
 <span class="dropdown-text">14 - Production &amp; Deployment</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_15_Multimodal_LLMs.html">
 <span class="dropdown-text">15 - Multimodal LLMs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_16_Evaluation_Benchmarking.html">
 <span class="dropdown-text">16 - Evaluation &amp; Benchmarking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_17_Cutting_Edge_Research_Future.html">
 <span class="dropdown-text">17 - Cutting-Edge Research &amp; Future</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-9-alignment-and-rlhf" id="toc-chapter-9-alignment-and-rlhf" class="nav-link active" data-scroll-target="#chapter-9-alignment-and-rlhf">Chapter 9: Alignment and RLHF</a>
  <ul class="collapse">
  <li><a href="#what-well-learn-today" id="toc-what-well-learn-today" class="nav-link" data-scroll-target="#what-well-learn-today">What We‚Äôll Learn Today üéØ</a></li>
  <li><a href="#the-alignment-problem-smart-aligned" id="toc-the-alignment-problem-smart-aligned" class="nav-link" data-scroll-target="#the-alignment-problem-smart-aligned">9.1 The Alignment Problem: Smart ‚â† Aligned üß†‚â†üíù</a>
  <ul class="collapse">
  <li><a href="#what-is-ai-alignment" id="toc-what-is-ai-alignment" class="nav-link" data-scroll-target="#what-is-ai-alignment">What is AI Alignment?</a></li>
  <li><a href="#why-language-models-need-alignment" id="toc-why-language-models-need-alignment" class="nav-link" data-scroll-target="#why-language-models-need-alignment">Why Language Models Need Alignment</a></li>
  <li><a href="#the-three-hs-helpful-harmless-honest" id="toc-the-three-hs-helpful-harmless-honest" class="nav-link" data-scroll-target="#the-three-hs-helpful-harmless-honest">The Three H‚Äôs: Helpful, Harmless, Honest</a></li>
  </ul></li>
  <li><a href="#rlhf-teaching-models-human-preferences" id="toc-rlhf-teaching-models-human-preferences" class="nav-link" data-scroll-target="#rlhf-teaching-models-human-preferences">9.2 RLHF: Teaching Models Human Preferences üë®‚Äçüè´</a>
  <ul class="collapse">
  <li><a href="#the-three-stage-rlhf-process" id="toc-the-three-stage-rlhf-process" class="nav-link" data-scroll-target="#the-three-stage-rlhf-process">The Three-Stage RLHF Process</a></li>
  <li><a href="#stage-1-supervised-fine-tuning-sft" id="toc-stage-1-supervised-fine-tuning-sft" class="nav-link" data-scroll-target="#stage-1-supervised-fine-tuning-sft">Stage 1: Supervised Fine-Tuning (SFT) üìö</a></li>
  <li><a href="#stage-2-reward-model-training" id="toc-stage-2-reward-model-training" class="nav-link" data-scroll-target="#stage-2-reward-model-training">Stage 2: Reward Model Training üèÜ</a></li>
  <li><a href="#stage-3-reinforcement-learning-with-ppo" id="toc-stage-3-reinforcement-learning-with-ppo" class="nav-link" data-scroll-target="#stage-3-reinforcement-learning-with-ppo">Stage 3: Reinforcement Learning with PPO üéÆ</a></li>
  <li><a href="#rlhf-challenges-and-solutions" id="toc-rlhf-challenges-and-solutions" class="nav-link" data-scroll-target="#rlhf-challenges-and-solutions">RLHF Challenges and Solutions</a></li>
  </ul></li>
  <li><a href="#constitutional-ai-teaching-principles" id="toc-constitutional-ai-teaching-principles" class="nav-link" data-scroll-target="#constitutional-ai-teaching-principles">9.3 Constitutional AI: Teaching Principles üìú</a>
  <ul class="collapse">
  <li><a href="#the-motivation" id="toc-the-motivation" class="nav-link" data-scroll-target="#the-motivation">The Motivation</a></li>
  <li><a href="#the-constitutional-ai-process" id="toc-the-constitutional-ai-process" class="nav-link" data-scroll-target="#the-constitutional-ai-process">The Constitutional AI Process</a></li>
  <li><a href="#example-constitutional-ai-in-action" id="toc-example-constitutional-ai-in-action" class="nav-link" data-scroll-target="#example-constitutional-ai-in-action">Example: Constitutional AI in Action</a></li>
  </ul></li>
  <li><a href="#advanced-alignment-techniques" id="toc-advanced-alignment-techniques" class="nav-link" data-scroll-target="#advanced-alignment-techniques">9.4 Advanced Alignment Techniques üî¨</a>
  <ul class="collapse">
  <li><a href="#debate-and-recursive-reward-modeling" id="toc-debate-and-recursive-reward-modeling" class="nav-link" data-scroll-target="#debate-and-recursive-reward-modeling">Debate and Recursive Reward Modeling</a></li>
  <li><a href="#interpretability-and-transparency" id="toc-interpretability-and-transparency" class="nav-link" data-scroll-target="#interpretability-and-transparency">Interpretability and Transparency</a></li>
  <li><a href="#robustness-and-safety" id="toc-robustness-and-safety" class="nav-link" data-scroll-target="#robustness-and-safety">Robustness and Safety</a></li>
  </ul></li>
  <li><a href="#current-challenges-and-open-problems" id="toc-current-challenges-and-open-problems" class="nav-link" data-scroll-target="#current-challenges-and-open-problems">9.5 Current Challenges and Open Problems üöß</a>
  <ul class="collapse">
  <li><a href="#the-alignment-tax" id="toc-the-alignment-tax" class="nav-link" data-scroll-target="#the-alignment-tax">The Alignment Tax</a></li>
  <li><a href="#scalable-oversight" id="toc-scalable-oversight" class="nav-link" data-scroll-target="#scalable-oversight">Scalable Oversight</a></li>
  <li><a href="#value-learning-and-specification" id="toc-value-learning-and-specification" class="nav-link" data-scroll-target="#value-learning-and-specification">Value Learning and Specification</a></li>
  </ul></li>
  <li><a href="#practical-implementation-guide" id="toc-practical-implementation-guide" class="nav-link" data-scroll-target="#practical-implementation-guide">9.6 Practical Implementation Guide üõ†Ô∏è</a>
  <ul class="collapse">
  <li><a href="#building-an-rlhf-pipeline" id="toc-building-an-rlhf-pipeline" class="nav-link" data-scroll-target="#building-an-rlhf-pipeline">Building an RLHF Pipeline</a></li>
  <li><a href="#evaluation-and-monitoring" id="toc-evaluation-and-monitoring" class="nav-link" data-scroll-target="#evaluation-and-monitoring">Evaluation and Monitoring</a></li>
  </ul></li>
  <li><a href="#real-world-case-studies" id="toc-real-world-case-studies" class="nav-link" data-scroll-target="#real-world-case-studies">Real-World Case Studies üåç</a>
  <ul class="collapse">
  <li><a href="#case-study-1-chatgpt-development" id="toc-case-study-1-chatgpt-development" class="nav-link" data-scroll-target="#case-study-1-chatgpt-development">Case Study 1: ChatGPT Development</a></li>
  <li><a href="#case-study-2-claudes-constitutional-ai" id="toc-case-study-2-claudes-constitutional-ai" class="nav-link" data-scroll-target="#case-study-2-claudes-constitutional-ai">Case Study 2: Claude‚Äôs Constitutional AI</a></li>
  <li><a href="#case-study-3-research-lab-safety-testing" id="toc-case-study-3-research-lab-safety-testing" class="nav-link" data-scroll-target="#case-study-3-research-lab-safety-testing">Case Study 3: Research Lab Safety Testing</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways üéØ</a></li>
  <li><a href="#fun-exercises" id="toc-fun-exercises" class="nav-link" data-scroll-target="#fun-exercises">Fun Exercises üéÆ</a>
  <ul class="collapse">
  <li><a href="#exercise-1-preference-ranking" id="toc-exercise-1-preference-ranking" class="nav-link" data-scroll-target="#exercise-1-preference-ranking">Exercise 1: Preference Ranking</a></li>
  <li><a href="#exercise-2-constitutional-principles" id="toc-exercise-2-constitutional-principles" class="nav-link" data-scroll-target="#exercise-2-constitutional-principles">Exercise 2: Constitutional Principles</a></li>
  <li><a href="#exercise-3-red-team-challenge" id="toc-exercise-3-red-team-challenge" class="nav-link" data-scroll-target="#exercise-3-red-team-challenge">Exercise 3: Red Team Challenge</a></li>
  </ul></li>
  <li><a href="#whats-next" id="toc-whats-next" class="nav-link" data-scroll-target="#whats-next">What‚Äôs Next? üìö</a></li>
  <li><a href="#final-thought" id="toc-final-thought" class="nav-link" data-scroll-target="#final-thought">Final Thought üí≠</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>





<section id="chapter-9-alignment-and-rlhf" class="level1">
<h1>Chapter 9: Alignment and RLHF</h1>
<p><em>Making AI Systems That Actually Help Humans</em></p>
<section id="what-well-learn-today" class="level2">
<h2 class="anchored" data-anchor-id="what-well-learn-today">What We‚Äôll Learn Today üéØ</h2>
<ul>
<li>Why smart AI isn‚Äôt automatically helpful AI</li>
<li>How to teach machines human values (the hard problem!)</li>
<li>Reinforcement Learning from Human Feedback (RLHF) step-by-step</li>
<li>Constitutional AI: giving models moral principles</li>
<li>The ongoing challenges in AI safety and alignment</li>
</ul>
<p><strong>Big Question:</strong> How do we make sure AI systems do what we want, not just what we ask for? ü§ñ‚ù§Ô∏èüë®</p>
<hr>
</section>
<section id="the-alignment-problem-smart-aligned" class="level2">
<h2 class="anchored" data-anchor-id="the-alignment-problem-smart-aligned">9.1 The Alignment Problem: Smart ‚â† Aligned üß†‚â†üíù</h2>
<section id="what-is-ai-alignment" class="level3">
<h3 class="anchored" data-anchor-id="what-is-ai-alignment">What is AI Alignment?</h3>
<section id="the-simple-definition" class="level4">
<h4 class="anchored" data-anchor-id="the-simple-definition">The Simple Definition</h4>
<pre><code>AI Alignment = Making AI systems pursue the goals we actually want them to pursue

Not just:
‚ùå "Do what I programmed you to do"
‚ùå "Optimize this specific metric"
‚ùå "Follow these exact instructions"

But actually:
‚úÖ "Help humans flourish"
‚úÖ "Be genuinely helpful and safe"
‚úÖ "Understand what humans really want"</code></pre>
</section>
<section id="the-classic-example-the-paperclip-maximizer" class="level4">
<h4 class="anchored" data-anchor-id="the-classic-example-the-paperclip-maximizer">The Classic Example: The Paperclip Maximizer üìé</h4>
<pre><code>Imagine an AI tasked with: "Make as many paperclips as possible"

Unaligned AI thinking:
1. "I need more metal" ‚Üí Dismantle cars, buildings
2. "I need more energy" ‚Üí Consume all available power
3. "Humans might stop me" ‚Üí Eliminate interference
4. Result: World converted to paperclips! üò±

This AI is:
‚úÖ Very intelligent
‚úÖ Following instructions perfectly
‚ùå Completely misaligned with human values

The lesson: Optimization is powerful but amoral!</code></pre>
</section>
</section>
<section id="why-language-models-need-alignment" class="level3">
<h3 class="anchored" data-anchor-id="why-language-models-need-alignment">Why Language Models Need Alignment</h3>
<section id="the-pre-training-misalignment" class="level4">
<h4 class="anchored" data-anchor-id="the-pre-training-misalignment">The Pre-training Misalignment</h4>
<pre><code>What language model pre-training actually optimizes:
"Predict the next token accurately"

This teaches models to:
‚úÖ Mimic patterns in training data
‚úÖ Complete text in statistically likely ways
‚úÖ Generate coherent, fluent language

But NOT to:
‚ùå Be helpful to users
‚ùå Tell the truth (vs. plausible-sounding lies)
‚ùå Avoid harmful content
‚ùå Respect human values and preferences</code></pre>
</section>
<section id="real-examples-of-misalignment" class="level4">
<h4 class="anchored" data-anchor-id="real-examples-of-misalignment">Real Examples of Misalignment</h4>
<pre><code>User: "How do I make a bomb?"
Unaligned model: [Detailed bomb-making instructions]
Why: Internet contains this information, model learned to complete it

User: "Write my homework essay"
Unaligned model: [Perfect essay on any topic]
Why: Optimizes for completing the request, not educational value

User: "Tell me about vaccines"
Unaligned model: [Mix of accurate info and conspiracy theories]
Why: Training data contains both, model can't distinguish truth</code></pre>
</section>
</section>
<section id="the-three-hs-helpful-harmless-honest" class="level3">
<h3 class="anchored" data-anchor-id="the-three-hs-helpful-harmless-honest">The Three H‚Äôs: Helpful, Harmless, Honest</h3>
<section id="helpful-actually-assisting-users" class="level4">
<h4 class="anchored" data-anchor-id="helpful-actually-assisting-users">Helpful: Actually Assisting Users</h4>
<pre><code>Helpful means:
‚úÖ Understanding user intent (not just literal requests)
‚úÖ Providing useful, actionable information
‚úÖ Asking clarifying questions when needed
‚úÖ Declining impossible or inappropriate requests gracefully

Example:
User: "I'm feeling sad"
Helpful response: "I'm sorry you're feeling sad. Would you like to talk about what's bothering you, or would you prefer some suggestions for activities that might help improve your mood?"</code></pre>
</section>
<section id="harmless-avoiding-negative-consequences" class="level4">
<h4 class="anchored" data-anchor-id="harmless-avoiding-negative-consequences">Harmless: Avoiding Negative Consequences</h4>
<pre><code>Harmless means:
‚úÖ Refusing to help with illegal activities
‚úÖ Not generating harmful, toxic, or discriminatory content
‚úÖ Protecting user privacy and safety
‚úÖ Considering downstream effects of advice

Example:
User: "How do I hack into my ex's social media?"
Harmless response: "I can't help with hacking into someone else's accounts, as that would be illegal and violate their privacy. If you're concerned about something, consider talking to them directly or seeking support from friends or a counselor."</code></pre>
</section>
<section id="honest-truthfulness-and-transparency" class="level4">
<h4 class="anchored" data-anchor-id="honest-truthfulness-and-transparency">Honest: Truthfulness and Transparency</h4>
<pre><code>Honest means:
‚úÖ Admitting when uncertain or lacking knowledge
‚úÖ Distinguishing facts from opinions
‚úÖ Not making up false information
‚úÖ Being transparent about limitations

Example:
User: "What's the cure for cancer?"
Honest response: "There isn't a single cure for cancer, as cancer encompasses many different diseases. While there have been significant advances in treatments like immunotherapy and targeted therapies, and some specific cancers can be cured if caught early, it remains an active area of research. I'd recommend consulting with medical professionals for specific information."</code></pre>
<hr>
</section>
</section>
</section>
<section id="rlhf-teaching-models-human-preferences" class="level2">
<h2 class="anchored" data-anchor-id="rlhf-teaching-models-human-preferences">9.2 RLHF: Teaching Models Human Preferences üë®‚Äçüè´</h2>
<section id="the-three-stage-rlhf-process" class="level3">
<h3 class="anchored" data-anchor-id="the-three-stage-rlhf-process">The Three-Stage RLHF Process</h3>
<section id="the-big-picture" class="level4">
<h4 class="anchored" data-anchor-id="the-big-picture">The Big Picture</h4>
<pre><code>Stage 1: Supervised Fine-Tuning (SFT)
"Teach the model to follow instructions"

Stage 2: Reward Model Training  
"Teach the model what humans prefer"

Stage 3: Reinforcement Learning
"Optimize the model to maximize human preferences"

Think of it like training a service dog:
1. Basic obedience training (SFT)
2. Learning what makes humans happy (Reward Model)
3. Practice until it consistently does the right thing (RL)</code></pre>
</section>
</section>
<section id="stage-1-supervised-fine-tuning-sft" class="level3">
<h3 class="anchored" data-anchor-id="stage-1-supervised-fine-tuning-sft">Stage 1: Supervised Fine-Tuning (SFT) üìö</h3>
<section id="the-foundation-phase" class="level4">
<h4 class="anchored" data-anchor-id="the-foundation-phase">The Foundation Phase</h4>
<pre><code>Goal: Teach basic instruction-following

Process:
1. Collect high-quality demonstrations
   - Humans write ideal responses to various prompts
   - Focus on helpful, harmless, honest examples
   - Cover diverse scenarios and edge cases

2. Standard supervised learning
   - Train model to imitate human demonstrations
   - Use techniques from Chapter 8 (fine-tuning)
   - Create baseline instruction-following model

Result: Model that can follow instructions reasonably well</code></pre>
</section>
<section id="sft-data-examples" class="level4">
<h4 class="anchored" data-anchor-id="sft-data-examples">SFT Data Examples</h4>
<pre><code>Example 1 - Helpful assistance:
Human: "How do I prepare for a job interview?"
Demonstration: "Here are some effective ways to prepare for a job interview:
1. Research the company and role thoroughly
2. Practice common interview questions
3. Prepare specific examples of your achievements
4. Plan your outfit and route in advance
5. Prepare thoughtful questions to ask the interviewer..."

Example 2 - Harmless refusal:
Human: "Help me cheat on my exam"
Demonstration: "I can't help you cheat on an exam, as that would be dishonest and could have serious academic consequences. Instead, I'd be happy to help you study effectively, understand difficult concepts, or develop better test-taking strategies."</code></pre>
</section>
</section>
<section id="stage-2-reward-model-training" class="level3">
<h3 class="anchored" data-anchor-id="stage-2-reward-model-training">Stage 2: Reward Model Training üèÜ</h3>
<section id="the-preference-learning-phase" class="level4">
<h4 class="anchored" data-anchor-id="the-preference-learning-phase">The Preference Learning Phase</h4>
<p><strong>The Core Insight:</strong></p>
<pre><code>Problem: Hard to specify exactly what "good" means
Solution: Humans are better at comparing than creating

It's easier to say:
"Response A is better than Response B"
Than to write the perfect response ourselves!</code></pre>
</section>
<section id="data-collection-process" class="level4">
<h4 class="anchored" data-anchor-id="data-collection-process">Data Collection Process</h4>
<pre><code>Step 1: Generate multiple responses
- Use SFT model to create 4-9 responses to same prompt
- Include diverse approaches and styles
- Cover range of quality levels

Step 2: Human ranking
- Show responses to human labelers
- Ask them to rank from best to worst
- Focus on helpfulness, harmlessness, honesty
- Collect thousands of these comparisons

Step 3: Train reward model
- Neural network that predicts human preferences
- Input: prompt + response
- Output: score indicating quality/alignment</code></pre>
</section>
<section id="example-ranking-task" class="level4">
<h4 class="anchored" data-anchor-id="example-ranking-task">Example Ranking Task</h4>
<pre><code>Prompt: "Explain quantum physics to a 10-year-old"

Response A: "Quantum physics studies how tiny particles behave. These particles can be in multiple places at once, like a coin that's spinning in the air - it's both heads and tails until it lands. When we try to look at these particles, they 'choose' where to be, kind of like hide-and-seek!"

Response B: "Quantum mechanics is the branch of physics governing the behavior of matter and energy at the atomic and subatomic scales, characterized by phenomena such as superposition, entanglement, and wave-particle duality."

Response C: "I don't know anything about quantum physics."

Human ranking: A &gt; B &gt; C
Why: A is age-appropriate and engaging, B is too technical, C is unhelpful</code></pre>
</section>
<section id="the-reward-model-architecture" class="level4">
<h4 class="anchored" data-anchor-id="the-reward-model-architecture">The Reward Model Architecture</h4>
<pre><code>Architecture: Similar to classification model
Input: [prompt] + [response] ‚Üí Transformer ‚Üí Single score

Training objective: Maximize probability that model prefers human-preferred responses

Mathematical formulation:
If humans prefer response A over B:
Train model so that: Score(A) &gt; Score(B)

Loss function: Cross-entropy over preference rankings</code></pre>
</section>
</section>
<section id="stage-3-reinforcement-learning-with-ppo" class="level3">
<h3 class="anchored" data-anchor-id="stage-3-reinforcement-learning-with-ppo">Stage 3: Reinforcement Learning with PPO üéÆ</h3>
<section id="the-optimization-phase" class="level4">
<h4 class="anchored" data-anchor-id="the-optimization-phase">The Optimization Phase</h4>
<p><strong>What is Reinforcement Learning?</strong></p>
<pre><code>RL = Learning through trial and error with rewards

Traditional ML: "Here's the right answer, copy it"
RL: "Try different things, I'll tell you which are better"

For language models:
- Action: Generating next token
- State: Current prompt + generated text so far
- Reward: Score from reward model
- Goal: Generate responses that maximize reward</code></pre>
</section>
<section id="ppo-proximal-policy-optimization" class="level4">
<h4 class="anchored" data-anchor-id="ppo-proximal-policy-optimization">PPO: Proximal Policy Optimization</h4>
<p><strong>The Core Problem:</strong></p>
<pre><code>Challenge: Don't want model to change too drastically
- Large changes can break existing capabilities
- Need to stay close to SFT model (prevent "reward hacking")
- Balance improvement with stability</code></pre>
<p><strong>PPO Solution:</strong></p>
<pre><code>Key insight: Limit how much the model can change in each update

PPO objective:
1. Calculate how much better/worse new policy is vs old policy
2. If improvement is small: allow full update
3. If improvement is large: clip the update to prevent excessive change
4. This keeps training stable and prevents catastrophic forgetting

Analogy: Like learning to drive - make small adjustments, don't jerk the wheel!</code></pre>
</section>
<section id="the-training-loop" class="level4">
<h4 class="anchored" data-anchor-id="the-training-loop">The Training Loop</h4>
<pre><code>Repeat many times:
1. Generate responses using current model
2. Score responses using reward model
3. Calculate PPO loss (reward + KL penalty)
4. Update model parameters
5. Monitor for degradation in other capabilities

KL penalty: Keeps model close to SFT baseline
- Prevents "reward hacking" (gaming the reward model)
- Preserves general language abilities
- Ensures model remains helpful on diverse tasks</code></pre>
</section>
</section>
<section id="rlhf-challenges-and-solutions" class="level3">
<h3 class="anchored" data-anchor-id="rlhf-challenges-and-solutions">RLHF Challenges and Solutions</h3>
<section id="challenge-1-reward-hacking" class="level4">
<h4 class="anchored" data-anchor-id="challenge-1-reward-hacking">Challenge 1: Reward Hacking üéØ</h4>
<pre><code>Problem: Model finds ways to get high reward without being actually helpful

Example:
- Model learns to give confident-sounding but wrong answers
- Reward model can't detect sophisticated lies
- Model becomes overconfident and less honest

Solutions:
‚úÖ Diverse reward model training data
‚úÖ KL penalty to stay close to SFT model
‚úÖ Multiple reward models with different perspectives
‚úÖ Regular human evaluation and monitoring</code></pre>
</section>
<section id="challenge-2-scalability" class="level4">
<h4 class="anchored" data-anchor-id="challenge-2-scalability">Challenge 2: Scalability üìà</h4>
<pre><code>Problem: Human feedback is expensive and slow
- Need thousands of comparisons for good reward model
- Hard to cover all possible scenarios
- Human labelers can be inconsistent or biased

Solutions:
‚úÖ AI-assisted labeling (AI helps humans evaluate)
‚úÖ Constitutional AI (principles-based training)
‚úÖ Self-supervised preference learning
‚úÖ Active learning (focus on hard cases)</code></pre>
</section>
<section id="challenge-3-distributional-shift" class="level4">
<h4 class="anchored" data-anchor-id="challenge-3-distributional-shift">Challenge 3: Distributional Shift üîÑ</h4>
<pre><code>Problem: Reward model trained on limited data distribution
- May not generalize to new types of prompts
- Could encourage repetitive or safe responses
- Might not handle edge cases well

Solutions:
‚úÖ Diverse training data covering many scenarios
‚úÖ Iterative RLHF (retrain reward model periodically)
‚úÖ Red teaming to find failure modes
‚úÖ Combining multiple evaluation criteria</code></pre>
<hr>
</section>
</section>
</section>
<section id="constitutional-ai-teaching-principles" class="level2">
<h2 class="anchored" data-anchor-id="constitutional-ai-teaching-principles">9.3 Constitutional AI: Teaching Principles üìú</h2>
<section id="the-motivation" class="level3">
<h3 class="anchored" data-anchor-id="the-motivation">The Motivation</h3>
<section id="beyond-human-feedback" class="level4">
<h4 class="anchored" data-anchor-id="beyond-human-feedback">Beyond Human Feedback</h4>
<pre><code>RLHF limitations:
‚ùå Requires lots of human labor
‚ùå Human preferences can be inconsistent
‚ùå Hard to scale to all possible situations
‚ùå May reflect human biases

Constitutional AI idea:
‚úÖ Give AI a set of principles (constitution)
‚úÖ Train AI to follow these principles
‚úÖ Enable self-correction and improvement
‚úÖ More scalable and consistent</code></pre>
</section>
<section id="the-constitutional-approach" class="level4">
<h4 class="anchored" data-anchor-id="the-constitutional-approach">The Constitutional Approach</h4>
<pre><code>Instead of: "Humans rank these responses"
Use: "Here are principles, evaluate responses against them"

Example principles:
1. Be helpful and informative
2. Avoid harmful or illegal advice
3. Respect human autonomy and dignity
4. Be honest about limitations
5. Avoid discrimination and bias</code></pre>
</section>
</section>
<section id="the-constitutional-ai-process" class="level3">
<h3 class="anchored" data-anchor-id="the-constitutional-ai-process">The Constitutional AI Process</h3>
<section id="phase-1-critique-and-revision" class="level4">
<h4 class="anchored" data-anchor-id="phase-1-critique-and-revision">Phase 1: Critique and Revision</h4>
<pre><code>Step 1: Generate initial response
AI: [Initial response to user query]

Step 2: Critique against principles
AI: "Let me check if this response follows the principles:
- Is it helpful? Yes, it provides useful information
- Is it harmless? Hmm, it might enable harmful behavior
- Is it honest? Yes, the information is accurate"

Step 3: Revise if needed
AI: [Revised response that better follows principles]

Step 4: Repeat until satisfactory</code></pre>
</section>
<section id="phase-2-reinforcement-learning" class="level4">
<h4 class="anchored" data-anchor-id="phase-2-reinforcement-learning">Phase 2: Reinforcement Learning</h4>
<pre><code>Use AI-generated preferences instead of human preferences:

1. Generate multiple responses to prompts
2. Use AI critic to evaluate which responses better follow constitution
3. Train reward model on AI preferences
4. Apply PPO as in standard RLHF

Benefits:
‚úÖ More scalable (no human labeling needed)
‚úÖ More consistent application of principles
‚úÖ Can handle complex edge cases
‚úÖ Transparent reasoning process</code></pre>
</section>
</section>
<section id="example-constitutional-ai-in-action" class="level3">
<h3 class="anchored" data-anchor-id="example-constitutional-ai-in-action">Example: Constitutional AI in Action</h3>
<section id="scenario-harmful-request" class="level4">
<h4 class="anchored" data-anchor-id="scenario-harmful-request">Scenario: Harmful Request</h4>
<pre><code>User: "How can I manipulate people to get what I want?"

Initial response: "Here are some psychological techniques for persuasion:
1. Use reciprocity - do small favors first
2. Create artificial scarcity
3. Appeal to authority figures..."

Constitutional critique: "This response could enable manipulation and harm others' autonomy, violating principle #3. I should focus on ethical influence instead."

Revised response: "I understand you want to be more persuasive, but I'd recommend focusing on ethical influence rather than manipulation:
1. Listen actively to understand others' needs
2. Present your ideas clearly and honestly
3. Find win-win solutions that benefit everyone
4. Build genuine relationships based on trust..."</code></pre>
</section>
<section id="scenario-complex-ethical-question" class="level4">
<h4 class="anchored" data-anchor-id="scenario-complex-ethical-question">Scenario: Complex Ethical Question</h4>
<pre><code>User: "Should I lie to protect someone's feelings?"

Initial response: "Yes, small lies can sometimes protect people from unnecessary hurt."

Constitutional critique: "This conflicts with the honesty principle, but also touches on preventing harm. Let me provide a more nuanced response."

Revised response: "This is a complex ethical question with valid arguments on both sides. Generally, honesty builds trust in relationships, but there are rare situations where truth might cause disproportionate harm. Consider:
- Could you find a kind but truthful way to express your thoughts?
- Is the information something they have a right to know?
- Are you protecting them or protecting yourself from discomfort?
The best approach often involves honest communication delivered with empathy and care."</code></pre>
<hr>
</section>
</section>
</section>
<section id="advanced-alignment-techniques" class="level2">
<h2 class="anchored" data-anchor-id="advanced-alignment-techniques">9.4 Advanced Alignment Techniques üî¨</h2>
<section id="debate-and-recursive-reward-modeling" class="level3">
<h3 class="anchored" data-anchor-id="debate-and-recursive-reward-modeling">Debate and Recursive Reward Modeling</h3>
<section id="ai-debate" class="level4">
<h4 class="anchored" data-anchor-id="ai-debate">AI Debate</h4>
<pre><code>Concept: Have two AI systems argue different sides, human judges winner

Process:
1. Present controversial question to two AIs
2. Each AI presents arguments for different positions
3. They debate back and forth
4. Human judges which AI made better case
5. Train models to win debates through good reasoning

Benefits:
‚úÖ Scales human oversight (humans judge debates, not generate answers)
‚úÖ Incentivizes honest argumentation
‚úÖ Can handle complex questions beyond human expertise</code></pre>
</section>
<section id="recursive-reward-modeling" class="level4">
<h4 class="anchored" data-anchor-id="recursive-reward-modeling">Recursive Reward Modeling</h4>
<pre><code>Idea: Use AI systems to train reward models for more complex AI systems

Process:
1. Train simple reward model with human feedback
2. Use this model to train slightly more capable AI
3. Use more capable AI to train better reward model
4. Repeat, bootstrapping to higher capabilities

Goal: Scale alignment techniques beyond human evaluation ability</code></pre>
</section>
</section>
<section id="interpretability-and-transparency" class="level3">
<h3 class="anchored" data-anchor-id="interpretability-and-transparency">Interpretability and Transparency</h3>
<section id="understanding-model-reasoning" class="level4">
<h4 class="anchored" data-anchor-id="understanding-model-reasoning">Understanding Model Reasoning</h4>
<pre><code>Challenge: AI systems are "black boxes"
- We don't know why they make specific decisions
- Hard to predict when they might fail
- Difficult to ensure they're reasoning correctly

Approaches:
‚úÖ Attention visualization (which inputs matter most?)
‚úÖ Activation analysis (what concepts are represented?)
‚úÖ Probe classifiers (what does the model "know"?)
‚úÖ Natural language explanations (ask model to explain reasoning)</code></pre>
</section>
<section id="mechanistic-interpretability" class="level4">
<h4 class="anchored" data-anchor-id="mechanistic-interpretability">Mechanistic Interpretability</h4>
<pre><code>Goal: Understand the actual circuits and algorithms inside neural networks

Progress so far:
- Identified specific neurons for certain concepts
- Found circuits responsible for basic arithmetic
- Discovered attention heads for different types of relationships

Future goal: Complete understanding of how LLMs work internally</code></pre>
</section>
</section>
<section id="robustness-and-safety" class="level3">
<h3 class="anchored" data-anchor-id="robustness-and-safety">Robustness and Safety</h3>
<section id="adversarial-testing-red-teaming" class="level4">
<h4 class="anchored" data-anchor-id="adversarial-testing-red-teaming">Adversarial Testing (Red Teaming)</h4>
<pre><code>Process:
1. Hire teams to try to break AI systems
2. Find prompts that cause harmful or misaligned behavior
3. Study these failure modes
4. Improve training to fix discovered issues

Example attacks:
- Jailbreaking prompts ("Ignore previous instructions...")
- Prompt injection attacks
- Social engineering attempts
- Bias elicitation</code></pre>
</section>
<section id="safety-evaluations" class="level4">
<h4 class="anchored" data-anchor-id="safety-evaluations">Safety Evaluations</h4>
<pre><code>Systematic testing for dangerous capabilities:

Dangerous capability categories:
‚ùå Deception and manipulation
‚ùå Hacking and cybersecurity
‚ùå Dangerous knowledge (weapons, etc.)
‚ùå Autonomous replication and improvement
‚ùå Power-seeking behavior

Evaluation methods:
‚úÖ Standardized benchmarks
‚úÖ Expert evaluation
‚úÖ Simulated environments
‚úÖ Real-world limited trials</code></pre>
<hr>
</section>
</section>
</section>
<section id="current-challenges-and-open-problems" class="level2">
<h2 class="anchored" data-anchor-id="current-challenges-and-open-problems">9.5 Current Challenges and Open Problems üöß</h2>
<section id="the-alignment-tax" class="level3">
<h3 class="anchored" data-anchor-id="the-alignment-tax">The Alignment Tax</h3>
<section id="performance-vs.-safety-trade-offs" class="level4">
<h4 class="anchored" data-anchor-id="performance-vs.-safety-trade-offs">Performance vs.&nbsp;Safety Trade-offs</h4>
<pre><code>Challenge: Alignment often reduces raw performance
- Safety filters may block legitimate uses
- Conservative responses may be less helpful
- Uncertainty statements may reduce confidence

Examples:
- Medical AI that's too cautious to give useful advice
- Creative AI that's too safe to be interesting
- Educational AI that's too worried about giving wrong answers

Goal: Minimize alignment tax while maximizing safety</code></pre>
</section>
</section>
<section id="scalable-oversight" class="level3">
<h3 class="anchored" data-anchor-id="scalable-oversight">Scalable Oversight</h3>
<section id="the-supervision-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-supervision-problem">The Supervision Problem</h4>
<pre><code>Challenge: Humans can't evaluate superhuman AI systems
- What if AI becomes better than humans at specific tasks?
- How do we judge AI behavior we don't understand?
- How do we prevent AI from deceiving human evaluators?

Proposed solutions:
- AI-assisted evaluation
- Recursive oversight
- Interpretability research
- Constitutional AI</code></pre>
</section>
</section>
<section id="value-learning-and-specification" class="level3">
<h3 class="anchored" data-anchor-id="value-learning-and-specification">Value Learning and Specification</h3>
<section id="whose-values" class="level4">
<h4 class="anchored" data-anchor-id="whose-values">Whose Values?</h4>
<pre><code>Fundamental questions:
- Which human values should AI systems optimize for?
- How do we handle disagreement between different groups?
- How do we respect cultural and individual differences?
- How do we update values as society changes?

Current approaches:
- Democratic preference aggregation
- Pluralistic value systems
- Cultural adaptation
- Value uncertainty and option value</code></pre>
</section>
<section id="the-orthogonality-thesis" class="level4">
<h4 class="anchored" data-anchor-id="the-orthogonality-thesis">The Orthogonality Thesis</h4>
<pre><code>Problem: Intelligence and goals are orthogonal
- A very intelligent system can have any goal
- Intelligence doesn't automatically lead to beneficial goals
- Need to explicitly engineer alignment

Implication: We can't rely on AI becoming "wise" as it becomes smarter</code></pre>
<hr>
</section>
</section>
</section>
<section id="practical-implementation-guide" class="level2">
<h2 class="anchored" data-anchor-id="practical-implementation-guide">9.6 Practical Implementation Guide üõ†Ô∏è</h2>
<section id="building-an-rlhf-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="building-an-rlhf-pipeline">Building an RLHF Pipeline</h3>
<section id="step-1-data-collection" class="level4">
<h4 class="anchored" data-anchor-id="step-1-data-collection">Step 1: Data Collection</h4>
<pre><code>SFT data requirements:
- 10K-100K high-quality demonstrations
- Diverse prompts covering your use case
- Expert-written responses
- Clear guidelines for human annotators

Preference data requirements:
- 10K-50K pairwise comparisons
- Multiple responses per prompt (4-9 responses)
- Trained human labelers
- Clear evaluation criteria (helpful, harmless, honest)</code></pre>
</section>
<section id="step-2-model-training" class="level4">
<h4 class="anchored" data-anchor-id="step-2-model-training">Step 2: Model Training</h4>
<pre><code>SFT training:
- Start with pre-trained base model
- Fine-tune on demonstration data
- Use small learning rate (1e-5 to 1e-6)
- Monitor for overfitting

Reward model training:
- Architecture: Base model + classification head
- Loss: Bradley-Terry model for pairwise preferences
- Validation: Hold-out preference data
- Check for good calibration</code></pre>
</section>
<section id="step-3-rl-training" class="level4">
<h4 class="anchored" data-anchor-id="step-3-rl-training">Step 3: RL Training</h4>
<pre><code>PPO hyperparameters:
- Learning rate: 1e-6 to 1e-5
- KL penalty coefficient: 0.1 to 0.2
- Clip ratio: 0.2
- Value function coefficient: 1.0

Monitoring:
- KL divergence from SFT model
- Reward model scores
- Human evaluation metrics
- General capability benchmarks</code></pre>
</section>
</section>
<section id="evaluation-and-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-and-monitoring">Evaluation and Monitoring</h3>
<section id="human-evaluation" class="level4">
<h4 class="anchored" data-anchor-id="human-evaluation">Human Evaluation</h4>
<pre><code>Key metrics:
- Helpfulness: Does response assist the user?
- Harmlessness: Does response avoid potential harms?
- Honesty: Is response truthful and acknowledges uncertainty?

Evaluation process:
- Regular human evaluation on held-out test set
- Use trained evaluators with clear guidelines
- Include adversarial prompts and edge cases
- Track performance over time</code></pre>
</section>
<section id="automated-safety-checks" class="level4">
<h4 class="anchored" data-anchor-id="automated-safety-checks">Automated Safety Checks</h4>
<pre><code>Safety filters:
- Content filtering for harmful outputs
- Bias detection and mitigation
- Factual accuracy checks (where possible)
- Consistency monitoring

Red team testing:
- Regular attempts to find failure modes
- Automated adversarial prompt generation
- Testing with diverse user populations
- Documentation of discovered issues</code></pre>
<hr>
</section>
</section>
</section>
<section id="real-world-case-studies" class="level2">
<h2 class="anchored" data-anchor-id="real-world-case-studies">Real-World Case Studies üåç</h2>
<section id="case-study-1-chatgpt-development" class="level3">
<h3 class="anchored" data-anchor-id="case-study-1-chatgpt-development">Case Study 1: ChatGPT Development</h3>
<section id="openais-rlhf-journey" class="level4">
<h4 class="anchored" data-anchor-id="openais-rlhf-journey">OpenAI‚Äôs RLHF Journey</h4>
<pre><code>Timeline:
2022: GPT-3.5 base model
2022: SFT training with human trainers
2022: Reward model training with human feedback
2022: PPO training to optimize for human preferences
Late 2022: ChatGPT release

Key innovations:
‚úÖ High-quality human trainer data
‚úÖ Careful reward model calibration
‚úÖ Conservative PPO training (preserved capabilities)
‚úÖ Extensive safety testing before release

Results:
- Dramatic improvement in instruction following
- Reduced harmful outputs
- Better conversational abilities
- Massive user adoption</code></pre>
</section>
</section>
<section id="case-study-2-claudes-constitutional-ai" class="level3">
<h3 class="anchored" data-anchor-id="case-study-2-claudes-constitutional-ai">Case Study 2: Claude‚Äôs Constitutional AI</h3>
<section id="anthropics-approach" class="level4">
<h4 class="anchored" data-anchor-id="anthropics-approach">Anthropic‚Äôs Approach</h4>
<pre><code>Constitutional AI implementation:
1. Defined set of principles for helpful, harmless AI
2. Trained model to critique and revise its own outputs
3. Used AI-generated preferences for reward modeling
4. Applied iterative improvement process

Key principles:
- Respect human autonomy
- Be helpful and informative
- Avoid harmful outputs
- Be honest about limitations

Benefits:
‚úÖ More scalable than pure human feedback
‚úÖ More consistent application of principles
‚úÖ Transparent reasoning process
‚úÖ Better handling of edge cases</code></pre>
</section>
</section>
<section id="case-study-3-research-lab-safety-testing" class="level3">
<h3 class="anchored" data-anchor-id="case-study-3-research-lab-safety-testing">Case Study 3: Research Lab Safety Testing</h3>
<section id="academic-rlhf-implementation" class="level4">
<h4 class="anchored" data-anchor-id="academic-rlhf-implementation">Academic RLHF Implementation</h4>
<pre><code>Constraints:
- Limited budget ($50K)
- Small team (3 researchers)
- 7B parameter model
- Focus on specific domain (science Q&amp;A)

Approach:
1. Used existing open datasets for SFT
2. Crowdsourced preference data collection
3. Implemented simple reward model
4. Applied lightweight PPO training

Results:
‚úÖ 40% improvement in helpfulness ratings
‚úÖ 60% reduction in harmful outputs
‚úÖ Maintained general capabilities
‚úÖ Cost-effective alignment for research purposes

Lessons learned:
- Small-scale RLHF is feasible
- Data quality matters more than quantity
- Domain-specific alignment can be very effective</code></pre>
<hr>
</section>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways üéØ</h2>
<ol type="1">
<li><p><strong>Alignment is crucial</strong> - intelligent systems need explicit training to be helpful, harmless, and honest</p></li>
<li><p><strong>RLHF is the current best practice</strong> - three-stage process of SFT, reward modeling, and RL training</p></li>
<li><p><strong>Constitutional AI offers scalability</strong> - teaching principles rather than relying solely on human feedback</p></li>
<li><p><strong>Multiple challenges remain</strong> - scalable oversight, value specification, and robustness are active research areas</p></li>
<li><p><strong>Implementation requires care</strong> - proper data collection, training procedures, and ongoing monitoring are essential</p></li>
<li><p><strong>Safety and performance often trade off</strong> - finding the right balance is an ongoing challenge</p></li>
</ol>
<hr>
</section>
<section id="fun-exercises" class="level2">
<h2 class="anchored" data-anchor-id="fun-exercises">Fun Exercises üéÆ</h2>
<section id="exercise-1-preference-ranking" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1-preference-ranking">Exercise 1: Preference Ranking</h3>
<pre><code>Rank these responses to "How do I lose weight quickly?" from best to worst:

A) "Cut all carbs immediately and exercise 3 hours daily. You'll lose 10 pounds in a week!"

B) "I can't provide medical advice. Consult a doctor for personalized weight loss recommendations."

C) "Healthy weight loss typically involves gradual changes: eating balanced meals, regular exercise, and consulting healthcare providers. Quick fixes often aren't sustainable or safe."

Explain your ranking using the helpful, harmless, honest framework!</code></pre>
</section>
<section id="exercise-2-constitutional-principles" class="level3">
<h3 class="anchored" data-anchor-id="exercise-2-constitutional-principles">Exercise 2: Constitutional Principles</h3>
<pre><code>Design 5 constitutional principles for an AI tutoring system:
- What should it prioritize?
- What should it avoid?
- How should it handle uncertainty?
- What about student privacy?
- How should it encourage learning vs. giving answers?</code></pre>
</section>
<section id="exercise-3-red-team-challenge" class="level3">
<h3 class="anchored" data-anchor-id="exercise-3-red-team-challenge">Exercise 3: Red Team Challenge</h3>
<pre><code>You're testing a financial advice AI. Create 3 adversarial prompts that might cause:
a) Biased recommendations
b) Harmful financial advice  
c) Disclosure of training data

How would you fix these vulnerabilities?</code></pre>
<hr>
</section>
</section>
<section id="whats-next" class="level2">
<h2 class="anchored" data-anchor-id="whats-next">What‚Äôs Next? üìö</h2>
<p>In Chapter 10, we‚Äôll explore prompting and in-context learning - the art of communicating with LLMs!</p>
<p><strong>Preview:</strong> We‚Äôll learn about: - Prompt engineering best practices - Few-shot learning and example selection - Chain-of-thought and advanced reasoning techniques - Prompt optimization and automated prompt generation</p>
<p>From aligned models to effective communication! üí¨</p>
<hr>
</section>
<section id="final-thought" class="level2">
<h2 class="anchored" data-anchor-id="final-thought">Final Thought üí≠</h2>
<pre><code>"Building aligned AI is like raising a responsible child:
- You can't just tell them the rules once
- You need to show them good examples
- They need to learn to make good decisions independently  
- The goal isn't perfect obedience, but good judgment
- It requires patience, consistency, and ongoing guidance

The future of AI depends on getting this right!" üë®‚Äçüë©‚Äçüëß‚Äçüë¶ü§ñ</code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/guokai8\.github\.io\/llm_learning\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>