<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chapter_02_math_foundations – Large Language Models 教程</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a8976c3e89df70b272bdfba3d2fda974.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Large Language Models 教程</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">首页</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">章节</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-">    
        <li>
    <a class="dropdown-item" href="./Chapter_01_Introduction_LLMs.html">
 <span class="dropdown-text">01 - LLM 介绍</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_02_Math_Foundations.html">
 <span class="dropdown-text">02 - 数学基础</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_03_NLP_Fundamentals.html">
 <span class="dropdown-text">03 - NLP 基础</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_04_Transformer_Architecture.html">
 <span class="dropdown-text">04 - Transformer 架构</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_05_Modern_Transformer_Variants_Complete.html">
 <span class="dropdown-text">05 - 现代 Transformer</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_08_Fine_Tuning.html">
 <span class="dropdown-text">08 - 微调</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_09_Alignment_RLHF.html">
 <span class="dropdown-text">09 - 对齐与 RLHF</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_10_Prompting_InContext_Learning.html">
 <span class="dropdown-text">10 - Prompt 工程</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_11_RAG.html">
 <span class="dropdown-text">11 - RAG</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_12_LLM_Agents_Tool_Use.html">
 <span class="dropdown-text">12 - LLM Agents</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_13_Optimization_Inference.html">
 <span class="dropdown-text">13 - 优化推理</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_14_Production_Deployment.html">
 <span class="dropdown-text">14 - 生产部署</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_15_Multimodal_LLMs.html">
 <span class="dropdown-text">15 - 多模态 LLM</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_16_Evaluation_Benchmarking.html">
 <span class="dropdown-text">16 - 评估基准</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_17_Cutting_Edge_Research_Future.html">
 <span class="dropdown-text">17 - 前沿研究</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-2-mathematical-foundations" id="toc-chapter-2-mathematical-foundations" class="nav-link active" data-scroll-target="#chapter-2-mathematical-foundations">Chapter 2: Mathematical Foundations</a>
  <ul class="collapse">
  <li><a href="#what-well-learn-today" id="toc-what-well-learn-today" class="nav-link" data-scroll-target="#what-well-learn-today">What We’ll Learn Today 🎯</a></li>
  <li><a href="#why-do-we-need-math-for-llms" id="toc-why-do-we-need-math-for-llms" class="nav-link" data-scroll-target="#why-do-we-need-math-for-llms">2.1 Why Do We Need Math for LLMs? 🤔</a>
  <ul class="collapse">
  <li><a href="#the-big-picture" id="toc-the-big-picture" class="nav-link" data-scroll-target="#the-big-picture">The Big Picture</a></li>
  <li><a href="#what-math-actually-does-in-llms" id="toc-what-math-actually-does-in-llms" class="nav-link" data-scroll-target="#what-math-actually-does-in-llms">What Math Actually Does in LLMs</a></li>
  </ul></li>
  <li><a href="#linear-algebra-the-language-of-ai" id="toc-linear-algebra-the-language-of-ai" class="nav-link" data-scroll-target="#linear-algebra-the-language-of-ai">2.1 Linear Algebra: The Language of AI 📊</a>
  <ul class="collapse">
  <li><a href="#what-is-a-vector-the-intuitive-way" id="toc-what-is-a-vector-the-intuitive-way" class="nav-link" data-scroll-target="#what-is-a-vector-the-intuitive-way">What is a Vector? (The Intuitive Way)</a></li>
  <li><a href="#vector-operations-what-can-we-do" id="toc-vector-operations-what-can-we-do" class="nav-link" data-scroll-target="#vector-operations-what-can-we-do">Vector Operations: What Can We Do?</a></li>
  <li><a href="#matrices-collections-of-vectors" id="toc-matrices-collections-of-vectors" class="nav-link" data-scroll-target="#matrices-collections-of-vectors">Matrices: Collections of Vectors</a></li>
  <li><a href="#matrix-multiplication-the-core-operation" id="toc-matrix-multiplication-the-core-operation" class="nav-link" data-scroll-target="#matrix-multiplication-the-core-operation">Matrix Multiplication: The Core Operation</a></li>
  </ul></li>
  <li><a href="#probability-dealing-with-uncertainty" id="toc-probability-dealing-with-uncertainty" class="nav-link" data-scroll-target="#probability-dealing-with-uncertainty">2.2 Probability: Dealing with Uncertainty 🎲</a>
  <ul class="collapse">
  <li><a href="#why-do-we-need-probability-in-llms" id="toc-why-do-we-need-probability-in-llms" class="nav-link" data-scroll-target="#why-do-we-need-probability-in-llms">Why Do We Need Probability in LLMs?</a></li>
  <li><a href="#probability-distributions-the-llms-way-of-thinking" id="toc-probability-distributions-the-llms-way-of-thinking" class="nav-link" data-scroll-target="#probability-distributions-the-llms-way-of-thinking">Probability Distributions: The LLM’s Way of Thinking</a></li>
  <li><a href="#information-theory-measuring-surprise" id="toc-information-theory-measuring-surprise" class="nav-link" data-scroll-target="#information-theory-measuring-surprise">Information Theory: Measuring Surprise</a></li>
  </ul></li>
  <li><a href="#optimization-teaching-computers-to-learn" id="toc-optimization-teaching-computers-to-learn" class="nav-link" data-scroll-target="#optimization-teaching-computers-to-learn">2.3 Optimization: Teaching Computers to Learn 📈</a>
  <ul class="collapse">
  <li><a href="#the-learning-problem" id="toc-the-learning-problem" class="nav-link" data-scroll-target="#the-learning-problem">The Learning Problem</a></li>
  <li><a href="#gradient-descent-the-learning-algorithm" id="toc-gradient-descent-the-learning-algorithm" class="nav-link" data-scroll-target="#gradient-descent-the-learning-algorithm">Gradient Descent: The Learning Algorithm</a></li>
  <li><a href="#different-types-of-gradient-descent" id="toc-different-types-of-gradient-descent" class="nav-link" data-scroll-target="#different-types-of-gradient-descent">Different Types of Gradient Descent</a></li>
  <li><a href="#advanced-optimizers-the-modern-way" id="toc-advanced-optimizers-the-modern-way" class="nav-link" data-scroll-target="#advanced-optimizers-the-modern-way">Advanced Optimizers: The Modern Way</a></li>
  </ul></li>
  <li><a href="#neural-networks-the-building-blocks" id="toc-neural-networks-the-building-blocks" class="nav-link" data-scroll-target="#neural-networks-the-building-blocks">2.4 Neural Networks: The Building Blocks 🧠</a>
  <ul class="collapse">
  <li><a href="#what-is-a-neuron-simplified" id="toc-what-is-a-neuron-simplified" class="nav-link" data-scroll-target="#what-is-a-neuron-simplified">What is a Neuron? (Simplified)</a></li>
  <li><a href="#mathematical-formula-dont-panic" id="toc-mathematical-formula-dont-panic" class="nav-link" data-scroll-target="#mathematical-formula-dont-panic">Mathematical Formula (Don’t Panic!)</a></li>
  <li><a href="#activation-functions-adding-non-linearity" id="toc-activation-functions-adding-non-linearity" class="nav-link" data-scroll-target="#activation-functions-adding-non-linearity">Activation Functions: Adding Non-linearity</a></li>
  <li><a href="#putting-it-together-a-simple-network" id="toc-putting-it-together-a-simple-network" class="nav-link" data-scroll-target="#putting-it-together-a-simple-network">Putting It Together: A Simple Network</a></li>
  <li><a href="#backpropagation-how-networks-learn" id="toc-backpropagation-how-networks-learn" class="nav-link" data-scroll-target="#backpropagation-how-networks-learn">Backpropagation: How Networks Learn</a></li>
  </ul></li>
  <li><a href="#putting-it-all-together-how-math-powers-llms" id="toc-putting-it-all-together-how-math-powers-llms" class="nav-link" data-scroll-target="#putting-it-all-together-how-math-powers-llms">2.5 Putting It All Together: How Math Powers LLMs 🔄</a>
  <ul class="collapse">
  <li><a href="#the-complete-picture" id="toc-the-complete-picture" class="nav-link" data-scroll-target="#the-complete-picture">The Complete Picture</a></li>
  <li><a href="#a-day-in-the-life-of-a-parameter" id="toc-a-day-in-the-life-of-a-parameter" class="nav-link" data-scroll-target="#a-day-in-the-life-of-a-parameter">A Day in the Life of a Parameter 📊</a></li>
  </ul></li>
  <li><a href="#common-questions-students-ask" id="toc-common-questions-students-ask" class="nav-link" data-scroll-target="#common-questions-students-ask">Common Questions Students Ask 🙋‍♀️</a>
  <ul class="collapse">
  <li><a href="#q-do-i-need-to-memorize-all-these-formulas" id="toc-q-do-i-need-to-memorize-all-these-formulas" class="nav-link" data-scroll-target="#q-do-i-need-to-memorize-all-these-formulas">Q: “Do I need to memorize all these formulas?”</a></li>
  <li><a href="#q-why-is-matrix-multiplication-so-important" id="toc-q-why-is-matrix-multiplication-so-important" class="nav-link" data-scroll-target="#q-why-is-matrix-multiplication-so-important">Q: “Why is matrix multiplication so important?”</a></li>
  <li><a href="#q-how-does-the-math-relate-to-chatgpt" id="toc-q-how-does-the-math-relate-to-chatgpt" class="nav-link" data-scroll-target="#q-how-does-the-math-relate-to-chatgpt">Q: “How does the math relate to ChatGPT?”</a></li>
  <li><a href="#q-is-this-math-hard" id="toc-q-is-this-math-hard" class="nav-link" data-scroll-target="#q-is-this-math-hard">Q: “Is this math hard?”</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways 🎯</a></li>
  <li><a href="#fun-exercises-to-solidify-understanding" id="toc-fun-exercises-to-solidify-understanding" class="nav-link" data-scroll-target="#fun-exercises-to-solidify-understanding">Fun Exercises to Solidify Understanding 🎮</a>
  <ul class="collapse">
  <li><a href="#exercise-1-vector-similarity" id="toc-exercise-1-vector-similarity" class="nav-link" data-scroll-target="#exercise-1-vector-similarity">Exercise 1: Vector Similarity</a></li>
  <li><a href="#exercise-2-probability-practice" id="toc-exercise-2-probability-practice" class="nav-link" data-scroll-target="#exercise-2-probability-practice">Exercise 2: Probability Practice</a></li>
  <li><a href="#exercise-3-gradient-descent-thinking" id="toc-exercise-3-gradient-descent-thinking" class="nav-link" data-scroll-target="#exercise-3-gradient-descent-thinking">Exercise 3: Gradient Descent Thinking</a></li>
  </ul></li>
  <li><a href="#whats-next" id="toc-whats-next" class="nav-link" data-scroll-target="#whats-next">What’s Next? 📚</a></li>
  <li><a href="#final-thought" id="toc-final-thought" class="nav-link" data-scroll-target="#final-thought">Final Thought 💭</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="chapter-2-mathematical-foundations" class="level1">
<h1>Chapter 2: Mathematical Foundations</h1>
<p><em>Making Math Intuitive for LLMs</em></p>
<section id="what-well-learn-today" class="level2">
<h2 class="anchored" data-anchor-id="what-well-learn-today">What We’ll Learn Today 🎯</h2>
<ul>
<li>Why math is the secret sauce behind LLMs (with intuitive explanations!)</li>
<li>Linear algebra: The language of neural networks</li>
<li>Probability: How computers handle uncertainty</li>
<li>Optimization: Teaching computers to get better</li>
<li>Neural networks: The building blocks explained simply</li>
</ul>
<hr>
</section>
<section id="why-do-we-need-math-for-llms" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-need-math-for-llms">2.1 Why Do We Need Math for LLMs? 🤔</h2>
<section id="the-big-picture" class="level3">
<h3 class="anchored" data-anchor-id="the-big-picture">The Big Picture</h3>
<pre><code>Think of math as the "physics" of AI:
- Just like physics explains how cars work
- Math explains how LLMs work
- Understanding the math helps you build better models</code></pre>
</section>
<section id="what-math-actually-does-in-llms" class="level3">
<h3 class="anchored" data-anchor-id="what-math-actually-does-in-llms">What Math Actually Does in LLMs</h3>
<ol type="1">
<li><strong>Linear Algebra:</strong> Transforms information between layers</li>
<li><strong>Probability:</strong> Handles uncertainty and makes predictions</li>
<li><strong>Optimization:</strong> Helps the model learn from data</li>
<li><strong>Calculus:</strong> Figures out how to improve the model</li>
</ol>
<p><strong>Don’t worry!</strong> We’ll make everything intuitive with analogies and examples! 🚀</p>
<hr>
</section>
</section>
<section id="linear-algebra-the-language-of-ai" class="level2">
<h2 class="anchored" data-anchor-id="linear-algebra-the-language-of-ai">2.1 Linear Algebra: The Language of AI 📊</h2>
<section id="what-is-a-vector-the-intuitive-way" class="level3">
<h3 class="anchored" data-anchor-id="what-is-a-vector-the-intuitive-way">What is a Vector? (The Intuitive Way)</h3>
<section id="analogy-1-a-recipe" class="level4">
<h4 class="anchored" data-anchor-id="analogy-1-a-recipe">Analogy 1: A Recipe 👨‍🍳</h4>
<pre><code>A chocolate cake recipe:
- 2 cups flour
- 1 cup sugar  
- 0.5 cups cocoa
- 3 eggs

This is a vector: [2, 1, 0.5, 3]</code></pre>
</section>
<section id="analogy-2-student-grades" class="level4">
<h4 class="anchored" data-anchor-id="analogy-2-student-grades">Analogy 2: Student Grades 🎓</h4>
<pre><code>Alice's grades:
- Math: 85
- English: 92
- Science: 78
- History: 90

This is a vector: [85, 92, 78, 90]</code></pre>
</section>
<section id="in-llms-word-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="in-llms-word-embeddings">In LLMs: Word Embeddings</h4>
<pre><code>The word "cat" might be represented as:
[0.2, -0.1, 0.8, 0.3, -0.5, ...]

Each number captures some aspect of "cat-ness":
- Maybe position 0 = "is an animal" (0.2 = somewhat)
- Maybe position 2 = "is cute" (0.8 = very much!)</code></pre>
</section>
</section>
<section id="vector-operations-what-can-we-do" class="level3">
<h3 class="anchored" data-anchor-id="vector-operations-what-can-we-do">Vector Operations: What Can We Do?</h3>
<section id="vector-addition-combining-information" class="level4">
<h4 class="anchored" data-anchor-id="vector-addition-combining-information">1. Vector Addition: Combining Information</h4>
<pre><code>Cat vector:    [0.2, -0.1, 0.8]
Small vector:  [0.1,  0.2, 0.1]
---------------------------------
Small cat:     [0.3,  0.1, 0.9]</code></pre>
<p><strong>Real Example in Word2Vec:</strong></p>
<pre><code>"King" - "Man" + "Woman" ≈ "Queen"</code></pre>
<p>This actually works! The math captures semantic relationships!</p>
</section>
<section id="dot-product-measuring-similarity" class="level4">
<h4 class="anchored" data-anchor-id="dot-product-measuring-similarity">2. Dot Product: Measuring Similarity</h4>
<pre><code>Vector A: [1, 2, 3]
Vector B: [2, 1, 0]

Dot product = 1×2 + 2×1 + 3×0 = 4</code></pre>
<p><strong>Intuition:</strong> How much do two vectors “point in the same direction”? - High dot product = very similar - Low dot product = very different - Zero dot product = completely unrelated</p>
<p><strong>In LLMs:</strong> This is how attention mechanism decides what to pay attention to!</p>
</section>
</section>
<section id="matrices-collections-of-vectors" class="level3">
<h3 class="anchored" data-anchor-id="matrices-collections-of-vectors">Matrices: Collections of Vectors</h3>
<section id="analogy-a-gradebook" class="level4">
<h4 class="anchored" data-anchor-id="analogy-a-gradebook">Analogy: A Gradebook 📋</h4>
<pre><code>        Math  English  Science
Alice    85     92      78
Bob      79     85      82  
Carol    92     88      95

This is a 3×3 matrix!</code></pre>
</section>
<section id="in-neural-networks-transformation-machines" class="level4">
<h4 class="anchored" data-anchor-id="in-neural-networks-transformation-machines">In Neural Networks: Transformation Machines</h4>
<pre><code>Think of a matrix as a "transformation machine":
Input: [student info] → Matrix → Output: [predicted grades]</code></pre>
</section>
</section>
<section id="matrix-multiplication-the-core-operation" class="level3">
<h3 class="anchored" data-anchor-id="matrix-multiplication-the-core-operation">Matrix Multiplication: The Core Operation</h3>
<section id="the-intuitive-way-to-think-about-it" class="level4">
<h4 class="anchored" data-anchor-id="the-intuitive-way-to-think-about-it">The Intuitive Way to Think About It</h4>
<pre><code>Matrix multiplication = "applying a transformation"

Example:
- Input: Information about a word
- Matrix: "Attention transformation"
- Output: How much to pay attention to other words</code></pre>
</section>
<section id="why-this-matters-for-llms" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters-for-llms">Why This Matters for LLMs</h4>
<p><strong>Every layer in a neural network does this:</strong></p>
<pre><code>Layer input → Matrix multiplication → Add bias → Apply activation → Layer output</code></pre>
<p><strong>Real Example:</strong></p>
<pre><code>Word "cat" embedding: [0.2, 0.8, 0.3]
Attention matrix: [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
Result: How much "cat" should attend to other words</code></pre>
<hr>
</section>
</section>
</section>
<section id="probability-dealing-with-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="probability-dealing-with-uncertainty">2.2 Probability: Dealing with Uncertainty 🎲</h2>
<section id="why-do-we-need-probability-in-llms" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-need-probability-in-llms">Why Do We Need Probability in LLMs?</h3>
<section id="the-fundamental-problem" class="level4">
<h4 class="anchored" data-anchor-id="the-fundamental-problem">The Fundamental Problem</h4>
<pre><code>When predicting the next word, there are multiple valid options:
"The cat sat on the ___"
- mat (likely)
- floor (likely)  
- dinosaur (unlikely but possible!)

We need a way to express confidence in our predictions.</code></pre>
</section>
</section>
<section id="probability-distributions-the-llms-way-of-thinking" class="level3">
<h3 class="anchored" data-anchor-id="probability-distributions-the-llms-way-of-thinking">Probability Distributions: The LLM’s Way of Thinking</h3>
<section id="what-is-a-probability-distribution" class="level4">
<h4 class="anchored" data-anchor-id="what-is-a-probability-distribution">What is a Probability Distribution?</h4>
<pre><code>Think of it as the model's "confidence levels":

For "The cat sat on the ___":
- "mat": 30% confidence
- "floor": 25% confidence  
- "chair": 20% confidence
- "table": 15% confidence
- "dinosaur": 0.1% confidence
- (all other words): remaining %</code></pre>
</section>
<section id="the-softmax-function-converting-numbers-to-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="the-softmax-function-converting-numbers-to-probabilities">The Softmax Function: Converting Numbers to Probabilities</h4>
<pre><code>Raw model outputs (logits): [2.1, 1.8, 1.5, 0.3, -5.2]
After softmax: [0.30, 0.25, 0.20, 0.15, 0.001]

Magic! Now they sum to 1 and look like probabilities!</code></pre>
<p><strong>How Softmax Works (Intuitively):</strong></p>
<pre><code>1. Make all numbers positive: e^x
2. Make them sum to 1: divide by the total
3. Higher original numbers → higher probabilities</code></pre>
</section>
</section>
<section id="information-theory-measuring-surprise" class="level3">
<h3 class="anchored" data-anchor-id="information-theory-measuring-surprise">Information Theory: Measuring Surprise</h3>
<section id="entropy-how-predictable-is-text" class="level4">
<h4 class="anchored" data-anchor-id="entropy-how-predictable-is-text">Entropy: How Predictable is Text?</h4>
<pre><code>Low entropy text: "2 + 2 = 4"
(very predictable, no surprise)

High entropy text: "The purple elephant danced with quantum mechanics"
(very unpredictable, much surprise!)</code></pre>
</section>
<section id="cross-entropy-loss-training-llms" class="level4">
<h4 class="anchored" data-anchor-id="cross-entropy-loss-training-llms">Cross-Entropy Loss: Training LLMs</h4>
<pre><code>This is how we measure "how wrong" the model is:

True answer: "mat" (100% confidence)
Model prediction: "mat" (30%), "floor" (25%), others (45%)

Cross-entropy loss = How surprised we are by the model's mistake</code></pre>
<p><strong>Intuition:</strong> We want to minimize surprise when the model is wrong!</p>
<hr>
</section>
</section>
</section>
<section id="optimization-teaching-computers-to-learn" class="level2">
<h2 class="anchored" data-anchor-id="optimization-teaching-computers-to-learn">2.3 Optimization: Teaching Computers to Learn 📈</h2>
<section id="the-learning-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-learning-problem">The Learning Problem</h3>
<section id="analogy-learning-to-play-basketball" class="level4">
<h4 class="anchored" data-anchor-id="analogy-learning-to-play-basketball">Analogy: Learning to Play Basketball 🏀</h4>
<pre><code>1. Try shooting from different positions
2. See which shots go in vs miss
3. Adjust your technique based on results
4. Repeat until you're good

This is exactly what neural networks do!</code></pre>
</section>
</section>
<section id="gradient-descent-the-learning-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-the-learning-algorithm">Gradient Descent: The Learning Algorithm</h3>
<section id="analogy-finding-the-valley" class="level4">
<h4 class="anchored" data-anchor-id="analogy-finding-the-valley">Analogy: Finding the Valley 🏔️</h4>
<pre><code>Imagine you're blindfolded on a mountain and want to reach the lowest valley:

1. Feel the slope under your feet
2. Take a step in the downhill direction  
3. Feel the new slope
4. Repeat until you reach the bottom

Gradient descent works the same way!</code></pre>
</section>
<section id="the-math-made-simple" class="level4">
<h4 class="anchored" data-anchor-id="the-math-made-simple">The Math (Made Simple)</h4>
<pre><code>1. Calculate how wrong the model is (loss)
2. Figure out which direction to adjust parameters (gradient)
3. Take a small step in that direction
4. Repeat millions of times</code></pre>
<p><strong>Key Insight:</strong> The “gradient” tells us which way is “downhill” for our error!</p>
</section>
</section>
<section id="different-types-of-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="different-types-of-gradient-descent">Different Types of Gradient Descent</h3>
<section id="batch-gradient-descent" class="level4">
<h4 class="anchored" data-anchor-id="batch-gradient-descent">1. Batch Gradient Descent</h4>
<pre><code>Analogy: Learning from all your basketball shots at once
- Look at ALL your misses
- Calculate the average mistake
- Adjust technique based on average

Pros: Very stable
Cons: Very slow for large datasets</code></pre>
</section>
<section id="stochastic-gradient-descent-sgd" class="level4">
<h4 class="anchored" data-anchor-id="stochastic-gradient-descent-sgd">2. Stochastic Gradient Descent (SGD)</h4>
<pre><code>Analogy: Learning from one shot at a time
- Take one shot
- Immediately adjust technique
- Take another shot

Pros: Much faster
Cons: Noisy, jumpy learning</code></pre>
</section>
<section id="mini-batch-gradient-descent" class="level4">
<h4 class="anchored" data-anchor-id="mini-batch-gradient-descent">3. Mini-batch Gradient Descent</h4>
<pre><code>Analogy: Learning from small groups of shots
- Take 10 shots
- Calculate average mistake for those 10
- Adjust technique
- Repeat with next 10 shots

This is the sweet spot! Used in most LLMs.</code></pre>
</section>
</section>
<section id="advanced-optimizers-the-modern-way" class="level3">
<h3 class="anchored" data-anchor-id="advanced-optimizers-the-modern-way">Advanced Optimizers: The Modern Way</h3>
<section id="adam-optimizer-the-smart-student" class="level4">
<h4 class="anchored" data-anchor-id="adam-optimizer-the-smart-student">Adam Optimizer: The Smart Student</h4>
<pre><code>Regular gradient descent: Always takes same-sized steps
Adam: Adapts step size based on confidence

Think of Adam as a smart student who:
1. Takes bigger steps when confident
2. Takes smaller steps when uncertain
3. Remembers recent learning patterns
4. Adjusts accordingly</code></pre>
<hr>
</section>
</section>
</section>
<section id="neural-networks-the-building-blocks" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks-the-building-blocks">2.4 Neural Networks: The Building Blocks 🧠</h2>
<section id="what-is-a-neuron-simplified" class="level3">
<h3 class="anchored" data-anchor-id="what-is-a-neuron-simplified">What is a Neuron? (Simplified)</h3>
<section id="biological-inspiration" class="level4">
<h4 class="anchored" data-anchor-id="biological-inspiration">Biological Inspiration</h4>
<pre><code>Brain neuron:
1. Receives signals from other neurons
2. Combines these signals
3. If total signal &gt; threshold, fires
4. Sends signal to other neurons</code></pre>
</section>
<section id="artificial-neuron" class="level4">
<h4 class="anchored" data-anchor-id="artificial-neuron">Artificial Neuron</h4>
<pre><code>Artificial neuron:
1. Receives numbers from other neurons
2. Multiplies each by a weight (importance)
3. Adds them up
4. Applies activation function
5. Outputs a number</code></pre>
</section>
</section>
<section id="mathematical-formula-dont-panic" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formula-dont-panic">Mathematical Formula (Don’t Panic!)</h3>
<pre><code>output = activation_function(weight₁ × input₁ + weight₂ × input₂ + ... + bias)</code></pre>
<p><strong>Example:</strong></p>
<pre><code>Inputs: [0.2, 0.8, 0.3] (word embedding)
Weights: [0.5, 0.7, 0.1] (learned parameters)
Bias: 0.2

Calculation: 0.5×0.2 + 0.7×0.8 + 0.1×0.3 + 0.2 = 0.9
If activation = ReLU: output = max(0, 0.9) = 0.9</code></pre>
</section>
<section id="activation-functions-adding-non-linearity" class="level3">
<h3 class="anchored" data-anchor-id="activation-functions-adding-non-linearity">Activation Functions: Adding Non-linearity</h3>
<section id="why-do-we-need-them" class="level4">
<h4 class="anchored" data-anchor-id="why-do-we-need-them">Why Do We Need Them?</h4>
<pre><code>Without activation functions:
Neural network = Just complicated linear algebra
Can only draw straight lines through data

With activation functions:
Neural network = Can learn complex patterns
Can draw curves, handle complex relationships</code></pre>
</section>
<section id="common-activation-functions" class="level4">
<h4 class="anchored" data-anchor-id="common-activation-functions">Common Activation Functions</h4>
<p><strong>1. ReLU (Rectified Linear Unit)</strong></p>
<pre><code>ReLU(x) = max(0, x)

Think: "Only pass positive signals"
- If input &gt; 0: pass it through
- If input ≤ 0: output 0

Why it's popular: Simple, fast, works well!</code></pre>
<p><strong>2. Sigmoid</strong></p>
<pre><code>Sigmoid(x) = 1 / (1 + e^(-x))

Think: "Squash everything between 0 and 1"
Good for: When you need probabilities
Problem: Can cause vanishing gradients</code></pre>
<p><strong>3. GELU (used in modern LLMs)</strong></p>
<pre><code>GELU(x) = x × P(X ≤ x) where X ~ N(0,1)

Think: "Smooth version of ReLU with probabilistic gating"
Why modern models use it: Better empirical performance</code></pre>
</section>
</section>
<section id="putting-it-together-a-simple-network" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-together-a-simple-network">Putting It Together: A Simple Network</h3>
<section id="layer-by-layer-breakdown" class="level4">
<h4 class="anchored" data-anchor-id="layer-by-layer-breakdown">Layer-by-Layer Breakdown</h4>
<pre><code>Input Layer: Raw data (e.g., word embeddings)
    ↓
Hidden Layer 1: Transforms input to find patterns
    ↓  
Hidden Layer 2: Finds more complex patterns
    ↓
Output Layer: Makes final prediction</code></pre>
</section>
<section id="information-flow-example" class="level4">
<h4 class="anchored" data-anchor-id="information-flow-example">Information Flow Example</h4>
<pre><code>Input: "The cat sat on the"
↓
Embedding Layer: Convert words to numbers
↓
Hidden Layers: Learn patterns like "cat sits on things"
↓
Output Layer: Predict next word probabilities</code></pre>
</section>
</section>
<section id="backpropagation-how-networks-learn" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-how-networks-learn">Backpropagation: How Networks Learn</h3>
<section id="the-intuitive-explanation" class="level4">
<h4 class="anchored" data-anchor-id="the-intuitive-explanation">The Intuitive Explanation</h4>
<pre><code>Think of a chain of students passing a message:
Student A → Student B → Student C → Final Answer

If the final answer is wrong:
1. Figure out how much Student C contributed to error
2. Figure out how much Student B contributed to error  
3. Figure out how much Student A contributed to error
4. Each student adjusts their behavior accordingly

This is backpropagation!</code></pre>
</section>
<section id="the-process" class="level4">
<h4 class="anchored" data-anchor-id="the-process">The Process</h4>
<pre><code>1. Forward pass: Input flows through network → prediction
2. Calculate loss: How wrong was the prediction?
3. Backward pass: Figure out each parameter's contribution to error
4. Update parameters: Adjust each parameter to reduce error
5. Repeat for next example</code></pre>
<hr>
</section>
</section>
</section>
<section id="putting-it-all-together-how-math-powers-llms" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together-how-math-powers-llms">2.5 Putting It All Together: How Math Powers LLMs 🔄</h2>
<section id="the-complete-picture" class="level3">
<h3 class="anchored" data-anchor-id="the-complete-picture">The Complete Picture</h3>
<section id="training-process" class="level4">
<h4 class="anchored" data-anchor-id="training-process">Training Process</h4>
<pre><code>1. Take a sentence: "The cat sat on the mat"
2. Convert to vectors: [[0.2, 0.8], [0.1, 0.9], ...]
3. Feed through neural network layers
4. Get probability distribution for next word
5. Compare with actual next word
6. Use backpropagation to adjust all parameters
7. Repeat billions of times!</code></pre>
</section>
<section id="why-each-math-component-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-each-math-component-matters">Why Each Math Component Matters</h4>
<p><strong>Linear Algebra:</strong> - Transforms information between layers - Enables parallel computation - Core of attention mechanism</p>
<p><strong>Probability:</strong> - Handles uncertainty in predictions - Enables sampling different responses - Measures model confidence</p>
<p><strong>Optimization:</strong> - Enables learning from data - Finds best parameters automatically - Makes training feasible</p>
<p><strong>Neural Networks:</strong> - Provides flexible function approximation - Enables complex pattern recognition - Scales to massive datasets</p>
</section>
</section>
<section id="a-day-in-the-life-of-a-parameter" class="level3">
<h3 class="anchored" data-anchor-id="a-day-in-the-life-of-a-parameter">A Day in the Life of a Parameter 📊</h3>
<pre><code>Morning: Parameter starts with random value (say, 0.1)

Training Example 1: "The cat..."
- Forward pass: Helps predict "sat" 
- Actual next word: "sat" ✓
- Gradient: Small positive adjustment
- New value: 0.101

Training Example 2: "The dog..."  
- Forward pass: Helps predict "barked"
- Actual next word: "ran"
- Gradient: Small negative adjustment  
- New value: 0.099

After millions of examples:
- Parameter has learned optimal value
- Contributes to good predictions
- Part of the "knowledge" in the model</code></pre>
<hr>
</section>
</section>
<section id="common-questions-students-ask" class="level2">
<h2 class="anchored" data-anchor-id="common-questions-students-ask">Common Questions Students Ask 🙋‍♀️</h2>
<section id="q-do-i-need-to-memorize-all-these-formulas" class="level3">
<h3 class="anchored" data-anchor-id="q-do-i-need-to-memorize-all-these-formulas">Q: “Do I need to memorize all these formulas?”</h3>
<p><strong>A:</strong> No! Focus on understanding the intuition. The formulas are tools, not the goal.</p>
</section>
<section id="q-why-is-matrix-multiplication-so-important" class="level3">
<h3 class="anchored" data-anchor-id="q-why-is-matrix-multiplication-so-important">Q: “Why is matrix multiplication so important?”</h3>
<p><strong>A:</strong> It’s the fundamental operation that transforms information in neural networks. Think of it as the “engine” that powers AI.</p>
</section>
<section id="q-how-does-the-math-relate-to-chatgpt" class="level3">
<h3 class="anchored" data-anchor-id="q-how-does-the-math-relate-to-chatgpt">Q: “How does the math relate to ChatGPT?”</h3>
<p><strong>A:</strong> Every time ChatGPT generates a word, it’s doing millions of these mathematical operations to predict what comes next!</p>
</section>
<section id="q-is-this-math-hard" class="level3">
<h3 class="anchored" data-anchor-id="q-is-this-math-hard">Q: “Is this math hard?”</h3>
<p><strong>A:</strong> The concepts are more important than the calculations. Modern frameworks (PyTorch, TensorFlow) handle the math for you!</p>
<hr>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways 🎯</h2>
<ol type="1">
<li><p><strong>Linear algebra</strong> is the language neural networks use to transform information</p></li>
<li><p><strong>Probability</strong> helps models express uncertainty and make predictions</p></li>
<li><p><strong>Optimization</strong> is how models learn from data automatically</p></li>
<li><p><strong>Neural networks</strong> are function approximators that can learn complex patterns</p></li>
<li><p><strong>All these math concepts work together</strong> to enable the amazing capabilities we see in LLMs</p></li>
</ol>
<hr>
</section>
<section id="fun-exercises-to-solidify-understanding" class="level2">
<h2 class="anchored" data-anchor-id="fun-exercises-to-solidify-understanding">Fun Exercises to Solidify Understanding 🎮</h2>
<section id="exercise-1-vector-similarity" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1-vector-similarity">Exercise 1: Vector Similarity</h3>
<pre><code>Calculate dot product similarity:
Cat: [1, 0, 1]     (mammal=1, flies=0, cute=1)
Dog: [1, 0, 1]     (mammal=1, flies=0, cute=1)  
Bird: [0, 1, 1]    (mammal=0, flies=1, cute=1)

Which animals are most similar?</code></pre>
</section>
<section id="exercise-2-probability-practice" class="level3">
<h3 class="anchored" data-anchor-id="exercise-2-probability-practice">Exercise 2: Probability Practice</h3>
<pre><code>Model outputs for "The cat is ___":
Raw scores: [2.0, 1.5, 0.5] for words ["sleeping", "running", "flying"]

Which word is most likely?
(Hint: Higher score = higher probability after softmax)</code></pre>
</section>
<section id="exercise-3-gradient-descent-thinking" class="level3">
<h3 class="anchored" data-anchor-id="exercise-3-gradient-descent-thinking">Exercise 3: Gradient Descent Thinking</h3>
<pre><code>You're trying to minimize errors in predicting cat behavior.
Current error: High
Which direction should you adjust your "cat understanding" parameter?</code></pre>
<hr>
</section>
</section>
<section id="whats-next" class="level2">
<h2 class="anchored" data-anchor-id="whats-next">What’s Next? 📚</h2>
<p>In Chapter 3, we’ll see how these mathematical concepts apply specifically to natural language processing!</p>
<p><strong>Preview:</strong> We’ll learn about: - How computers break down text (tokenization) - Word embeddings: turning words into math - Language models: predicting what comes next - Evaluation: how do we know if our model is good?</p>
<p>Remember: Math is just a tool to help us build amazing things. The real magic is in how we apply it! 🚀</p>
<hr>
</section>
<section id="final-thought" class="level2">
<h2 class="anchored" data-anchor-id="final-thought">Final Thought 💭</h2>
<pre><code>"Mathematics is not about numbers, equations, computations, or algorithms: 
it is about understanding." - William Paul Thurston

In LLMs, math helps us understand how to build machines that understand language.
Pretty cool, right? 😊</code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>