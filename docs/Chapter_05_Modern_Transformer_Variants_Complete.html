<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>chapter_05_modern_transformer_variants_complete – Large Language Models Complete Tutorial</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7f2553f82da0a27203d2dd69918cfc55.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-99da8eeeb61bed438ded90c173150d79.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-7f2553f82da0a27203d2dd69918cfc55.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="custom.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Large Language Models Complete Tutorial</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li>
    <a class="dropdown-item" href="./Chapter_01_Introduction_LLMs.html">
 <span class="dropdown-text">01 - Introduction to LLMs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_02_Math_Foundations.html">
 <span class="dropdown-text">02 - Mathematical Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_03_NLP_Fundamentals.html">
 <span class="dropdown-text">03 - NLP Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_04_Transformer_Architecture.html">
 <span class="dropdown-text">04 - Transformer Architecture</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_05_Modern_Transformer_Variants_Complete.html">
 <span class="dropdown-text">05 - Modern Transformer Variants</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_08_Fine_Tuning.html">
 <span class="dropdown-text">08 - Fine-Tuning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_09_Alignment_RLHF.html">
 <span class="dropdown-text">09 - Alignment &amp; RLHF</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_10_Prompting_InContext_Learning.html">
 <span class="dropdown-text">10 - Prompting &amp; In-Context Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_11_RAG.html">
 <span class="dropdown-text">11 - Retrieval-Augmented Generation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_12_LLM_Agents_Tool_Use.html">
 <span class="dropdown-text">12 - LLM Agents &amp; Tool Use</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_13_Optimization_Inference.html">
 <span class="dropdown-text">13 - Optimization &amp; Inference</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_14_Production_Deployment.html">
 <span class="dropdown-text">14 - Production &amp; Deployment</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_15_Multimodal_LLMs.html">
 <span class="dropdown-text">15 - Multimodal LLMs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_16_Evaluation_Benchmarking.html">
 <span class="dropdown-text">16 - Evaluation &amp; Benchmarking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./Chapter_17_Cutting_Edge_Research_Future.html">
 <span class="dropdown-text">17 - Cutting-Edge Research &amp; Future</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-5-modern-transformer-variants" id="toc-chapter-5-modern-transformer-variants" class="nav-link active" data-scroll-target="#chapter-5-modern-transformer-variants">Chapter 5: Modern Transformer Variants</a>
  <ul class="collapse">
  <li><a href="#what-well-learn-today" id="toc-what-well-learn-today" class="nav-link" data-scroll-target="#what-well-learn-today">What We’ll Learn Today 🎯</a></li>
  <li><a href="#the-transformer-family-tree" id="toc-the-transformer-family-tree" class="nav-link" data-scroll-target="#the-transformer-family-tree">5.1 The Transformer Family Tree 🌳</a>
  <ul class="collapse">
  <li><a href="#understanding-the-branches" id="toc-understanding-the-branches" class="nav-link" data-scroll-target="#understanding-the-branches">Understanding the Branches</a></li>
  </ul></li>
  <li><a href="#decoder-only-models-the-generation-champions" id="toc-decoder-only-models-the-generation-champions" class="nav-link" data-scroll-target="#decoder-only-models-the-generation-champions">5.2 Decoder-Only Models: The Generation Champions 🎭</a>
  <ul class="collapse">
  <li><a href="#gpt-series-the-evolution-story" id="toc-gpt-series-the-evolution-story" class="nav-link" data-scroll-target="#gpt-series-the-evolution-story">GPT Series: The Evolution Story</a></li>
  <li><a href="#llama-the-open-source-hero" id="toc-llama-the-open-source-hero" class="nav-link" data-scroll-target="#llama-the-open-source-hero">LLaMA: The Open Source Hero 🦙</a></li>
  <li><a href="#mistral-the-efficiency-expert" id="toc-mistral-the-efficiency-expert" class="nav-link" data-scroll-target="#mistral-the-efficiency-expert">Mistral: The Efficiency Expert ⚡</a></li>
  </ul></li>
  <li><a href="#encoder-only-models-the-understanding-specialists" id="toc-encoder-only-models-the-understanding-specialists" class="nav-link" data-scroll-target="#encoder-only-models-the-understanding-specialists">5.3 Encoder-Only Models: The Understanding Specialists 🔍</a>
  <ul class="collapse">
  <li><a href="#bert-the-bidirectional-breakthrough" id="toc-bert-the-bidirectional-breakthrough" class="nav-link" data-scroll-target="#bert-the-bidirectional-breakthrough">BERT: The Bidirectional Breakthrough</a></li>
  <li><a href="#roberta-bert-done-right" id="toc-roberta-bert-done-right" class="nav-link" data-scroll-target="#roberta-bert-done-right">RoBERTa: BERT Done Right</a></li>
  <li><a href="#electra-the-efficient-alternative" id="toc-electra-the-efficient-alternative" class="nav-link" data-scroll-target="#electra-the-efficient-alternative">ELECTRA: The Efficient Alternative</a></li>
  </ul></li>
  <li><a href="#encoder-decoder-models-the-transformation-masters" id="toc-encoder-decoder-models-the-transformation-masters" class="nav-link" data-scroll-target="#encoder-decoder-models-the-transformation-masters">5.4 Encoder-Decoder Models: The Transformation Masters 🔄</a>
  <ul class="collapse">
  <li><a href="#t5-text-to-text-transfer-transformer" id="toc-t5-text-to-text-transfer-transformer" class="nav-link" data-scroll-target="#t5-text-to-text-transfer-transformer">T5: Text-to-Text Transfer Transformer</a></li>
  <li><a href="#bart-the-denoising-expert" id="toc-bart-the-denoising-expert" class="nav-link" data-scroll-target="#bart-the-denoising-expert">BART: The Denoising Expert</a></li>
  </ul></li>
  <li><a href="#efficiency-innovations-making-transformers-faster" id="toc-efficiency-innovations-making-transformers-faster" class="nav-link" data-scroll-target="#efficiency-innovations-making-transformers-faster">5.5 Efficiency Innovations: Making Transformers Faster 🏃‍♂️</a>
  <ul class="collapse">
  <li><a href="#the-quadratic-problem" id="toc-the-quadratic-problem" class="nav-link" data-scroll-target="#the-quadratic-problem">The Quadratic Problem</a></li>
  <li><a href="#flashattention-the-memory-magician" id="toc-flashattention-the-memory-magician" class="nav-link" data-scroll-target="#flashattention-the-memory-magician">FlashAttention: The Memory Magician 🎩</a></li>
  <li><a href="#pagedattention-virtual-memory-for-ai" id="toc-pagedattention-virtual-memory-for-ai" class="nav-link" data-scroll-target="#pagedattention-virtual-memory-for-ai">PagedAttention: Virtual Memory for AI 💾</a></li>
  <li><a href="#grouped-query-attention-gqa-smart-sharing" id="toc-grouped-query-attention-gqa-smart-sharing" class="nav-link" data-scroll-target="#grouped-query-attention-gqa-smart-sharing">Grouped Query Attention (GQA): Smart Sharing 🤝</a></li>
  </ul></li>
  <li><a href="#architectural-innovations-the-latest-tricks" id="toc-architectural-innovations-the-latest-tricks" class="nav-link" data-scroll-target="#architectural-innovations-the-latest-tricks">5.6 Architectural Innovations: The Latest Tricks 🔧</a>
  <ul class="collapse">
  <li><a href="#mixture-of-experts-moe-specialization-at-scale" id="toc-mixture-of-experts-moe-specialization-at-scale" class="nav-link" data-scroll-target="#mixture-of-experts-moe-specialization-at-scale">Mixture of Experts (MoE): Specialization at Scale</a></li>
  <li><a href="#alternative-attention-mechanisms" id="toc-alternative-attention-mechanisms" class="nav-link" data-scroll-target="#alternative-attention-mechanisms">Alternative Attention Mechanisms</a></li>
  <li><a href="#rmsnorm-simplifying-normalization" id="toc-rmsnorm-simplifying-normalization" class="nav-link" data-scroll-target="#rmsnorm-simplifying-normalization">RMSNorm: Simplifying Normalization</a></li>
  </ul></li>
  <li><a href="#choosing-the-right-architecture" id="toc-choosing-the-right-architecture" class="nav-link" data-scroll-target="#choosing-the-right-architecture">5.7 Choosing the Right Architecture 🎯</a>
  <ul class="collapse">
  <li><a href="#decision-framework" id="toc-decision-framework" class="nav-link" data-scroll-target="#decision-framework">Decision Framework</a></li>
  <li><a href="#modern-trends" id="toc-modern-trends" class="nav-link" data-scroll-target="#modern-trends">Modern Trends</a></li>
  </ul></li>
  <li><a href="#real-world-examples" id="toc-real-world-examples" class="nav-link" data-scroll-target="#real-world-examples">Real-World Examples 🌍</a>
  <ul class="collapse">
  <li><a href="#case-study-1-building-a-chatbot" id="toc-case-study-1-building-a-chatbot" class="nav-link" data-scroll-target="#case-study-1-building-a-chatbot">Case Study 1: Building a Chatbot</a></li>
  <li><a href="#case-study-2-document-classification" id="toc-case-study-2-document-classification" class="nav-link" data-scroll-target="#case-study-2-document-classification">Case Study 2: Document Classification</a></li>
  <li><a href="#case-study-3-content-summarization" id="toc-case-study-3-content-summarization" class="nav-link" data-scroll-target="#case-study-3-content-summarization">Case Study 3: Content Summarization</a></li>
  </ul></li>
  <li><a href="#common-misconceptions" id="toc-common-misconceptions" class="nav-link" data-scroll-target="#common-misconceptions">Common Misconceptions 🚫</a>
  <ul class="collapse">
  <li><a href="#bigger-is-always-better" id="toc-bigger-is-always-better" class="nav-link" data-scroll-target="#bigger-is-always-better">“Bigger is Always Better”</a></li>
  <li><a href="#you-need-the-latest-architecture" id="toc-you-need-the-latest-architecture" class="nav-link" data-scroll-target="#you-need-the-latest-architecture">“You Need the Latest Architecture”</a></li>
  <li><a href="#all-tasks-need-huge-models" id="toc-all-tasks-need-huge-models" class="nav-link" data-scroll-target="#all-tasks-need-huge-models">“All Tasks Need Huge Models”</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways 🎯</a></li>
  <li><a href="#fun-exercises" id="toc-fun-exercises" class="nav-link" data-scroll-target="#fun-exercises">Fun Exercises 🎮</a>
  <ul class="collapse">
  <li><a href="#exercise-1-architecture-selection" id="toc-exercise-1-architecture-selection" class="nav-link" data-scroll-target="#exercise-1-architecture-selection">Exercise 1: Architecture Selection</a></li>
  <li><a href="#exercise-2-efficiency-analysis" id="toc-exercise-2-efficiency-analysis" class="nav-link" data-scroll-target="#exercise-2-efficiency-analysis">Exercise 2: Efficiency Analysis</a></li>
  <li><a href="#exercise-3-model-comparison" id="toc-exercise-3-model-comparison" class="nav-link" data-scroll-target="#exercise-3-model-comparison">Exercise 3: Model Comparison</a></li>
  </ul></li>
  <li><a href="#whats-next" id="toc-whats-next" class="nav-link" data-scroll-target="#whats-next">What’s Next? 📚</a></li>
  <li><a href="#final-thought" id="toc-final-thought" class="nav-link" data-scroll-target="#final-thought">Final Thought 💭</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>





<section id="chapter-5-modern-transformer-variants" class="level1">
<h1>Chapter 5: Modern Transformer Variants</h1>
<p><em>From BERT to GPT to LLaMA: The Family Tree of Modern AI</em></p>
<section id="what-well-learn-today" class="level2">
<h2 class="anchored" data-anchor-id="what-well-learn-today">What We’ll Learn Today 🎯</h2>
<ul>
<li>How different transformers are like different tools for different jobs</li>
<li>Why GPT became the “Swiss Army knife” of AI</li>
<li>Efficiency tricks that make transformers faster and cheaper</li>
<li>The latest architectural innovations (explained simply!)</li>
<li>How to choose the right transformer for your task</li>
</ul>
<p><strong>Big Idea:</strong> The original transformer was just the beginning - now we have specialized variants optimized for different purposes!</p>
<hr>
</section>
<section id="the-transformer-family-tree" class="level2">
<h2 class="anchored" data-anchor-id="the-transformer-family-tree">5.1 The Transformer Family Tree 🌳</h2>
<section id="understanding-the-branches" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-branches">Understanding the Branches</h3>
<section id="the-original-transformer-2017-the-ancestor" class="level4">
<h4 class="anchored" data-anchor-id="the-original-transformer-2017-the-ancestor">The Original Transformer (2017): The Ancestor</h4>
<pre><code>"Attention Is All You Need" paper introduced:
✅ Encoder-decoder architecture
✅ Self-attention mechanism
✅ Multi-head attention
✅ Position encoding

But it was just for translation tasks!</code></pre>
</section>
<section id="the-three-main-branches" class="level4">
<h4 class="anchored" data-anchor-id="the-three-main-branches">The Three Main Branches</h4>
<pre><code>                    Original Transformer
                           |
        ┌─────────────────┼─────────────────┐
        |                 |                 |
   Encoder-Only      Decoder-Only     Encoder-Decoder
   (Understanding)   (Generation)     (Transformation)
        |                 |                 |
      BERT              GPT               T5
    (2018)            (2018)            (2019)</code></pre>
<hr>
</section>
</section>
</section>
<section id="decoder-only-models-the-generation-champions" class="level2">
<h2 class="anchored" data-anchor-id="decoder-only-models-the-generation-champions">5.2 Decoder-Only Models: The Generation Champions 🎭</h2>
<section id="gpt-series-the-evolution-story" class="level3">
<h3 class="anchored" data-anchor-id="gpt-series-the-evolution-story">GPT Series: The Evolution Story</h3>
<section id="gpt-1-2018-the-proof-of-concept" class="level4">
<h4 class="anchored" data-anchor-id="gpt-1-2018-the-proof-of-concept">GPT-1 (2018): The Proof of Concept</h4>
<pre><code>Think of GPT-1 as the first smartphone:
- It worked, but was pretty basic
- 117M parameters (tiny by today's standards!)
- Showed that "pre-train then fine-tune" works
- Context length: 512 tokens (about 1 paragraph)</code></pre>
<p><strong>Key Innovation:</strong></p>
<pre><code>Instead of training from scratch for each task:
1. Pre-train on lots of text (learn language)
2. Fine-tune on specific task (learn the job)

This was revolutionary! Like teaching someone general knowledge,
then specialized training for specific jobs.</code></pre>
</section>
<section id="gpt-2-2019-the-surprise-star" class="level4">
<h4 class="anchored" data-anchor-id="gpt-2-2019-the-surprise-star">GPT-2 (2019): The Surprise Star</h4>
<pre><code>GPT-2 was like discovering your smartphone could also:
- Take photos, play games, navigate, etc.
- 1.5B parameters (10x larger!)
- Context length: 1024 tokens
- Zero-shot task transfer!</code></pre>
<p><strong>The “Zero-Shot” Breakthrough:</strong></p>
<pre><code>Amazing discovery: You could give GPT-2 examples and it would learn!

Example:
Input: "French: Bonjour, English: Hello, French: Au revoir, English:"
Output: "Goodbye"

No fine-tuning needed! It just learned from the pattern!</code></pre>
<p><strong>Fun Fact:</strong> OpenAI initially didn’t release GPT-2 because they worried about misuse!</p>
</section>
<section id="gpt-3-2020-the-game-changer" class="level4">
<h4 class="anchored" data-anchor-id="gpt-3-2020-the-game-changer">GPT-3 (2020): The Game Changer 🚀</h4>
<pre><code>GPT-3 was like upgrading from a smartphone to a supercomputer:
- 175B parameters (100x larger than GPT-2!)
- Context length: 2048 tokens (later extended)
- Emergent abilities nobody expected!</code></pre>
<p><strong>Emergent Abilities (Nobody Taught It These!):</strong></p>
<pre><code>✨ Code generation: "Write a Python function to sort a list"
✨ Math reasoning: "If I have 3 apples and buy 2 more..."
✨ Creative writing: "Write a poem about AI"
✨ Language translation: Even for languages barely in training data!
✨ Few-shot learning: Learn tasks from just a few examples</code></pre>
<p><strong>In-Context Learning:</strong></p>
<pre><code>The magic of GPT-3: It learns from the conversation itself!

You: "Translate these examples: cat→gato, dog→perro, bird→?"
GPT-3: "pájaro"

It figured out the pattern without any training updates!</code></pre>
</section>
<section id="gpt-4-2023-the-multimodal-marvel" class="level4">
<h4 class="anchored" data-anchor-id="gpt-4-2023-the-multimodal-marvel">GPT-4 (2023): The Multimodal Marvel</h4>
<pre><code>GPT-4 is like having a super-intelligent assistant that can:
- Read text and see images
- Reason about complex problems
- Write code that actually works
- Pass professional exams (bar exam, medical exams!)</code></pre>
<p><strong>Architecture Secrets (Rumored):</strong></p>
<pre><code>🤔 Mixture of Experts (MoE) - multiple specialized sub-models
🤔 Multimodal training - text and images together
🤔 Better reasoning through more compute at inference time
🤔 Advanced safety training and alignment</code></pre>
</section>
</section>
<section id="llama-the-open-source-hero" class="level3">
<h3 class="anchored" data-anchor-id="llama-the-open-source-hero">LLaMA: The Open Source Hero 🦙</h3>
<section id="why-llama-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-llama-matters">Why LLaMA Matters</h4>
<pre><code>Problem: GPT models are closed source and expensive
Solution: Meta releases LLaMA - high-quality, open research models!

Impact: Sparked an entire ecosystem of open-source models</code></pre>
</section>
<section id="llamas-smart-design-choices" class="level4">
<h4 class="anchored" data-anchor-id="llamas-smart-design-choices">LLaMA’s Smart Design Choices</h4>
<p><strong>1. RMSNorm Instead of LayerNorm</strong></p>
<pre><code>LayerNorm: Normalize using mean AND variance
RMSNorm: Normalize using only RMS (root mean square)

Benefits:
✅ Simpler computation
✅ Slightly faster
✅ Better numerical stability
✅ Same performance</code></pre>
<p><strong>2. SwiGLU Activation Function</strong></p>
<pre><code>Traditional: ReLU or GELU
LLaMA: SwiGLU (Swish-Gated Linear Unit)

Think of it as a "smart gate" that decides what information to pass through:
SwiGLU(x) = Swish(x·W₁) ⊙ (x·W₂)

Benefits: Better performance, especially for large models</code></pre>
<p><strong>3. RoPE Position Encoding</strong></p>
<pre><code>Problem: Sinusoidal encoding doesn't extrapolate well to longer sequences
Solution: RoPE (Rotary Position Embedding)

Analogy: Instead of adding position info, "rotate" the embeddings
Result: Much better handling of long sequences!</code></pre>
</section>
<section id="llama-2-the-improved-version" class="level4">
<h4 class="anchored" data-anchor-id="llama-2-the-improved-version">LLaMA 2: The Improved Version</h4>
<pre><code>Improvements over LLaMA 1:
✅ Longer context (4K tokens vs 2K)
✅ Better training data curation
✅ Grouped Query Attention (more efficient)
✅ RLHF training (more helpful and safe)</code></pre>
</section>
</section>
<section id="mistral-the-efficiency-expert" class="level3">
<h3 class="anchored" data-anchor-id="mistral-the-efficiency-expert">Mistral: The Efficiency Expert ⚡</h3>
<section id="sliding-window-attention" class="level4">
<h4 class="anchored" data-anchor-id="sliding-window-attention">Sliding Window Attention</h4>
<pre><code>Problem: Full attention is O(n²) - very expensive for long sequences
Mistral's solution: Each token only attends to the last W tokens

Analogy: Instead of remembering everything from the entire conversation,
only remember the last few sentences clearly.

Benefits:
✅ Linear memory usage
✅ Faster inference
✅ Can still capture long-range dependencies through layer stacking</code></pre>
</section>
<section id="mixtral-mixture-of-experts-8" class="level4">
<h4 class="anchored" data-anchor-id="mixtral-mixture-of-experts-8">Mixtral: Mixture of Experts 🧠×8</h4>
<pre><code>Revolutionary idea: Instead of one big brain, have 8 specialized brains!

For each token:
1. Router decides which 2 experts to use
2. Only activate those 2 experts
3. Combine their outputs

Result: 8×7B model that only uses 2×7B computation per token!</code></pre>
<hr>
</section>
</section>
</section>
<section id="encoder-only-models-the-understanding-specialists" class="level2">
<h2 class="anchored" data-anchor-id="encoder-only-models-the-understanding-specialists">5.3 Encoder-Only Models: The Understanding Specialists 🔍</h2>
<section id="bert-the-bidirectional-breakthrough" class="level3">
<h3 class="anchored" data-anchor-id="bert-the-bidirectional-breakthrough">BERT: The Bidirectional Breakthrough</h3>
<section id="the-key-innovation-bidirectional-context" class="level4">
<h4 class="anchored" data-anchor-id="the-key-innovation-bidirectional-context">The Key Innovation: Bidirectional Context</h4>
<pre><code>Previous models: Read left-to-right (like humans reading)
BERT: Read in both directions simultaneously!

Sentence: "The cat sat on the mat"
Traditional: When processing "cat", only sees "The"
BERT: When processing "cat", sees "The" AND "sat on the mat"</code></pre>
</section>
<section id="masked-language-modeling-training" class="level4">
<h4 class="anchored" data-anchor-id="masked-language-modeling-training">Masked Language Modeling Training</h4>
<pre><code>Training trick: Randomly mask words and predict them

Input: "The [MASK] sat on the mat"
BERT's job: Figure out the masked word is "cat"

Why this works: Forces model to use context from both sides!</code></pre>
<p><strong>Masking Strategy:</strong></p>
<pre><code>For 15% of tokens:
- 80% replace with [MASK]: "The [MASK] sat"
- 10% replace with random word: "The dog sat" 
- 10% keep original: "The cat sat"

Why the variety? Prevents model from just memorizing that [MASK] means "predict this"</code></pre>
</section>
<section id="berts-superpowers" class="level4">
<h4 class="anchored" data-anchor-id="berts-superpowers">BERT’s Superpowers</h4>
<pre><code>✅ Reading comprehension: Understands context deeply
✅ Question answering: Can find answers in passages
✅ Sentiment analysis: Understands emotional tone
✅ Named entity recognition: Identifies people, places, things
✅ Text classification: Categorizes documents</code></pre>
</section>
</section>
<section id="roberta-bert-done-right" class="level3">
<h3 class="anchored" data-anchor-id="roberta-bert-done-right">RoBERTa: BERT Done Right</h3>
<section id="what-facebook-fixed" class="level4">
<h4 class="anchored" data-anchor-id="what-facebook-fixed">What Facebook Fixed</h4>
<pre><code>Original BERT had some questionable choices:
❌ Next Sentence Prediction (turned out useless)
❌ Static masking (same mask every epoch)
❌ Small batch sizes
❌ Short training

RoBERTa improvements:
✅ Removed Next Sentence Prediction
✅ Dynamic masking (different mask each epoch)
✅ Larger batch sizes (8K vs 256)
✅ More training data and longer training</code></pre>
<p><strong>Result:</strong> Significant improvements across all benchmarks with no architectural changes!</p>
</section>
</section>
<section id="electra-the-efficient-alternative" class="level3">
<h3 class="anchored" data-anchor-id="electra-the-efficient-alternative">ELECTRA: The Efficient Alternative</h3>
<section id="the-clever-training-trick" class="level4">
<h4 class="anchored" data-anchor-id="the-clever-training-trick">The Clever Training Trick</h4>
<pre><code>Problem: BERT only learns from 15% of tokens (the masked ones)
ELECTRA idea: Learn from ALL tokens!

How:
1. Small "generator" model replaces some tokens
2. Main "discriminator" model detects which tokens are fake
3. Train both models together

Analogy: Like training a detective (discriminator) with a forger (generator)</code></pre>
<p><strong>Benefits:</strong></p>
<pre><code>✅ More efficient training (learns from all tokens)
✅ Better sample efficiency
✅ Competitive performance with less compute</code></pre>
<hr>
</section>
</section>
</section>
<section id="encoder-decoder-models-the-transformation-masters" class="level2">
<h2 class="anchored" data-anchor-id="encoder-decoder-models-the-transformation-masters">5.4 Encoder-Decoder Models: The Transformation Masters 🔄</h2>
<section id="t5-text-to-text-transfer-transformer" class="level3">
<h3 class="anchored" data-anchor-id="t5-text-to-text-transfer-transformer">T5: Text-to-Text Transfer Transformer</h3>
<section id="the-unifying-philosophy" class="level4">
<h4 class="anchored" data-anchor-id="the-unifying-philosophy">The Unifying Philosophy</h4>
<pre><code>Revolutionary idea: EVERYTHING is text-to-text!

Instead of different model architectures for different tasks:
- Translation: "translate English to French: Hello" → "Bonjour"
- Summarization: "summarize: [long text]" → "[summary]"
- Classification: "sentiment: I love this!" → "positive"</code></pre>
</section>
<section id="span-corruption-training" class="level4">
<h4 class="anchored" data-anchor-id="span-corruption-training">Span Corruption Training</h4>
<pre><code>Instead of masking individual words:
1. Mask random spans of text
2. Replace with special tokens: &lt;extra_id_0&gt;, &lt;extra_id_1&gt;, etc.
3. Predict the masked spans

Input: "The cat &lt;extra_id_0&gt; on the &lt;extra_id_1&gt;"
Target: "&lt;extra_id_0&gt; sat &lt;extra_id_1&gt; mat"</code></pre>
</section>
<section id="why-t5-is-powerful" class="level4">
<h4 class="anchored" data-anchor-id="why-t5-is-powerful">Why T5 is Powerful</h4>
<pre><code>✅ Unified framework for all NLP tasks
✅ Easy to add new tasks (just change the prefix!)
✅ Transfer learning across different tasks
✅ Clean, consistent interface</code></pre>
</section>
</section>
<section id="bart-the-denoising-expert" class="level3">
<h3 class="anchored" data-anchor-id="bart-the-denoising-expert">BART: The Denoising Expert</h3>
<section id="multiple-corruption-strategies" class="level4">
<h4 class="anchored" data-anchor-id="multiple-corruption-strategies">Multiple Corruption Strategies</h4>
<pre><code>BART uses various ways to corrupt text:
1. Token masking: Replace with [MASK]
2. Token deletion: Remove tokens entirely
3. Text infilling: Replace spans with single mask
4. Sentence permutation: Shuffle sentences
5. Document rotation: Rotate to start at random position</code></pre>
<p><strong>Best Recipe:</strong> Text infilling + sentence permutation</p>
</section>
<section id="when-to-use-bart" class="level4">
<h4 class="anchored" data-anchor-id="when-to-use-bart">When to Use BART</h4>
<pre><code>✅ Summarization (especially good at this!)
✅ Text generation with some conditioning
✅ Translation
✅ Question answering</code></pre>
<hr>
</section>
</section>
</section>
<section id="efficiency-innovations-making-transformers-faster" class="level2">
<h2 class="anchored" data-anchor-id="efficiency-innovations-making-transformers-faster">5.5 Efficiency Innovations: Making Transformers Faster 🏃‍♂️</h2>
<section id="the-quadratic-problem" class="level3">
<h3 class="anchored" data-anchor-id="the-quadratic-problem">The Quadratic Problem</h3>
<section id="why-standard-attention-is-expensive" class="level4">
<h4 class="anchored" data-anchor-id="why-standard-attention-is-expensive">Why Standard Attention is Expensive</h4>
<pre><code>For sequence length n:
- Attention matrix: n × n
- Memory: O(n²)
- Computation: O(n²)

Examples:
- 1K tokens: 1M attention weights
- 10K tokens: 100M attention weights
- 100K tokens: 10B attention weights!

This gets expensive FAST! 💸</code></pre>
</section>
</section>
<section id="flashattention-the-memory-magician" class="level3">
<h3 class="anchored" data-anchor-id="flashattention-the-memory-magician">FlashAttention: The Memory Magician 🎩</h3>
<section id="the-key-insight" class="level4">
<h4 class="anchored" data-anchor-id="the-key-insight">The Key Insight</h4>
<pre><code>Problem: Standard attention loads entire attention matrix into memory
Solution: Compute attention in chunks that fit in fast memory (SRAM)

Analogy: Instead of printing an entire book and then reading it,
read and process one chapter at a time.</code></pre>
</section>
<section id="how-flashattention-works" class="level4">
<h4 class="anchored" data-anchor-id="how-flashattention-works">How FlashAttention Works</h4>
<pre><code>1. Divide Q, K, V into blocks
2. Compute attention for each block pair
3. Use clever math to combine results correctly
4. Never store the full attention matrix!

Result: Same results, but 2-4x less memory and 2-4x faster!</code></pre>
</section>
<section id="flashattention-benefits" class="level4">
<h4 class="anchored" data-anchor-id="flashattention-benefits">FlashAttention Benefits</h4>
<pre><code>✅ Exact attention (no approximation!)
✅ 2-4x memory reduction
✅ 2-4x speed improvement
✅ Enables much longer sequences
✅ Just a drop-in replacement</code></pre>
</section>
</section>
<section id="pagedattention-virtual-memory-for-ai" class="level3">
<h3 class="anchored" data-anchor-id="pagedattention-virtual-memory-for-ai">PagedAttention: Virtual Memory for AI 💾</h3>
<section id="the-innovation" class="level4">
<h4 class="anchored" data-anchor-id="the-innovation">The Innovation</h4>
<pre><code>Inspired by virtual memory in operating systems:
- Store attention cache in non-contiguous blocks
- Allocate memory dynamically as needed
- Share memory between similar requests

Analogy: Like how your computer manages memory for different programs</code></pre>
</section>
<section id="benefits-for-serving" class="level4">
<h4 class="anchored" data-anchor-id="benefits-for-serving">Benefits for Serving</h4>
<pre><code>✅ Reduces memory fragmentation
✅ Better batching efficiency
✅ Can handle variable-length sequences
✅ Up to 2.2x throughput improvement</code></pre>
</section>
</section>
<section id="grouped-query-attention-gqa-smart-sharing" class="level3">
<h3 class="anchored" data-anchor-id="grouped-query-attention-gqa-smart-sharing">Grouped Query Attention (GQA): Smart Sharing 🤝</h3>
<section id="the-insight" class="level4">
<h4 class="anchored" data-anchor-id="the-insight">The Insight</h4>
<pre><code>In multi-head attention:
- Each head has its own Q, K, V matrices
- But maybe multiple heads can share K and V!

Configurations:
- Multi-Head Attention: 8 Q, 8 K, 8 V
- Grouped Query Attention: 8 Q, 2 K, 2 V  
- Multi-Query Attention: 8 Q, 1 K, 1 V</code></pre>
</section>
<section id="benefits" class="level4">
<h4 class="anchored" data-anchor-id="benefits">Benefits</h4>
<pre><code>✅ Reduced memory usage during inference
✅ Faster generation (less memory bandwidth)
✅ Minimal quality loss
✅ Especially important for large models</code></pre>
<hr>
</section>
</section>
</section>
<section id="architectural-innovations-the-latest-tricks" class="level2">
<h2 class="anchored" data-anchor-id="architectural-innovations-the-latest-tricks">5.6 Architectural Innovations: The Latest Tricks 🔧</h2>
<section id="mixture-of-experts-moe-specialization-at-scale" class="level3">
<h3 class="anchored" data-anchor-id="mixture-of-experts-moe-specialization-at-scale">Mixture of Experts (MoE): Specialization at Scale</h3>
<section id="the-core-concept" class="level4">
<h4 class="anchored" data-anchor-id="the-core-concept">The Core Concept</h4>
<pre><code>Instead of one big model:
Have many specialized expert models!

For each input:
1. Router decides which experts to use
2. Only activate chosen experts (usually 1-2)
3. Combine expert outputs

Analogy: Like having different specialists (doctors, lawyers, engineers)
and consulting only the relevant ones for each question.</code></pre>
</section>
<section id="switch-transformer-googles-moe" class="level4">
<h4 class="anchored" data-anchor-id="switch-transformer-googles-moe">Switch Transformer: Google’s MoE</h4>
<pre><code>Innovation: Route each token to exactly one expert

Benefits:
✅ Simpler routing
✅ Better load balancing
✅ Easier to implement
✅ Scales to thousands of experts</code></pre>
</section>
<section id="glam-efficiently-scaling-moe" class="level4">
<h4 class="anchored" data-anchor-id="glam-efficiently-scaling-moe">GLaM: Efficiently Scaling MoE</h4>
<pre><code>GLaM showed that MoE models can:
✅ Outperform dense models with less compute
✅ Scale to trillions of parameters efficiently
✅ Maintain quality while being more efficient</code></pre>
</section>
</section>
<section id="alternative-attention-mechanisms" class="level3">
<h3 class="anchored" data-anchor-id="alternative-attention-mechanisms">Alternative Attention Mechanisms</h3>
<section id="linear-attention-on-complexity" class="level4">
<h4 class="anchored" data-anchor-id="linear-attention-on-complexity">Linear Attention: O(n) Complexity</h4>
<pre><code>Problem: Standard attention is O(n²)
Goal: Achieve O(n) complexity

Methods:
- Kernel methods (approximate softmax with kernels)
- Random feature maps
- Structured attention patterns

Trade-off: Efficiency vs. expressiveness</code></pre>
</section>
<section id="sparse-attention-patterns" class="level4">
<h4 class="anchored" data-anchor-id="sparse-attention-patterns">Sparse Attention Patterns</h4>
<pre><code>Instead of attending to all positions:
- Local attention: Only nearby positions
- Strided attention: Every k-th position
- Random attention: Random subset of positions

Examples:
- Longformer: Local + global attention
- BigBird: Local + global + random</code></pre>
</section>
</section>
<section id="rmsnorm-simplifying-normalization" class="level3">
<h3 class="anchored" data-anchor-id="rmsnorm-simplifying-normalization">RMSNorm: Simplifying Normalization</h3>
<section id="the-simplification" class="level4">
<h4 class="anchored" data-anchor-id="the-simplification">The Simplification</h4>
<pre><code>LayerNorm: Normalize using mean and variance
RMSNorm: Only use RMS (Root Mean Square)

Formula:
RMSNorm(x) = x / RMS(x) * scale

Where RMS(x) = sqrt(mean(x²))</code></pre>
</section>
<section id="why-it-works" class="level4">
<h4 class="anchored" data-anchor-id="why-it-works">Why It Works</h4>
<pre><code>✅ Simpler computation (no mean calculation)
✅ Better numerical stability
✅ Slightly faster
✅ Same performance as LayerNorm
✅ Used in many modern models (LLaMA, PaLM)</code></pre>
<hr>
</section>
</section>
</section>
<section id="choosing-the-right-architecture" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-right-architecture">5.7 Choosing the Right Architecture 🎯</h2>
<section id="decision-framework" class="level3">
<h3 class="anchored" data-anchor-id="decision-framework">Decision Framework</h3>
<section id="task-based-selection" class="level4">
<h4 class="anchored" data-anchor-id="task-based-selection">Task-Based Selection</h4>
<pre><code>Text Understanding Tasks:
✅ Classification → Encoder-only (BERT-style)
✅ Question Answering → Encoder-only
✅ Information Extraction → Encoder-only

Text Generation Tasks:
✅ Creative Writing → Decoder-only (GPT-style)
✅ Code Generation → Decoder-only
✅ Chatbots → Decoder-only

Text Transformation Tasks:
✅ Translation → Encoder-decoder (T5-style)
✅ Summarization → Encoder-decoder or Decoder-only
✅ Data-to-text → Encoder-decoder</code></pre>
</section>
<section id="practical-considerations" class="level4">
<h4 class="anchored" data-anchor-id="practical-considerations">Practical Considerations</h4>
<pre><code>Computational Budget:
- Limited compute → Smaller models with efficiency tricks
- Abundant compute → Larger models

Deployment Constraints:
- Real-time inference → Optimized models (distilled, quantized)
- Batch processing → Standard models

Data Availability:
- Lots of task-specific data → Fine-tuning works well
- Limited data → Use large pre-trained models with prompting</code></pre>
</section>
</section>
<section id="modern-trends" class="level3">
<h3 class="anchored" data-anchor-id="modern-trends">Modern Trends</h3>
<section id="the-decoder-only-dominance" class="level4">
<h4 class="anchored" data-anchor-id="the-decoder-only-dominance">The Decoder-Only Dominance</h4>
<pre><code>Why decoder-only models are winning:
✅ Simpler architecture (easier to scale)
✅ Unified training objective
✅ Great few-shot learning capabilities
✅ Can handle many tasks with prompting
✅ Easier to serve (single model for many tasks)

Examples: GPT-4, ChatGPT, Claude, LLaMA, PaLM</code></pre>
</section>
<section id="the-scale-vs.-efficiency-trade-off" class="level4">
<h4 class="anchored" data-anchor-id="the-scale-vs.-efficiency-trade-off">The Scale vs.&nbsp;Efficiency Trade-off</h4>
<pre><code>Two competing trends:

Scaling Up:
- Bigger models, more parameters
- Examples: GPT-4, PaLM-2

Scaling Smart:
- Efficient architectures, better training
- Examples: Chinchilla, LLaMA, Mistral

Current winner: Scaling smart! 🏆</code></pre>
<hr>
</section>
</section>
</section>
<section id="real-world-examples" class="level2">
<h2 class="anchored" data-anchor-id="real-world-examples">Real-World Examples 🌍</h2>
<section id="case-study-1-building-a-chatbot" class="level3">
<h3 class="anchored" data-anchor-id="case-study-1-building-a-chatbot">Case Study 1: Building a Chatbot</h3>
<pre><code>Requirements:
- Conversational AI
- Multiple topics
- Real-time responses

Choice: Decoder-only model (GPT-style)
Reasoning:
✅ Natural conversation flow
✅ Can handle diverse topics
✅ Good few-shot learning
✅ Single model for all conversations

Example: ChatGPT, Claude</code></pre>
</section>
<section id="case-study-2-document-classification" class="level3">
<h3 class="anchored" data-anchor-id="case-study-2-document-classification">Case Study 2: Document Classification</h3>
<pre><code>Requirements:
- Classify research papers by topic
- High accuracy needed
- Fixed input format

Choice: Encoder-only model (BERT-style)
Reasoning:
✅ Bidirectional context for understanding
✅ Good at classification tasks
✅ Can fine-tune for specific domains
✅ Efficient for classification

Example: SciBERT for scientific papers</code></pre>
</section>
<section id="case-study-3-content-summarization" class="level3">
<h3 class="anchored" data-anchor-id="case-study-3-content-summarization">Case Study 3: Content Summarization</h3>
<pre><code>Requirements:
- Summarize news articles
- Preserve key information
- Controlled output length

Choice: Encoder-decoder (T5-style) or large decoder-only
Reasoning:
✅ Input-output transformation
✅ Can control output length
✅ Good at preserving key information

Example: BART, T5, or prompted GPT-4</code></pre>
<hr>
</section>
</section>
<section id="common-misconceptions" class="level2">
<h2 class="anchored" data-anchor-id="common-misconceptions">Common Misconceptions 🚫</h2>
<section id="bigger-is-always-better" class="level3">
<h3 class="anchored" data-anchor-id="bigger-is-always-better">“Bigger is Always Better”</h3>
<pre><code>Reality: Quality matters more than size!

LLaMA 13B often outperforms GPT-3 175B because:
✅ Better training data
✅ More efficient architecture
✅ Longer training time
✅ Better hyperparameters</code></pre>
</section>
<section id="you-need-the-latest-architecture" class="level3">
<h3 class="anchored" data-anchor-id="you-need-the-latest-architecture">“You Need the Latest Architecture”</h3>
<pre><code>Reality: Well-trained older architectures can outperform poorly trained new ones!

BERT (2018) with good training can still beat many newer models on classification tasks.</code></pre>
</section>
<section id="all-tasks-need-huge-models" class="level3">
<h3 class="anchored" data-anchor-id="all-tasks-need-huge-models">“All Tasks Need Huge Models”</h3>
<pre><code>Reality: Task complexity determines model size needs!

Simple tasks: DistilBERT (66M parameters) might be enough
Complex reasoning: GPT-4 (1T+ parameters) might be needed</code></pre>
<hr>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways 🎯</h2>
<ol type="1">
<li><p><strong>Different architectures excel at different tasks</strong> - encoder-only for understanding, decoder-only for generation, encoder-decoder for transformation</p></li>
<li><p><strong>Efficiency innovations</strong> like FlashAttention and GQA make transformers more practical for real-world deployment</p></li>
<li><p><strong>The trend toward decoder-only models</strong> reflects their versatility and effectiveness across diverse tasks</p></li>
<li><p><strong>Smart scaling</strong> (better data, training, architecture) often beats pure parameter scaling</p></li>
<li><p><strong>Choose based on your specific needs</strong> - consider task type, computational budget, and deployment constraints</p></li>
</ol>
<hr>
</section>
<section id="fun-exercises" class="level2">
<h2 class="anchored" data-anchor-id="fun-exercises">Fun Exercises 🎮</h2>
<section id="exercise-1-architecture-selection" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1-architecture-selection">Exercise 1: Architecture Selection</h3>
<pre><code>For each task, choose the best architecture and explain why:

a) Email spam detection
b) Poetry generation  
c) Language translation
d) Code completion
e) Sentiment analysis of tweets

Hint: Think about whether you need understanding or generation!</code></pre>
</section>
<section id="exercise-2-efficiency-analysis" class="level3">
<h3 class="anchored" data-anchor-id="exercise-2-efficiency-analysis">Exercise 2: Efficiency Analysis</h3>
<pre><code>You have a 1000-token sequence:
- Standard attention: How many attention weights?
- With sliding window (W=100): How many attention weights?
- With sparse attention (every 10th position): How many weights?

Calculate the memory savings!</code></pre>
</section>
<section id="exercise-3-model-comparison" class="level3">
<h3 class="anchored" data-anchor-id="exercise-3-model-comparison">Exercise 3: Model Comparison</h3>
<pre><code>Compare these for a chatbot application:
- BERT-Large (bidirectional, 340M params)
- GPT-3.5 (autoregressive, 175B params)  
- T5-Large (encoder-decoder, 770M params)

Which would you choose and why?</code></pre>
<hr>
</section>
</section>
<section id="whats-next" class="level2">
<h2 class="anchored" data-anchor-id="whats-next">What’s Next? 📚</h2>
<p>In Chapter 6, we’ll explore scaling laws and learn how to design optimal model architectures!</p>
<p><strong>Preview:</strong> We’ll discover: - The mathematical relationships that govern model performance - How to find the optimal balance between model size and training data - Why some models punch above their weight - The future of efficient model design</p>
<p>Get ready to understand the science behind scaling AI! 🔬</p>
<hr>
</section>
<section id="final-thought" class="level2">
<h2 class="anchored" data-anchor-id="final-thought">Final Thought 💭</h2>
<pre><code>"The transformer was just the beginning. 
What we're seeing now is the specialization and optimization phase -
like how cars evolved from the Model T to modern vehicles optimized for different purposes.

The best model isn't always the biggest one - 
it's the one that's designed smartly for your specific needs!" 🚗➡️🏎️</code></pre>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>