{"entries":[],"headings":["chapter-5-modern-transformer-variants","what-well-learn-today","the-transformer-family-tree","understanding-the-branches","the-original-transformer-2017-the-ancestor","the-three-main-branches","decoder-only-models-the-generation-champions","gpt-series-the-evolution-story","gpt-1-2018-the-proof-of-concept","gpt-2-2019-the-surprise-star","gpt-3-2020-the-game-changer","gpt-4-2023-the-multimodal-marvel","llama-the-open-source-hero","why-llama-matters","llamas-smart-design-choices","llama-2-the-improved-version","mistral-the-efficiency-expert","sliding-window-attention","mixtral-mixture-of-experts-8","encoder-only-models-the-understanding-specialists","bert-the-bidirectional-breakthrough","the-key-innovation-bidirectional-context","masked-language-modeling-training","berts-superpowers","roberta-bert-done-right","what-facebook-fixed","electra-the-efficient-alternative","the-clever-training-trick","encoder-decoder-models-the-transformation-masters","t5-text-to-text-transfer-transformer","the-unifying-philosophy","span-corruption-training","why-t5-is-powerful","bart-the-denoising-expert","multiple-corruption-strategies","when-to-use-bart","efficiency-innovations-making-transformers-faster","the-quadratic-problem","why-standard-attention-is-expensive","flashattention-the-memory-magician","the-key-insight","how-flashattention-works","flashattention-benefits","pagedattention-virtual-memory-for-ai","the-innovation","benefits-for-serving","grouped-query-attention-gqa-smart-sharing","the-insight","benefits","architectural-innovations-the-latest-tricks","mixture-of-experts-moe-specialization-at-scale","the-core-concept","switch-transformer-googles-moe","glam-efficiently-scaling-moe","alternative-attention-mechanisms","linear-attention-on-complexity","sparse-attention-patterns","rmsnorm-simplifying-normalization","the-simplification","why-it-works","choosing-the-right-architecture","decision-framework","task-based-selection","practical-considerations","modern-trends","the-decoder-only-dominance","the-scale-vs.-efficiency-trade-off","real-world-examples","case-study-1-building-a-chatbot","case-study-2-document-classification","case-study-3-content-summarization","common-misconceptions","bigger-is-always-better","you-need-the-latest-architecture","all-tasks-need-huge-models","key-takeaways","fun-exercises","exercise-1-architecture-selection","exercise-2-efficiency-analysis","exercise-3-model-comparison","whats-next","final-thought"]}