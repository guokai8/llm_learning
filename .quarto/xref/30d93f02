{"entries":[],"headings":["chapter-4-the-transformer-architecture","what-well-learn-today","the-problem-that-started-it-all","the-rnn-limitation","imagine-reading-a-long-book","the-technical-problem","what-we-really-needed","attention-the-game-changer","attention-in-everyday-life","human-attention-analogy","reading-comprehension-example","attention-in-neural-networks","the-core-idea","mathematical-intuition-made-simple","the-attention-formula-dont-panic","step-by-step-breakdown","concrete-example","self-attention-words-talking-to-words","what-is-self-attention","the-revolutionary-idea","analogy-group-discussion","how-self-attention-works","step-1-create-q-k-v-for-each-word","step-2-every-word-attends-to-every-word","step-3-parallel-processing","self-attention-visualization","example-the-cat-sat-on-the-mat","multi-head-attention-multiple-perspectives","why-multiple-heads","the-limitation-of-single-attention","the-multi-head-solution","analogy-multiple-experts","how-multi-head-attention-works","step-1-create-multiple-q-k-v-sets","step-2-parallel-attention-computation","step-3-combine-all-heads","what-different-heads-learn","real-examples-from-research","position-encoding-teaching-order","the-position-problem","self-attention-is-order-blind","why-this-matters","solution-position-encoding","the-big-idea","analogy-house-addresses","sinusoidal-position-encoding-original-transformer","the-formula-visualized","intuitive-understanding","why-sine-and-cosine","modern-alternatives","rope-rotary-position-embedding","alibi-attention-with-linear-biases","the-complete-transformer-architecture","building-blocks-overview","the-transformer-layer-recipe","feed-forward-network-the-thinking-layer","what-it-does","analogy-processing-information","layer-normalization-keeping-things-stable","what-it-does-1","analogy-equalizing-audio","residual-connections-information-highways","the-skip-connection","why-this-matters-1","analogy-highway-with-exits","putting-it-all-together-the-transformer-block","information-flow","stack-multiple-blocks","different-transformer-architectures","encoder-only-bert-style","architecture","when-to-use","training-masked-language-modeling","decoder-only-gpt-style","architecture-1","when-to-use-1","training-causal-language-modeling","encoder-decoder-t5-style","architecture-2","when-to-use-2","training-text-to-text","architecture-comparison","why-transformers-were-revolutionary","before-transformers-the-struggles","rnn-problems","cnn-problems-for-nlp","transformer-breakthroughs","parallelization","direct-connections","scalability","transfer-learning","the-impact","what-transformers-enabled","the-scaling-era","practical-insights","when-to-use-which-architecture","quick-decision-guide","training-considerations","memory-requirements","compute-requirements","common-student-questions","q-why-is-attention-better-than-rnns","q-how-many-attention-heads-should-i-use","q-whats-the-difference-between-bert-and-gpt","q-why-do-we-need-position-encoding","q-is-the-transformer-perfect","key-takeaways","fun-exercises","exercise-1-attention-visualization","exercise-2-architecture-choice","exercise-3-position-encoding","whats-next","final-thought"]}