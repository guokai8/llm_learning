{"entries":[],"headings":["chapter-3-natural-language-processing-fundamentals","what-well-learn-today","tokenization-chopping-up-text","the-fundamental-challenge","the-problem","what-is-a-token","method-1-word-level-tokenization","how-it-works","analogy-cutting-a-sentence-like-a-pizza","advantages","problems","method-2-character-level-tokenization","how-it-works-1","analogy-reading-letter-by-letter","advantages-1","problems-1","method-3-subword-tokenization-the-sweet-spot","the-big-idea","analogy-smart-text-compression","byte-pair-encoding-bpe-the-most-popular-method","the-algorithm-step-by-step","why-bpe-is-brilliant","other-subword-methods","wordpiece-used-by-bert","sentencepiece-used-by-t5-many-modern-models","practical-example-how-gpt-tokenizes","choosing-vocabulary-size-the-trade-off","word-embeddings-from-words-to-vectors","the-core-problem","how-do-we-represent-words-to-computers","attempt-1-one-hot-encoding-the-naive-approach","how-it-works-2","problems-with-one-hot","the-distributional-hypothesis","the-key-insight","examples","word2vec-the-breakthrough","two-flavors-cbow-vs-skip-gram","skip-gram-in-detail-more-popular","the-magic-result","famous-word2vec-results","glove-global-vectors","the-motivation","how-glove-works","glove-vs-word2vec","fasttext-handling-rare-words","the-innovation","example","benefits","evaluating-word-embeddings","intrinsic-evaluation","extrinsic-evaluation","language-modeling-predicting-what-comes-next","what-is-language-modeling","the-core-task","why-this-matters","n-gram-language-models-the-classical-approach","the-markov-assumption","bigram-model-n2","training-n-gram-models","problems-with-n-gram-models","neural-language-models-the-revolution","the-big-idea-1","feed-forward-neural-language-model","recurrent-neural-language-models","modern-language-models-transformers","the-transformer-revolution","autoregressive-generation","evaluation-metrics-how-good-is-our-model","intrinsic-evaluation-perplexity","what-is-perplexity","mathematical-definition","intuitive-example","extrinsic-evaluation-downstream-tasks","text-generation-quality","human-evaluation","the-evaluation-challenge","why-evaluation-is-hard","current-best-practices","putting-it-all-together-the-nlp-pipeline","from-raw-text-to-model-predictions","step-1-text-preprocessing","step-2-tokenization","step-3-vocabulary-creation","step-4-embedding","step-5-model-processing","step-6-decoding","common-student-questions","q-why-do-we-need-so-many-different-tokenization-methods","q-how-do-embeddings-actually-capture-meaning","q-why-is-language-modeling-so-important","q-which-evaluation-metric-should-i-use","key-takeaways","fun-exercises","exercise-1-tokenization-practice","exercise-2-embedding-intuition","exercise-3-language-modeling","whats-next","final-thought"]}