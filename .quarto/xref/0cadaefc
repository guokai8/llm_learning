{"entries":[],"headings":["chapter-16-evaluation-and-benchmarking","what-well-learn-today","the-evaluation-challenge-more-than-just-accuracy","why-llm-evaluation-is-hard","traditional-ml-vs.-llm-evaluation","the-multidimensional-nature-of-quality","the-moving-target-problem","the-benchmark-ecosystem","popular-llm-benchmarks","the-benchmark-lifecycle","traditional-benchmarking-approaches","multiple-choice-and-classification-tasks","mmlu-the-knowledge-test","the-prompt-engineering-problem","reading-comprehension-and-reasoning","squad-reading-comprehension","hellaswag-common-sense-reasoning","mathematical-and-logical-reasoning","gsm8k-grade-school-math","the-chain-of-thought-revolution","human-evaluation-the-gold-standard","why-human-evaluation-matters","limitations-of-automatic-metrics","human-evaluation-advantages","human-evaluation-methods","pairwise-comparison","likert-scale-rating","task-specific-evaluation","challenges-with-human-evaluation","inter-annotator-agreement","bias-and-subjectivity","scale-and-cost","emerging-evaluation-frameworks","llm-as-a-judge","using-ai-to-evaluate-ai","gpt-4-as-universal-judge","constitutional-ai-evaluation","principle-based-assessment","real-world-performance-metrics","user-engagement-and-satisfaction","ab-testing-in-production","specialized-evaluation-domains","safety-and-alignment-evaluation","red-team-testing","bias-and-fairness-testing","multimodal-evaluation","vision-language-assessment","code-generation-evaluation","functional-correctness","real-world-code-quality","evaluation-best-practices-and-pitfalls","common-evaluation-mistakes","data-contamination","gaming-and-overfitting","statistical-significance","designing-good-evaluations","alignment-with-use-cases","comprehensive-coverage","continuous-evaluation","building-evaluation-systems","evaluation-infrastructure","automated-evaluation-pipelines","human-evaluation-platforms","evaluation-metrics-and-analysis","metric-selection-guidelines","statistical-analysis","the-future-of-ai-evaluation","emerging-evaluation-paradigms","evaluation-for-agi","embodied-ai-evaluation","continuous-learning-evaluation","evaluating-learning-ability","meta-evaluation","evaluating-the-evaluations","real-world-case-studies","case-study-1-openais-model-evaluation","gpt-4-technical-report-evaluation","case-study-2-anthropics-constitutional-ai-evaluation","principle-based-evaluation-framework","case-study-3-academia-vs.-industry-evaluation","different-evaluation-cultures","key-takeaways","fun-exercises","exercise-1-benchmark-design-challenge","exercise-2-evaluation-method-comparison","exercise-3-bias-detection-design","whats-next","final-thought"]}