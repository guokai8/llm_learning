{"title":"Chapter 5: Modern Transformer Variants","markdown":{"headingText":"Chapter 5: Modern Transformer Variants","containsRefs":false,"markdown":"*From BERT to GPT to LLaMA: The Family Tree of Modern AI*\n\n## What We'll Learn Today 🎯\n- How different transformers are like different tools for different jobs\n- Why GPT became the \"Swiss Army knife\" of AI\n- Efficiency tricks that make transformers faster and cheaper\n- The latest architectural innovations (explained simply!)\n- How to choose the right transformer for your task\n\n**Big Idea:** The original transformer was just the beginning - now we have specialized variants optimized for different purposes!\n\n---\n\n## 5.1 The Transformer Family Tree 🌳\n\n### Understanding the Branches\n\n#### The Original Transformer (2017): The Ancestor\n```\n\"Attention Is All You Need\" paper introduced:\n✅ Encoder-decoder architecture\n✅ Self-attention mechanism\n✅ Multi-head attention\n✅ Position encoding\n\nBut it was just for translation tasks!\n```\n\n#### The Three Main Branches\n```\n                    Original Transformer\n                           |\n        ┌─────────────────┼─────────────────┐\n        |                 |                 |\n   Encoder-Only      Decoder-Only     Encoder-Decoder\n   (Understanding)   (Generation)     (Transformation)\n        |                 |                 |\n      BERT              GPT               T5\n    (2018)            (2018)            (2019)\n```\n\n---\n\n## 5.2 Decoder-Only Models: The Generation Champions 🎭\n\n### GPT Series: The Evolution Story\n\n#### GPT-1 (2018): The Proof of Concept\n```\nThink of GPT-1 as the first smartphone:\n- It worked, but was pretty basic\n- 117M parameters (tiny by today's standards!)\n- Showed that \"pre-train then fine-tune\" works\n- Context length: 512 tokens (about 1 paragraph)\n```\n\n**Key Innovation:**\n```\nInstead of training from scratch for each task:\n1. Pre-train on lots of text (learn language)\n2. Fine-tune on specific task (learn the job)\n\nThis was revolutionary! Like teaching someone general knowledge,\nthen specialized training for specific jobs.\n```\n\n#### GPT-2 (2019): The Surprise Star\n```\nGPT-2 was like discovering your smartphone could also:\n- Take photos, play games, navigate, etc.\n- 1.5B parameters (10x larger!)\n- Context length: 1024 tokens\n- Zero-shot task transfer!\n```\n\n**The \"Zero-Shot\" Breakthrough:**\n```\nAmazing discovery: You could give GPT-2 examples and it would learn!\n\nExample:\nInput: \"French: Bonjour, English: Hello, French: Au revoir, English:\"\nOutput: \"Goodbye\"\n\nNo fine-tuning needed! It just learned from the pattern!\n```\n\n**Fun Fact:** OpenAI initially didn't release GPT-2 because they worried about misuse!\n\n#### GPT-3 (2020): The Game Changer 🚀\n```\nGPT-3 was like upgrading from a smartphone to a supercomputer:\n- 175B parameters (100x larger than GPT-2!)\n- Context length: 2048 tokens (later extended)\n- Emergent abilities nobody expected!\n```\n\n**Emergent Abilities (Nobody Taught It These!):**\n```\n✨ Code generation: \"Write a Python function to sort a list\"\n✨ Math reasoning: \"If I have 3 apples and buy 2 more...\"\n✨ Creative writing: \"Write a poem about AI\"\n✨ Language translation: Even for languages barely in training data!\n✨ Few-shot learning: Learn tasks from just a few examples\n```\n\n**In-Context Learning:**\n```\nThe magic of GPT-3: It learns from the conversation itself!\n\nYou: \"Translate these examples: cat→gato, dog→perro, bird→?\"\nGPT-3: \"pájaro\"\n\nIt figured out the pattern without any training updates!\n```\n\n#### GPT-4 (2023): The Multimodal Marvel\n```\nGPT-4 is like having a super-intelligent assistant that can:\n- Read text and see images\n- Reason about complex problems\n- Write code that actually works\n- Pass professional exams (bar exam, medical exams!)\n```\n\n**Architecture Secrets (Rumored):**\n```\n🤔 Mixture of Experts (MoE) - multiple specialized sub-models\n🤔 Multimodal training - text and images together\n🤔 Better reasoning through more compute at inference time\n🤔 Advanced safety training and alignment\n```\n\n### LLaMA: The Open Source Hero 🦙\n\n#### Why LLaMA Matters\n```\nProblem: GPT models are closed source and expensive\nSolution: Meta releases LLaMA - high-quality, open research models!\n\nImpact: Sparked an entire ecosystem of open-source models\n```\n\n#### LLaMA's Smart Design Choices\n\n**1. RMSNorm Instead of LayerNorm**\n```\nLayerNorm: Normalize using mean AND variance\nRMSNorm: Normalize using only RMS (root mean square)\n\nBenefits:\n✅ Simpler computation\n✅ Slightly faster\n✅ Better numerical stability\n✅ Same performance\n```\n\n**2. SwiGLU Activation Function**\n```\nTraditional: ReLU or GELU\nLLaMA: SwiGLU (Swish-Gated Linear Unit)\n\nThink of it as a \"smart gate\" that decides what information to pass through:\nSwiGLU(x) = Swish(x·W₁) ⊙ (x·W₂)\n\nBenefits: Better performance, especially for large models\n```\n\n**3. RoPE Position Encoding**\n```\nProblem: Sinusoidal encoding doesn't extrapolate well to longer sequences\nSolution: RoPE (Rotary Position Embedding)\n\nAnalogy: Instead of adding position info, \"rotate\" the embeddings\nResult: Much better handling of long sequences!\n```\n\n#### LLaMA 2: The Improved Version\n```\nImprovements over LLaMA 1:\n✅ Longer context (4K tokens vs 2K)\n✅ Better training data curation\n✅ Grouped Query Attention (more efficient)\n✅ RLHF training (more helpful and safe)\n```\n\n### Mistral: The Efficiency Expert ⚡\n\n#### Sliding Window Attention\n```\nProblem: Full attention is O(n²) - very expensive for long sequences\nMistral's solution: Each token only attends to the last W tokens\n\nAnalogy: Instead of remembering everything from the entire conversation,\nonly remember the last few sentences clearly.\n\nBenefits:\n✅ Linear memory usage\n✅ Faster inference\n✅ Can still capture long-range dependencies through layer stacking\n```\n\n#### Mixtral: Mixture of Experts 🧠×8\n```\nRevolutionary idea: Instead of one big brain, have 8 specialized brains!\n\nFor each token:\n1. Router decides which 2 experts to use\n2. Only activate those 2 experts\n3. Combine their outputs\n\nResult: 8×7B model that only uses 2×7B computation per token!\n```\n\n---\n\n## 5.3 Encoder-Only Models: The Understanding Specialists 🔍\n\n### BERT: The Bidirectional Breakthrough\n\n#### The Key Innovation: Bidirectional Context\n```\nPrevious models: Read left-to-right (like humans reading)\nBERT: Read in both directions simultaneously!\n\nSentence: \"The cat sat on the mat\"\nTraditional: When processing \"cat\", only sees \"The\"\nBERT: When processing \"cat\", sees \"The\" AND \"sat on the mat\"\n```\n\n#### Masked Language Modeling Training\n```\nTraining trick: Randomly mask words and predict them\n\nInput: \"The [MASK] sat on the mat\"\nBERT's job: Figure out the masked word is \"cat\"\n\nWhy this works: Forces model to use context from both sides!\n```\n\n**Masking Strategy:**\n```\nFor 15% of tokens:\n- 80% replace with [MASK]: \"The [MASK] sat\"\n- 10% replace with random word: \"The dog sat\" \n- 10% keep original: \"The cat sat\"\n\nWhy the variety? Prevents model from just memorizing that [MASK] means \"predict this\"\n```\n\n#### BERT's Superpowers\n```\n✅ Reading comprehension: Understands context deeply\n✅ Question answering: Can find answers in passages\n✅ Sentiment analysis: Understands emotional tone\n✅ Named entity recognition: Identifies people, places, things\n✅ Text classification: Categorizes documents\n```\n\n### RoBERTa: BERT Done Right\n\n#### What Facebook Fixed\n```\nOriginal BERT had some questionable choices:\n❌ Next Sentence Prediction (turned out useless)\n❌ Static masking (same mask every epoch)\n❌ Small batch sizes\n❌ Short training\n\nRoBERTa improvements:\n✅ Removed Next Sentence Prediction\n✅ Dynamic masking (different mask each epoch)\n✅ Larger batch sizes (8K vs 256)\n✅ More training data and longer training\n```\n\n**Result:** Significant improvements across all benchmarks with no architectural changes!\n\n### ELECTRA: The Efficient Alternative\n\n#### The Clever Training Trick\n```\nProblem: BERT only learns from 15% of tokens (the masked ones)\nELECTRA idea: Learn from ALL tokens!\n\nHow:\n1. Small \"generator\" model replaces some tokens\n2. Main \"discriminator\" model detects which tokens are fake\n3. Train both models together\n\nAnalogy: Like training a detective (discriminator) with a forger (generator)\n```\n\n**Benefits:**\n```\n✅ More efficient training (learns from all tokens)\n✅ Better sample efficiency\n✅ Competitive performance with less compute\n```\n\n---\n\n## 5.4 Encoder-Decoder Models: The Transformation Masters 🔄\n\n### T5: Text-to-Text Transfer Transformer\n\n#### The Unifying Philosophy\n```\nRevolutionary idea: EVERYTHING is text-to-text!\n\nInstead of different model architectures for different tasks:\n- Translation: \"translate English to French: Hello\" → \"Bonjour\"\n- Summarization: \"summarize: [long text]\" → \"[summary]\"\n- Classification: \"sentiment: I love this!\" → \"positive\"\n```\n\n#### Span Corruption Training\n```\nInstead of masking individual words:\n1. Mask random spans of text\n2. Replace with special tokens: <extra_id_0>, <extra_id_1>, etc.\n3. Predict the masked spans\n\nInput: \"The cat <extra_id_0> on the <extra_id_1>\"\nTarget: \"<extra_id_0> sat <extra_id_1> mat\"\n```\n\n#### Why T5 is Powerful\n```\n✅ Unified framework for all NLP tasks\n✅ Easy to add new tasks (just change the prefix!)\n✅ Transfer learning across different tasks\n✅ Clean, consistent interface\n```\n\n### BART: The Denoising Expert\n\n#### Multiple Corruption Strategies\n```\nBART uses various ways to corrupt text:\n1. Token masking: Replace with [MASK]\n2. Token deletion: Remove tokens entirely\n3. Text infilling: Replace spans with single mask\n4. Sentence permutation: Shuffle sentences\n5. Document rotation: Rotate to start at random position\n```\n\n**Best Recipe:** Text infilling + sentence permutation\n\n#### When to Use BART\n```\n✅ Summarization (especially good at this!)\n✅ Text generation with some conditioning\n✅ Translation\n✅ Question answering\n```\n\n---\n\n## 5.5 Efficiency Innovations: Making Transformers Faster 🏃‍♂️\n\n### The Quadratic Problem\n\n#### Why Standard Attention is Expensive\n```\nFor sequence length n:\n- Attention matrix: n × n\n- Memory: O(n²)\n- Computation: O(n²)\n\nExamples:\n- 1K tokens: 1M attention weights\n- 10K tokens: 100M attention weights\n- 100K tokens: 10B attention weights!\n\nThis gets expensive FAST! 💸\n```\n\n### FlashAttention: The Memory Magician 🎩\n\n#### The Key Insight\n```\nProblem: Standard attention loads entire attention matrix into memory\nSolution: Compute attention in chunks that fit in fast memory (SRAM)\n\nAnalogy: Instead of printing an entire book and then reading it,\nread and process one chapter at a time.\n```\n\n#### How FlashAttention Works\n```\n1. Divide Q, K, V into blocks\n2. Compute attention for each block pair\n3. Use clever math to combine results correctly\n4. Never store the full attention matrix!\n\nResult: Same results, but 2-4x less memory and 2-4x faster!\n```\n\n#### FlashAttention Benefits\n```\n✅ Exact attention (no approximation!)\n✅ 2-4x memory reduction\n✅ 2-4x speed improvement\n✅ Enables much longer sequences\n✅ Just a drop-in replacement\n```\n\n### PagedAttention: Virtual Memory for AI 💾\n\n#### The Innovation\n```\nInspired by virtual memory in operating systems:\n- Store attention cache in non-contiguous blocks\n- Allocate memory dynamically as needed\n- Share memory between similar requests\n\nAnalogy: Like how your computer manages memory for different programs\n```\n\n#### Benefits for Serving\n```\n✅ Reduces memory fragmentation\n✅ Better batching efficiency\n✅ Can handle variable-length sequences\n✅ Up to 2.2x throughput improvement\n```\n\n### Grouped Query Attention (GQA): Smart Sharing 🤝\n\n#### The Insight\n```\nIn multi-head attention:\n- Each head has its own Q, K, V matrices\n- But maybe multiple heads can share K and V!\n\nConfigurations:\n- Multi-Head Attention: 8 Q, 8 K, 8 V\n- Grouped Query Attention: 8 Q, 2 K, 2 V  \n- Multi-Query Attention: 8 Q, 1 K, 1 V\n```\n\n#### Benefits\n```\n✅ Reduced memory usage during inference\n✅ Faster generation (less memory bandwidth)\n✅ Minimal quality loss\n✅ Especially important for large models\n```\n\n---\n\n## 5.6 Architectural Innovations: The Latest Tricks 🔧\n\n### Mixture of Experts (MoE): Specialization at Scale\n\n#### The Core Concept\n```\nInstead of one big model:\nHave many specialized expert models!\n\nFor each input:\n1. Router decides which experts to use\n2. Only activate chosen experts (usually 1-2)\n3. Combine expert outputs\n\nAnalogy: Like having different specialists (doctors, lawyers, engineers)\nand consulting only the relevant ones for each question.\n```\n\n#### Switch Transformer: Google's MoE\n```\nInnovation: Route each token to exactly one expert\n\nBenefits:\n✅ Simpler routing\n✅ Better load balancing\n✅ Easier to implement\n✅ Scales to thousands of experts\n```\n\n#### GLaM: Efficiently Scaling MoE\n```\nGLaM showed that MoE models can:\n✅ Outperform dense models with less compute\n✅ Scale to trillions of parameters efficiently\n✅ Maintain quality while being more efficient\n```\n\n### Alternative Attention Mechanisms\n\n#### Linear Attention: O(n) Complexity\n```\nProblem: Standard attention is O(n²)\nGoal: Achieve O(n) complexity\n\nMethods:\n- Kernel methods (approximate softmax with kernels)\n- Random feature maps\n- Structured attention patterns\n\nTrade-off: Efficiency vs. expressiveness\n```\n\n#### Sparse Attention Patterns\n```\nInstead of attending to all positions:\n- Local attention: Only nearby positions\n- Strided attention: Every k-th position\n- Random attention: Random subset of positions\n\nExamples:\n- Longformer: Local + global attention\n- BigBird: Local + global + random\n```\n\n### RMSNorm: Simplifying Normalization\n\n#### The Simplification\n```\nLayerNorm: Normalize using mean and variance\nRMSNorm: Only use RMS (Root Mean Square)\n\nFormula:\nRMSNorm(x) = x / RMS(x) * scale\n\nWhere RMS(x) = sqrt(mean(x²))\n```\n\n#### Why It Works\n```\n✅ Simpler computation (no mean calculation)\n✅ Better numerical stability\n✅ Slightly faster\n✅ Same performance as LayerNorm\n✅ Used in many modern models (LLaMA, PaLM)\n```\n\n---\n\n## 5.7 Choosing the Right Architecture 🎯\n\n### Decision Framework\n\n#### Task-Based Selection\n```\nText Understanding Tasks:\n✅ Classification → Encoder-only (BERT-style)\n✅ Question Answering → Encoder-only\n✅ Information Extraction → Encoder-only\n\nText Generation Tasks:\n✅ Creative Writing → Decoder-only (GPT-style)\n✅ Code Generation → Decoder-only\n✅ Chatbots → Decoder-only\n\nText Transformation Tasks:\n✅ Translation → Encoder-decoder (T5-style)\n✅ Summarization → Encoder-decoder or Decoder-only\n✅ Data-to-text → Encoder-decoder\n```\n\n#### Practical Considerations\n```\nComputational Budget:\n- Limited compute → Smaller models with efficiency tricks\n- Abundant compute → Larger models\n\nDeployment Constraints:\n- Real-time inference → Optimized models (distilled, quantized)\n- Batch processing → Standard models\n\nData Availability:\n- Lots of task-specific data → Fine-tuning works well\n- Limited data → Use large pre-trained models with prompting\n```\n\n### Modern Trends\n\n#### The Decoder-Only Dominance\n```\nWhy decoder-only models are winning:\n✅ Simpler architecture (easier to scale)\n✅ Unified training objective\n✅ Great few-shot learning capabilities\n✅ Can handle many tasks with prompting\n✅ Easier to serve (single model for many tasks)\n\nExamples: GPT-4, ChatGPT, Claude, LLaMA, PaLM\n```\n\n#### The Scale vs. Efficiency Trade-off\n```\nTwo competing trends:\n\nScaling Up:\n- Bigger models, more parameters\n- Examples: GPT-4, PaLM-2\n\nScaling Smart:\n- Efficient architectures, better training\n- Examples: Chinchilla, LLaMA, Mistral\n\nCurrent winner: Scaling smart! 🏆\n```\n\n---\n\n## Real-World Examples 🌍\n\n### Case Study 1: Building a Chatbot\n```\nRequirements:\n- Conversational AI\n- Multiple topics\n- Real-time responses\n\nChoice: Decoder-only model (GPT-style)\nReasoning:\n✅ Natural conversation flow\n✅ Can handle diverse topics\n✅ Good few-shot learning\n✅ Single model for all conversations\n\nExample: ChatGPT, Claude\n```\n\n### Case Study 2: Document Classification\n```\nRequirements:\n- Classify research papers by topic\n- High accuracy needed\n- Fixed input format\n\nChoice: Encoder-only model (BERT-style)\nReasoning:\n✅ Bidirectional context for understanding\n✅ Good at classification tasks\n✅ Can fine-tune for specific domains\n✅ Efficient for classification\n\nExample: SciBERT for scientific papers\n```\n\n### Case Study 3: Content Summarization\n```\nRequirements:\n- Summarize news articles\n- Preserve key information\n- Controlled output length\n\nChoice: Encoder-decoder (T5-style) or large decoder-only\nReasoning:\n✅ Input-output transformation\n✅ Can control output length\n✅ Good at preserving key information\n\nExample: BART, T5, or prompted GPT-4\n```\n\n---\n\n## Common Misconceptions 🚫\n\n### \"Bigger is Always Better\"\n```\nReality: Quality matters more than size!\n\nLLaMA 13B often outperforms GPT-3 175B because:\n✅ Better training data\n✅ More efficient architecture\n✅ Longer training time\n✅ Better hyperparameters\n```\n\n### \"You Need the Latest Architecture\"\n```\nReality: Well-trained older architectures can outperform poorly trained new ones!\n\nBERT (2018) with good training can still beat many newer models on classification tasks.\n```\n\n### \"All Tasks Need Huge Models\"\n```\nReality: Task complexity determines model size needs!\n\nSimple tasks: DistilBERT (66M parameters) might be enough\nComplex reasoning: GPT-4 (1T+ parameters) might be needed\n```\n\n---\n\n## Key Takeaways 🎯\n\n1. **Different architectures excel at different tasks** - encoder-only for understanding, decoder-only for generation, encoder-decoder for transformation\n\n2. **Efficiency innovations** like FlashAttention and GQA make transformers more practical for real-world deployment\n\n3. **The trend toward decoder-only models** reflects their versatility and effectiveness across diverse tasks\n\n4. **Smart scaling** (better data, training, architecture) often beats pure parameter scaling\n\n5. **Choose based on your specific needs** - consider task type, computational budget, and deployment constraints\n\n---\n\n## Fun Exercises 🎮\n\n### Exercise 1: Architecture Selection\n```\nFor each task, choose the best architecture and explain why:\n\na) Email spam detection\nb) Poetry generation  \nc) Language translation\nd) Code completion\ne) Sentiment analysis of tweets\n\nHint: Think about whether you need understanding or generation!\n```\n\n### Exercise 2: Efficiency Analysis\n```\nYou have a 1000-token sequence:\n- Standard attention: How many attention weights?\n- With sliding window (W=100): How many attention weights?\n- With sparse attention (every 10th position): How many weights?\n\nCalculate the memory savings!\n```\n\n### Exercise 3: Model Comparison\n```\nCompare these for a chatbot application:\n- BERT-Large (bidirectional, 340M params)\n- GPT-3.5 (autoregressive, 175B params)  \n- T5-Large (encoder-decoder, 770M params)\n\nWhich would you choose and why?\n```\n\n---\n\n## What's Next? 📚\n\nIn Chapter 6, we'll explore scaling laws and learn how to design optimal model architectures!\n\n**Preview:** We'll discover:\n- The mathematical relationships that govern model performance\n- How to find the optimal balance between model size and training data\n- Why some models punch above their weight\n- The future of efficient model design\n\nGet ready to understand the science behind scaling AI! 🔬\n\n---\n\n## Final Thought 💭\n\n```\n\"The transformer was just the beginning. \nWhat we're seeing now is the specialization and optimization phase -\nlike how cars evolved from the Model T to modern vehicles optimized for different purposes.\n\nThe best model isn't always the biggest one - \nit's the one that's designed smartly for your specific needs!\" 🚗➡️🏎️\n```\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"highlight-style":"github","css":["custom.css"],"output-file":"Chapter_05_Modern_Transformer_Variants_Complete.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"flatly","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}