{"title":"Chapter 13: Optimization and Inference","markdown":{"headingText":"Chapter 13: Optimization and Inference","containsRefs":false,"markdown":"*Making LLMs Fast and Efficient for the Real World*\n\n## What We'll Learn Today üéØ\n- Why raw LLMs are too slow and expensive for production\n- Model compression techniques that maintain quality\n- Inference optimization strategies and frameworks\n- Hardware acceleration and specialized chips\n- Cost optimization and performance tuning\n\n**Key Challenge:** How do you make a 175B parameter model run fast enough for real users? ‚ö°üí∏\n\n---\n\n## 13.1 The Inference Challenge: From Lab to Production üè≠\n\n### The Reality Check\n\n#### Training vs. Inference: Different Worlds\n```\nTraining (Research/Development):\n- Run once, takes days/weeks\n- Cost is amortized over model lifetime\n- Can use massive GPU clusters\n- Accuracy is primary concern\n- Batch processing is fine\n\nInference (Production/Users):\n- Runs millions of times per day\n- Cost per request matters enormously\n- Must respond in milliseconds\n- Need acceptable accuracy AND speed\n- Real-time, interactive responses required\n```\n\n#### The Production Requirements\n```\nUser Expectations:\n‚ö° Latency: < 1 second response time\nüí∞ Cost: Affordable pricing per request  \nüéØ Quality: Good enough accuracy\nüìà Scale: Handle thousands of concurrent users\nüîÑ Reliability: 99.9% uptime\n\nChallenge: Meeting ALL these requirements simultaneously!\n```\n\n### The Computational Bottlenecks\n\n#### Memory Bandwidth: The Hidden Villain\n```\nProblem: Moving data is often slower than computing!\n\nFor GPT-3 (175B parameters):\n- Model size: ~350GB (FP16)\n- GPU memory: 40-80GB per card\n- Need 5-9 GPUs just to store the model\n- Loading parameters takes longer than actual computation\n\nAnalogy: It's like having a brilliant chef (GPU cores) but the ingredients (model weights) are stored in a warehouse across town. Most time is spent fetching ingredients, not cooking! üë®‚Äçüç≥üè™\n```\n\n#### Autoregressive Generation: Sequential Dependency\n```\nProblem: Can't parallelize text generation effectively\n\nTraditional computation: Process all inputs simultaneously\nLLM generation: Must generate one token at a time\n\nExample generation:\nStep 1: Generate \"The\"\nStep 2: Generate \"cat\" (depends on \"The\")\nStep 3: Generate \"sat\" (depends on \"The cat\")\nStep 4: Generate \"on\" (depends on \"The cat sat\")\n...\n\nEach step waits for the previous one! üêå\n```\n\n#### The KV Cache Challenge\n```\nDuring generation, must store Key and Value vectors for all previous tokens:\n\nSequence length: 2048 tokens\nKV cache size: ~1-2GB per sequence\nBatch size: 32 concurrent users\nTotal KV cache: 32-64GB!\n\nMemory grows quadratically with sequence length and batch size! üìà\n```\n\n---\n\n## 13.2 Model Compression: Smaller Models, Same Intelligence üóúÔ∏è\n\n### Quantization: Reducing Precision\n\n#### From 32-bit to 16-bit and Beyond\n```\nFloating Point Precision Levels:\n\nFP32 (32-bit): \n- Range: ¬±3.4 √ó 10^38\n- Precision: ~7 decimal digits\n- Standard for training\n\nFP16 (16-bit):\n- Range: ¬±6.5 √ó 10^4  \n- Precision: ~3 decimal digits\n- Common for inference\n\nINT8 (8-bit):\n- Range: -128 to 127 (or 0 to 255)\n- Much less precision\n- 4x memory savings vs FP32\n\nINT4 (4-bit):\n- Range: -8 to 7 (or 0 to 15)\n- Extreme quantization\n- 8x memory savings vs FP32\n```\n\n#### Why Quantization Works\n```\nKey insight: Neural networks are surprisingly robust to reduced precision!\n\nReasons:\n‚úÖ Many weights are small and contribute little\n‚úÖ Networks learn distributed representations\n‚úÖ Small errors average out across many parameters\n‚úÖ Most information is in the pattern, not exact values\n\nAnalogy: Like compressing a photo - you lose some detail but the image is still recognizable! üì∏\n```\n\n#### Quantization Strategies\n\n**Post-Training Quantization (PTQ)**\n```\nProcess:\n1. Take trained FP32 model\n2. Convert weights to lower precision\n3. Calibrate using representative data\n4. Deploy quantized model\n\nPros: Fast and simple\nCons: Some quality loss, limited to moderate compression\n```\n\n**Quantization-Aware Training (QAT)**\n```\nProcess:\n1. Train model with quantization simulation\n2. Model learns to be robust to quantization noise\n3. Better quality at low precision\n\nPros: Higher quality at extreme quantization\nCons: Requires retraining, more complex\n```\n\n**Dynamic Quantization**\n```\nStrategy: Quantize different parts differently\n- Attention weights: INT8\n- Embedding layers: FP16  \n- Output layer: FP16\n\nBalances compression with quality preservation!\n```\n\n### Pruning: Removing Unnecessary Connections\n\n#### Structured vs. Unstructured Pruning\n```\nUnstructured Pruning:\n- Remove individual weights (set to zero)\n- Sparse matrices with irregular patterns\n- High compression but requires special hardware\n\nStructured Pruning:\n- Remove entire neurons, attention heads, or layers\n- Dense matrices, regular patterns\n- Lower compression but efficient on standard hardware\n```\n\n#### Magnitude-Based Pruning\n```\nSimple strategy: Remove smallest weights\n\nAlgorithm:\n1. Rank all weights by absolute magnitude\n2. Remove bottom X% (e.g., 50%)\n3. Fine-tune remaining weights\n4. Repeat if needed\n\nIntuition: Small weights contribute less to the output\n```\n\n#### Attention Head Pruning\n```\nDiscovery: Many attention heads are redundant!\n\nProcess:\n1. Measure importance of each attention head\n2. Remove least important heads\n3. Fine-tune remaining model\n\nResults: Can often remove 30-50% of heads with minimal quality loss! üéØ\n```\n\n### Knowledge Distillation: Teaching Smaller Models\n\n#### The Teacher-Student Framework\n```\nConcept: Large \"teacher\" model teaches smaller \"student\" model\n\nProcess:\n1. Teacher model generates soft predictions on training data\n2. Student model learns to mimic teacher's outputs\n3. Student captures teacher's knowledge in fewer parameters\n\nBenefits:\n‚úÖ Student often outperforms training from scratch\n‚úÖ Preserves more knowledge than other compression methods\n‚úÖ Can transfer across different architectures\n```\n\n#### Distillation Example\n```\nTeacher: GPT-3 175B parameters\nStudent: GPT-2 sized model (1.5B parameters)\n\nTraining:\n- Input: \"The capital of France is\"\n- Teacher output: [0.95 \"Paris\", 0.03 \"Lyon\", 0.02 others...]\n- Student learns to produce similar probability distribution\n- Much more informative than just \"Paris\" label!\n\nResult: 100x smaller model with 90% of teacher performance! üéì\n```\n\n---\n\n## 13.3 Inference Optimization Techniques ‚ö°\n\n### Batching Strategies\n\n#### Static Batching\n```\nTraditional approach: Process fixed-size batches\n\nChallenges:\n- Sequences have different lengths\n- Padding wastes computation\n- Must wait for longest sequence in batch\n\nExample:\nBatch: [\"Hi\", \"How are you doing today?\", \"What's the weather?\"]\nPadded: [\"Hi\" + padding, \"How are you doing today?\", \"What's the weather?\" + padding]\nWasted computation on padding tokens! ‚ùå\n```\n\n#### Dynamic Batching\n```\nSmart approach: Group sequences by similar length\n\nBenefits:\n‚úÖ Minimal padding waste\n‚úÖ Better GPU utilization\n‚úÖ Higher throughput\n\nImplementation:\n1. Queue incoming requests\n2. Group by sequence length\n3. Process similar-length batches together\n4. Continuous batching as requests arrive\n```\n\n#### Continuous Batching\n```\nAdvanced technique: Add/remove sequences mid-generation\n\nTraditional: Wait for entire batch to complete\nContinuous: As sequences finish, add new ones to the batch\n\nBenefits:\n‚úÖ Higher GPU utilization\n‚úÖ Lower average latency\n‚úÖ Better resource efficiency\n\nExample frameworks: vLLM, TensorRT-LLM\n```\n\n### Attention Optimization\n\n#### FlashAttention: Memory-Efficient Attention\n```\nProblem: Standard attention uses O(n¬≤) memory\nSolution: Compute attention in blocks that fit in fast memory\n\nKey innovations:\n‚úÖ Tiling: Break computation into small blocks\n‚úÖ Recomputation: Compute intermediate values on-demand\n‚úÖ IO awareness: Minimize data movement\n\nResults:\n- 2-4x faster training and inference\n- Enables much longer sequences\n- No approximation - exact same results!\n```\n\n#### Multi-Query Attention (MQA)\n```\nStandard attention: Each head has separate K, V matrices\nMQA: All heads share same K, V matrices\n\nMemory savings:\n- Standard: 8 heads √ó (K + V) = 16 matrices\n- MQA: 8 Q + 1 K + 1 V = 10 matrices\n- ~40% reduction in KV cache size\n\nInference speedup: 1.5-2x faster generation! üöÄ\n```\n\n#### Grouped Query Attention (GQA)\n```\nCompromise between standard and MQA:\n- Group heads into clusters\n- Each group shares K, V matrices\n\nExample: 8 heads ‚Üí 2 groups of 4\n- 8 Q + 2 K + 2 V = 12 matrices\n- Better quality than MQA, still faster than standard\n```\n\n### Speculative Decoding\n\n#### The Core Idea\n```\nProblem: Autoregressive generation is inherently sequential\nSolution: Guess multiple tokens ahead, verify in parallel\n\nProcess:\n1. Small \"draft\" model generates several tokens quickly\n2. Large \"verification\" model checks all tokens in parallel\n3. Accept correct prefixes, discard wrong suffixes\n4. Continue from longest correct prefix\n\nSpeedup: 2-3x faster with same quality! ‚ö°\n```\n\n#### Example Execution\n```\nDraft model (fast): \"The cat sat on the\"\nLarge model (slow): Verifies all 6 tokens in parallel\nResult: [\"The\"‚úì, \"cat\"‚úì, \"sat\"‚úì, \"on\"‚úì, \"the\"‚úì]\nAccept all 5 tokens in one verification step!\n\nVs. traditional: 6 sequential calls to large model\nSpeedup: 6x in this example!\n```\n\n---\n\n## 13.4 Serving Frameworks and Infrastructure üèóÔ∏è\n\n### Popular Serving Frameworks\n\n#### vLLM: High-Throughput Inference\n```\nKey innovations:\n‚úÖ PagedAttention: Efficient KV cache management\n‚úÖ Continuous batching: Dynamic request handling\n‚úÖ Optimized CUDA kernels: Maximum GPU utilization\n\nBenefits:\n- 2-24x higher throughput vs naive implementations\n- Lower latency through smart batching\n- Easy integration with existing code\n\nUse cases: High-traffic production deployments\n```\n\n#### TensorRT-LLM: NVIDIA's Optimized Engine\n```\nFeatures:\n‚úÖ Aggressive kernel fusion and optimization\n‚úÖ Mixed precision inference\n‚úÖ Multi-GPU tensor parallelism\n‚úÖ In-flight batching\n\nPerformance gains:\n- Up to 10x speedup on NVIDIA GPUs\n- Excellent for real-time applications\n- Tight integration with NVIDIA ecosystem\n```\n\n#### Text Generation Inference (TGI): Hugging Face's Solution\n```\nStrengths:\n‚úÖ Easy deployment of Hugging Face models\n‚úÖ Built-in safety features and filtering\n‚úÖ Streaming responses and websocket support\n‚úÖ Prometheus metrics and monitoring\n\nBest for: Rapid prototyping and deployment\n```\n\n### Load Balancing and Scaling\n\n#### Horizontal Scaling Strategies\n```\nSingle Model Replication:\n- Deploy same model on multiple GPUs/nodes\n- Load balancer distributes requests\n- Simple but requires model replication\n\nModel Parallelism:\n- Split single model across multiple GPUs\n- Each GPU handles part of computation\n- More complex but efficient resource usage\n\nPipeline Parallelism:\n- Different layers on different GPUs\n- Requests flow through pipeline\n- Good for very large models\n```\n\n#### Auto-Scaling Patterns\n```\nMetrics to monitor:\n- Request queue length\n- GPU utilization\n- Response latency\n- Cost per request\n\nScaling triggers:\n- Scale up: Queue length > threshold\n- Scale down: Utilization < threshold for X minutes\n- Consider warmup time for new instances\n```\n\n---\n\n## 13.5 Hardware Acceleration üöÄ\n\n### GPU Optimization\n\n#### Memory Hierarchy Understanding\n```\nGPU Memory Types (fastest to slowest):\n1. Registers: Extremely fast, very limited\n2. Shared memory: Fast, limited per block\n3. L1/L2 cache: Automatic caching\n4. Global memory (VRAM): Large but slower\n5. Host memory (RAM): Much slower\n6. Storage: Very slow\n\nOptimization goal: Keep data in faster memory levels!\n```\n\n#### Kernel Fusion\n```\nProblem: Many small operations launch separate kernels\nSolution: Combine operations into single, larger kernels\n\nExample:\nSeparate: LayerNorm ‚Üí Activation ‚Üí Linear\nFused: LayerNorm+Activation+Linear in one kernel\n\nBenefits:\n‚úÖ Reduced memory bandwidth usage\n‚úÖ Lower kernel launch overhead\n‚úÖ Better data locality\n```\n\n### Specialized Hardware\n\n#### TPUs (Tensor Processing Units)\n```\nGoogle's AI chips optimized for:\n‚úÖ Matrix multiplications (AI workloads)\n‚úÖ Low precision arithmetic (INT8, bfloat16)\n‚úÖ High memory bandwidth\n‚úÖ Efficient for training and inference\n\nTrade-offs:\n+ Excellent for standard transformer models\n+ Very cost-effective for large scale\n- Less flexible than GPUs\n- Requires TPU-optimized code\n```\n\n#### Custom AI Chips\n```\nEmerging options:\n- Cerebras wafer-scale engines\n- Graphcore IPUs\n- Intel Habana Gaudi\n- AMD Instinct series\n- Apple M-series Neural Engine\n\nCommon optimizations:\n‚úÖ Mixed precision support\n‚úÖ Sparse computation capabilities\n‚úÖ On-chip memory optimization\n‚úÖ Reduced power consumption\n```\n\n### CPU Inference Optimization\n\n#### When to Use CPU\n```\nGood for:\n‚úÖ Small models (< 1B parameters)\n‚úÖ Low-latency requirements\n‚úÖ Cost-sensitive applications\n‚úÖ Edge deployment\n\nOptimization techniques:\n- Quantization (INT8, INT4)\n- Vector instructions (AVX, ARM NEON)\n- Multi-threading and NUMA awareness\n- Memory access optimization\n```\n\n---\n\n## 13.6 Cost Optimization Strategies üí∞\n\n### Understanding Inference Costs\n\n#### Cost Components\n```\nCloud Inference Costs:\n1. Compute: GPU/CPU rental per hour\n2. Memory: VRAM and system RAM usage\n3. Storage: Model weights and cache storage\n4. Network: Data transfer and bandwidth\n5. Request processing: Per-token or per-request pricing\n\nTypical breakdown:\n- Compute: 60-80% of costs\n- Memory: 15-25% of costs  \n- Other: 5-15% of costs\n```\n\n#### Cost per Token Analysis\n```\nExample calculation (GPT-3 class model):\n- GPU cost: $2/hour for A100\n- Throughput: 1000 tokens/second\n- Cost per token: $2/(3600 √ó 1000) = $0.00000056\n\nBut real costs include:\n+ Infrastructure overhead\n+ Load balancing and redundancy\n+ Development and maintenance\n+ Profit margins\n\nActual API costs: ~$0.001-0.01 per 1K tokens\n```\n\n### Optimization Strategies\n\n#### Model Selection Trade-offs\n```\nDecision matrix:\n\nSmall models (1-7B):\n+ Low cost per request\n+ Fast inference\n- Lower quality responses\nUse case: High-volume, simple tasks\n\nMedium models (13-30B):\n+ Good quality/cost balance\n+ Reasonable speed\n¬± Moderate costs\nUse case: General-purpose applications\n\nLarge models (70B+):\n+ Highest quality\n- High cost per request  \n- Slower inference\nUse case: Complex reasoning, premium applications\n```\n\n#### Caching Strategies\n```\nLevels of caching:\n\n1. Response caching:\n   - Cache complete responses for repeated queries\n   - High hit rate for FAQ-type questions\n\n2. Prefix caching:\n   - Cache computation for common prompt prefixes\n   - Useful for chat applications with system prompts\n\n3. KV cache optimization:\n   - Share KV cache across similar requests\n   - Reduce redundant computation\n\nExample savings: 30-70% cost reduction with good cache hit rates! üí∏\n```\n\n#### Request Routing\n```\nSmart routing strategies:\n\n1. Model cascade:\n   - Try small model first\n   - Route to larger model only if needed\n   - Reduces average cost per request\n\n2. Difficulty-based routing:\n   - Simple questions ‚Üí small model\n   - Complex questions ‚Üí large model\n   - Use classifier to determine complexity\n\n3. Quality-based routing:\n   - User tier determines model access\n   - Premium users get large models\n   - Standard users get efficient models\n```\n\n---\n\n## 13.7 Performance Monitoring and Debugging üìä\n\n### Key Metrics to Track\n\n#### Latency Metrics\n```\nImportant measurements:\n- Time to First Token (TTFT): How quickly generation starts\n- Inter-Token Latency: Time between subsequent tokens\n- End-to-End Latency: Total request processing time\n- Queue Time: Time waiting for processing\n\nTargets:\n- TTFT: < 500ms for interactive applications\n- Inter-token: < 50ms for smooth streaming\n- E2E: < 5s for most applications\n```\n\n#### Throughput Metrics\n```\nCapacity measurements:\n- Requests per second (RPS)\n- Tokens per second (TPS)\n- Concurrent users supported\n- GPU utilization percentage\n\nOptimization goal: Maximize throughput while maintaining latency targets\n```\n\n#### Quality Metrics\n```\nResponse quality tracking:\n- User satisfaction scores\n- Task completion rates\n- Error rates and failure modes\n- A/B testing results\n\nBalance: Don't sacrifice quality for speed!\n```\n\n### Profiling and Optimization\n\n#### GPU Profiling Tools\n```\nEssential tools:\n- NVIDIA Nsight: Comprehensive GPU profiling\n- nvprof/ncu: Command-line profiling\n- PyTorch Profiler: Framework-integrated profiling\n- TensorBoard: Visualization and analysis\n\nKey metrics to watch:\n- Kernel execution time\n- Memory bandwidth utilization\n- Occupancy rates\n- Memory access patterns\n```\n\n#### Common Performance Issues\n```\nBottleneck identification:\n\nMemory-bound:\n- Low compute utilization\n- High memory bandwidth usage\n- Solution: Reduce memory access, increase batch size\n\nCompute-bound:\n- High GPU utilization\n- Low memory bandwidth\n- Solution: Optimize kernels, reduce precision\n\nI/O bound:\n- Low GPU utilization overall\n- High queue times\n- Solution: Improve data loading, increase parallelism\n```\n\n---\n\n## 13.8 Real-World Optimization Case Studies üåç\n\n### Case Study 1: ChatGPT Optimization\n\n#### Scaling Challenges\n```\nOpenAI's journey:\n- Initial launch: Frequent overload and long response times\n- Peak usage: Millions of concurrent users\n- Response time target: < 2 seconds\n\nOptimization strategies:\n‚úÖ Model size optimization (different models for different use cases)\n‚úÖ Advanced batching and request routing\n‚úÖ Geographic distribution of inference clusters\n‚úÖ Aggressive caching and preprocessing\n‚úÖ Custom silicon development (rumored)\n\nResults: Maintained quality while scaling 100x+ ‚ö°\n```\n\n### Case Study 2: Code Generation Optimization\n\n#### GitHub Copilot's Approach\n```\nUnique challenges:\n- Real-time code completion (< 100ms latency)\n- Context-aware suggestions\n- High accuracy requirements\n- Massive scale (millions of developers)\n\nSolutions:\n‚úÖ Specialized smaller models for different languages\n‚úÖ Prefix caching for common code patterns  \n‚úÖ Edge deployment for low latency\n‚úÖ Incremental inference for completion\n\nTrade-offs: Multiple specialized models vs. one large general model\n```\n\n### Case Study 3: Mobile Deployment\n\n#### On-Device LLM Optimization\n```\nConstraints:\n- Limited memory (4-8GB total)\n- Battery life considerations  \n- No internet dependency\n- Acceptable performance on mobile CPUs\n\nTechniques used:\n‚úÖ Aggressive quantization (4-bit, 3-bit)\n‚úÖ Knowledge distillation to very small models\n‚úÖ Pruning and sparsity\n‚úÖ Mobile-optimized frameworks (Core ML, TensorFlow Lite)\n\nResults: 1-3B parameter models running on phones! üì±\n```\n\n---\n\n## Key Takeaways üéØ\n\n1. **Inference optimization is crucial for production** - raw models are too slow and expensive for real-world use\n\n2. **Memory bandwidth is often the bottleneck** - moving data costs more than computing with it\n\n3. **Multiple compression techniques can be combined** - quantization + pruning + distillation for maximum efficiency\n\n4. **Specialized serving frameworks matter** - vLLM, TensorRT-LLM provide significant speedups over naive implementations\n\n5. **Hardware choice impacts performance significantly** - match workload characteristics to hardware strengths\n\n6. **Cost optimization requires holistic thinking** - consider model selection, caching, routing, and scaling together\n\n7. **Monitoring and profiling are essential** - you can't optimize what you don't measure\n\n---\n\n## Fun Exercises üéÆ\n\n### Exercise 1: Optimization Strategy Design\n```\nYou need to deploy a customer service chatbot with these requirements:\n- < 1 second response time\n- Handle 1000 concurrent users\n- Budget: $1000/month\n- Quality: Must handle 90% of questions correctly\n\nDesign your optimization strategy:\n1. What model size would you choose?\n2. What compression techniques would you apply?\n3. What serving framework and hardware?\n4. How would you implement caching?\n```\n\n### Exercise 2: Quantization Analysis\n```\nA 7B parameter model uses:\n- FP16: 14GB memory\n- INT8: 7GB memory  \n- INT4: 3.5GB memory\n\nIf your GPU has 16GB memory and you need 4GB for KV cache:\n1. Which quantization levels fit?\n2. How would batch size change with each?\n3. What quality vs. speed trade-offs would you expect?\n```\n\n### Exercise 3: Cost Optimization\n```\nCalculate the cost savings:\n\nBaseline: Large model, $0.02 per 1K tokens\nOptimizations available:\n- Smaller model: 50% cost, 10% quality loss\n- Caching: 40% fewer requests to model\n- Request routing: 30% requests to small model\n\nWhat's the total cost reduction?\nWhen might this optimization not be worth it?\n```\n\n---\n\n## What's Next? üìö\n\nIn Chapter 14, we'll explore production deployment - taking optimized models and building robust, scalable systems!\n\n**Preview:** We'll learn about:\n- Infrastructure design and architecture patterns\n- Monitoring, logging, and observability\n- A/B testing and gradual rollouts\n- Security, privacy, and compliance considerations\n\nFrom fast models to bulletproof systems! üõ°Ô∏èüèóÔ∏è\n\n---\n\n## Final Thought üí≠\n\n```\n\"Optimization is where the magic of LLMs meets the reality of production:\n- Amazing research models become practical applications\n- Theoretical capabilities become accessible to millions\n- Expensive experiments become cost-effective services\n- Impressive demos become reliable products\n\nThe goal isn't just to make it work - it's to make it work well,\nfast, affordably, and at scale. That's the art of ML engineering!\" üé®‚öôÔ∏è\n```\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"highlight-style":"github","css":["custom.css"],"output-file":"Chapter_13_Optimization_Inference.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":{"light":"flatly","dark":"darkly"},"code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}