{"title":"Chapter 3: Natural Language Processing Fundamentals","markdown":{"headingText":"Chapter 3: Natural Language Processing Fundamentals","containsRefs":false,"markdown":"*From Words to Vectors: The Journey Begins*\n\n## What We'll Learn Today üéØ\n- How computers \"chop up\" text (tokenization) \n- Turning words into numbers that computers understand (embeddings)\n- Teaching computers to predict language (language modeling)\n- How to measure if our model is actually good (evaluation)\n\n**Big Idea:** Computers don't understand words - they only understand numbers. So we need to convert language into math!\n\n---\n\n## 3.1 Tokenization: Chopping Up Text üî™\n\n### The Fundamental Challenge\n\n#### The Problem\n```\nHumans read: \"I love machine learning!\"\nComputers see: A string of meaningless characters\n\nWe need to break text into meaningful pieces (tokens)\n```\n\n#### What is a Token?\n```\nThink of tokens as \"LEGO blocks\" of language:\n- Each block represents a meaningful unit\n- We can build sentences by combining blocks\n- The key question: How big should each block be?\n```\n\n### Method 1: Word-Level Tokenization üìù\n\n#### How It Works\n```\nInput: \"I love cats and dogs\"\nOutput: [\"I\", \"love\", \"cats\", \"and\", \"dogs\"]\n\nSimple rule: Split on spaces and punctuation\n```\n\n#### Analogy: Cutting a Sentence Like a Pizza üçï\n```\nImagine each word is a pizza slice:\n- Easy to understand what each slice represents\n- Each slice has clear meaning\n- You can eat (process) each slice independently\n```\n\n#### Advantages ‚úÖ\n- **Intuitive:** Each token has clear meaning\n- **Interpretable:** Easy for humans to understand  \n- **Semantic preservation:** Meaning of words is kept intact\n\n#### Problems ‚ùå\n\n**Problem 1: Vocabulary Explosion**\n```\nEnglish has ~170,000 words in current use\nAdd proper nouns, technical terms, slang...\nResult: HUGE vocabulary = HUGE memory requirements\n```\n\n**Problem 2: Out-of-Vocabulary (OOV) Words**\n```\nTraining data: \"I like cats\"\nNew text: \"I like GPU\" \nModel: \"What the heck is a GPU??\" ü§î\n```\n\n**Problem 3: Morphological Variants**\n```\nThe model treats these as completely different:\n- \"run\", \"running\", \"runs\", \"ran\"\n- But they're clearly related!\n```\n\n### Method 2: Character-Level Tokenization üî§\n\n#### How It Works\n```\nInput: \"I love cats\"\nOutput: [\"I\", \" \", \"l\", \"o\", \"v\", \"e\", \" \", \"c\", \"a\", \"t\", \"s\"]\n```\n\n#### Analogy: Reading Letter by Letter üìö\n```\nLike reading a book one letter at a time:\n- You never encounter an \"unknown\" letter\n- But it takes forever to get to the meaning\n- Hard to understand what's going on\n```\n\n#### Advantages ‚úÖ\n- **No OOV problem:** Fixed, small vocabulary (26 letters + punctuation)\n- **Language agnostic:** Works for any language\n- **Handles typos:** Can process any character combination\n\n#### Problems ‚ùå\n- **Very long sequences:** \"Hello\" becomes 5 tokens instead of 1\n- **Lost semantics:** Hard to capture word-level meaning\n- **Computational cost:** Much longer sequences to process\n\n### Method 3: Subword Tokenization (The Sweet Spot!) üéØ\n\n#### The Big Idea\n```\n\"What if we could have the best of both worlds?\"\n- Keep common words as single tokens (like word-level)\n- Break rare words into smaller pieces (like character-level)\n```\n\n#### Analogy: Smart Text Compression üì¶\n```\nThink of a smart compression algorithm:\n- Frequent patterns get short codes\n- Rare patterns get longer codes\n- But everything can still be represented!\n```\n\n### Byte Pair Encoding (BPE): The Most Popular Method\n\n#### The Algorithm (Step by Step)\n\n**Step 1: Start with characters**\n```\nText: \"low lower lowest\"\nInitial vocab: {\"l\", \"o\", \"w\", \"e\", \"r\", \"s\", \"t\", \"</w>\"}\nNote: </w> marks word endings\n```\n\n**Step 2: Count all adjacent pairs**\n```\nText: \"l o w </w> l o w e r </w> l o w e s t </w>\"\n\nPair counts:\n- \"l o\": 3 times\n- \"o w\": 3 times  \n- \"w e\": 2 times\n- \"e r\": 1 time\n- ... and so on\n```\n\n**Step 3: Merge the most frequent pair**\n```\nMost frequent: \"l o\" (appears 3 times)\nMerge: \"l o\" ‚Üí \"lo\"\n\nNew text: \"lo w </w> lo w e r </w> lo w e s t </w>\"\nAdd \"lo\" to vocabulary\n```\n\n**Step 4: Repeat until desired vocabulary size**\n```\nNext iteration: \"o w\" is most frequent\nMerge: \"o w\" ‚Üí \"ow\"\nResult: \"low </w> low e r </w> low e s t </w>\"\n\nContinue until we have enough tokens...\n```\n\n**Final Result:**\n```\nVocabulary: {\"l\", \"o\", \"w\", \"e\", \"r\", \"s\", \"t\", \"</w>\", \"lo\", \"ow\", \"low\", \"er\", \"est\"}\nText: [\"low</w>\", \"low\", \"er</w>\", \"low\", \"est</w>\"]\n```\n\n#### Why BPE is Brilliant üß†\n```\n‚úÖ Common words stay together: \"the\", \"and\", \"because\"\n‚úÖ Rare words get broken down: \"antidisestablishmentarianism\" \n‚úÖ No OOV problem: Any new word can be broken into known subwords\n‚úÖ Balances vocabulary size vs sequence length\n```\n\n### Other Subword Methods\n\n#### WordPiece (Used by BERT)\n```\nSimilar to BPE but:\n- Uses likelihood-based scoring instead of frequency\n- Adds \"##\" prefix for continuation subwords\n- Example: \"playing\" ‚Üí [\"play\", \"##ing\"]\n```\n\n#### SentencePiece (Used by T5, many modern models)\n```\nKey innovations:\n- Treats spaces as regular characters\n- No pre-tokenization required\n- Reversible: can perfectly reconstruct original text\n- Language-independent\n```\n\n### Practical Example: How GPT Tokenizes\n\n```\nInput: \"Hello, how are you today?\"\n\nGPT tokenization:\n[\"Hello\", \",\", \" how\", \" are\", \" you\", \" today\", \"?\"]\n\nNotice:\n- Spaces are included with words (\" how\")  \n- Punctuation often separate tokens\n- Common words stay together\n```\n\n### Choosing Vocabulary Size: The Trade-off\n\n```\nSmall vocabulary (1K tokens):\n‚úÖ Less memory \n‚ùå Longer sequences\n‚ùå Less semantic preservation\n\nLarge vocabulary (100K tokens):  \n‚úÖ Shorter sequences\n‚úÖ Better semantic preservation\n‚ùå More memory\n‚ùå Sparse training signal\n\nSweet spot: 30K-50K tokens for most models\n```\n\n---\n\n## 3.2 Word Embeddings: From Words to Vectors üî¢\n\n### The Core Problem\n\n#### How Do We Represent Words to Computers?\n```\nHumans: \"cat\" and \"dog\" are similar (both animals)\nComputer: \"cat\" = ? and \"dog\" = ?\n\nWe need a way to represent words as numbers that capture meaning!\n```\n\n### Attempt 1: One-Hot Encoding (The Naive Approach)\n\n#### How It Works\n```\nVocabulary: [\"cat\", \"dog\", \"bird\", \"car\"]\n\n\"cat\"  = [1, 0, 0, 0]\n\"dog\"  = [0, 1, 0, 0]  \n\"bird\" = [0, 0, 1, 0]\n\"car\"  = [0, 0, 0, 1]\n```\n\n#### Problems with One-Hot\n```\n‚ùå No semantic similarity:\n   - Distance between \"cat\" and \"dog\" = distance between \"cat\" and \"car\"\n   - Computer can't tell that cat and dog are both animals!\n\n‚ùå Huge vectors:\n   - 50K vocabulary = 50K dimensional vectors\n   - Mostly zeros (sparse and inefficient)\n```\n\n### The Distributional Hypothesis üí°\n\n#### The Key Insight\n```\n\"You shall know a word by the company it keeps\" - J.R. Firth\n\nWords that appear in similar contexts have similar meanings.\n```\n\n#### Examples\n```\nContext: \"The ___ is sleeping on the couch\"\nLikely words: cat, dog, baby, person\n‚Üí These words are similar!\n\nContext: \"The ___ flew over the trees\"  \nLikely words: bird, plane, helicopter\n‚Üí These words are similar!\n```\n\n### Word2Vec: The Breakthrough\n\n#### Two Flavors: CBOW vs Skip-gram\n\n**CBOW (Continuous Bag of Words):**\n```\nGiven context words ‚Üí Predict center word\n\nInput: [\"the\", \"cat\", \"on\", \"the\"]\nTarget: \"sat\"\n\nThink: \"Given these surrounding words, what's the missing word?\"\n```\n\n**Skip-gram:**\n```\nGiven center word ‚Üí Predict context words\n\nInput: \"sat\"  \nTarget: [\"the\", \"cat\", \"on\", \"the\"]\n\nThink: \"Given this word, what words should appear around it?\"\n```\n\n#### Skip-gram in Detail (More Popular)\n\n**Step 1: Set up the problem**\n```\nSentence: \"The cat sat on the mat\"\nWindow size: 2\n\nFor word \"sat\":\nContext: [\"The\", \"cat\", \"on\", \"the\"]\n```\n\n**Step 2: The neural network**\n```\nInput: One-hot vector for \"sat\"\n‚Üì\nHidden layer: Dense vector (embedding!)\n‚Üì\nOutput: Probability distribution over all words\n\nGoal: High probability for context words, low for others\n```\n\n**Step 3: Training**\n```\nFor each (center, context) pair:\n1. Forward pass: Get predicted probabilities\n2. Calculate loss: How wrong were we?\n3. Backpropagation: Update embeddings\n4. Repeat millions of times!\n```\n\n#### The Magic Result ü™Ñ\n```\nAfter training, similar words have similar embeddings:\n\n\"king\" ‚âà [0.2, 0.8, -0.1, 0.5, ...]\n\"queen\" ‚âà [0.3, 0.7, -0.2, 0.4, ...]\n\"car\" ‚âà [-0.1, 0.1, 0.9, -0.3, ...]\n\nDistance between king and queen < Distance between king and car!\n```\n\n#### Famous Word2Vec Results\n```\nVector arithmetic that actually works:\n\"king\" - \"man\" + \"woman\" ‚âà \"queen\"\n\"Paris\" - \"France\" + \"Italy\" ‚âà \"Rome\"\n\"walking\" - \"walk\" + \"swim\" ‚âà \"swimming\"\n\nThis blew everyone's minds! ü§Ø\n```\n\n### GloVe: Global Vectors\n\n#### The Motivation\n```\nWord2Vec problem: Only uses local context windows\nGloVe idea: Use global corpus statistics!\n```\n\n#### How GloVe Works\n```\n1. Build global co-occurrence matrix\n   - Count how often word i appears with word j\n   \n2. Optimize objective function:\n   - Want: dot product of embeddings ‚âà log(co-occurrence count)\n   - With weights to handle rare/frequent words\n   \n3. Result: Embeddings that capture global statistics\n```\n\n#### GloVe vs Word2Vec\n```\nWord2Vec: Local context, prediction-based\nGloVe: Global statistics, count-based\nPerformance: Similar, but GloVe often faster to train\n```\n\n### FastText: Handling Rare Words\n\n#### The Innovation\n```\nProblem: Word2Vec can't handle words not seen in training\nSolution: Represent words as sum of character n-grams!\n```\n\n#### Example\n```\nWord: \"where\"\nCharacter 3-grams: [\"<wh\", \"whe\", \"her\", \"ere\", \"re>\"]\nPlus the full word: \"where\"\n\nFinal embedding = sum of all n-gram embeddings\n```\n\n#### Benefits\n```\n‚úÖ Can handle OOV words (break into n-grams)\n‚úÖ Captures morphological relationships\n‚úÖ Works great for morphologically rich languages\n‚úÖ Same speed as Word2Vec\n```\n\n### Evaluating Word Embeddings\n\n#### Intrinsic Evaluation\n\n**Word Similarity Tasks:**\n```\nHuman judgment: \"cat\" and \"dog\" similarity = 7/10\nModel similarity: cosine(cat_vector, dog_vector) = 0.73\nCorrelation: How well do model scores match human scores?\n```\n\n**Analogy Tasks:**\n```\nQuestion: \"man\" : \"king\" :: \"woman\" : ?\nMethod: king - man + woman = ?\nCorrect if closest word is \"queen\"\n```\n\n#### Extrinsic Evaluation\n```\nUse embeddings in downstream tasks:\n- Sentiment analysis\n- Named entity recognition  \n- Text classification\n\nBetter embeddings ‚Üí Better downstream performance\n```\n\n---\n\n## 3.3 Language Modeling: Predicting What Comes Next üîÆ\n\n### What is Language Modeling?\n\n#### The Core Task\n```\nGiven: \"The cat sat on the\"\nPredict: \"mat\" (most likely), \"floor\", \"chair\", etc.\n\nLanguage model assigns probabilities to sequences of words\n```\n\n#### Why This Matters\n```\nIf you can predict the next word well:\n‚úÖ You understand grammar\n‚úÖ You understand semantics  \n‚úÖ You understand context\n‚úÖ You can generate coherent text!\n\nThis is the foundation of GPT, ChatGPT, and all modern LLMs!\n```\n\n### N-gram Language Models (The Classical Approach)\n\n#### The Markov Assumption\n```\nAssumption: Next word depends only on previous N words\nP(word | entire history) ‚âà P(word | previous N words)\n```\n\n#### Bigram Model (N=2)\n```\nP(word_i | word_1, ..., word_{i-1}) ‚âà P(word_i | word_{i-1})\n\nExample:\nP(\"cat\" | \"The\") = Count(\"The cat\") / Count(\"The\")\n\nIf we saw \"The cat\" 100 times and \"The\" 1000 times:\nP(\"cat\" | \"The\") = 100/1000 = 0.1\n```\n\n#### Training N-gram Models\n```\n1. Count all N-gram occurrences in training data\n2. Estimate probabilities using counts\n3. Apply smoothing for unseen N-grams\n```\n\n#### Problems with N-gram Models\n```\n‚ùå Can't capture long-range dependencies\n‚ùå Sparse data problem (many N-grams never seen)\n‚ùå No semantic understanding\n‚ùå Exponential parameter growth with N\n```\n\n### Neural Language Models: The Revolution\n\n#### The Big Idea\n```\nInstead of counting N-grams:\nUse neural networks to predict next word!\n\nBenefits:\n‚úÖ Distributed representations\n‚úÖ Automatic feature learning\n‚úÖ Better generalization\n‚úÖ Can handle longer contexts\n```\n\n#### Feed-forward Neural Language Model\n\n**Architecture:**\n```\nInput: Previous N words\n‚Üì\nEmbedding layer: Convert words to vectors\n‚Üì  \nConcatenate: Combine all word vectors\n‚Üì\nHidden layers: Learn complex patterns\n‚Üì\nOutput layer: Probability distribution over vocabulary\n```\n\n**Example:**\n```\nContext: \"The cat sat\"\nWord embeddings: [e_the, e_cat, e_sat]\nConcatenated: [e_the || e_cat || e_sat]  \nHidden layer: Learn patterns like \"animals sit on things\"\nOutput: P(next_word = \"on\") = 0.8\n```\n\n#### Recurrent Neural Language Models\n\n**The Innovation: Memory!**\n```\nProblem with feed-forward: Fixed context window\nSolution: RNN can theoretically handle unlimited context!\n```\n\n**How RNNs Work:**\n```\nhidden_0 = initial_state\nfor each word in sequence:\n    hidden_i = RNN(word_i, hidden_{i-1})\n    predict_next = output_layer(hidden_i)\n```\n\n**Benefits:**\n```\n‚úÖ Variable-length sequences\n‚úÖ Shared parameters across positions  \n‚úÖ Theoretical unlimited memory\n‚úÖ Sequential processing matches language nature\n```\n\n**Problems:**\n```\n‚ùå Vanishing gradients (forgets long-term info)\n‚ùå Sequential processing (can't parallelize)\n‚ùå Still struggles with very long dependencies\n```\n\n### Modern Language Models: Transformers\n\n#### The Transformer Revolution\n```\nKey insight: We don't need recurrence!\nSelf-attention can capture all relationships directly!\n\nEvery word can directly \"talk\" to every other word\nNo more forgetting long-term dependencies!\n```\n\n#### Autoregressive Generation\n```\nHow GPT generates text:\n\n1. Start with prompt: \"The cat\"\n2. Predict next word: P(sat | The cat) ‚Üí \"sat\"  \n3. Add to sequence: \"The cat sat\"\n4. Predict next: P(on | The cat sat) ‚Üí \"on\"\n5. Continue: \"The cat sat on the mat\"\n```\n\n---\n\n## 3.4 Evaluation Metrics: How Good is Our Model? üìä\n\n### Intrinsic Evaluation: Perplexity\n\n#### What is Perplexity?\n```\nPerplexity measures: \"How surprised is the model by the actual text?\"\n\nLow perplexity = Model predicts text well\nHigh perplexity = Model is confused by the text\n```\n\n#### Mathematical Definition\n```\nPerplexity = 2^(-average log probability)\n\nExample:\nIf model assigns probability 0.25 to each word:\nPerplexity = 1/0.25 = 4\n\nInterpretation: \"Model is as confused as random choice among 4 options\"\n```\n\n#### Intuitive Example\n```\nText: \"The cat sat on the mat\"\nGood model: P = [0.9, 0.8, 0.7, 0.9, 0.8, 0.9]\nBad model: P = [0.1, 0.2, 0.3, 0.1, 0.2, 0.1]\n\nGood model ‚Üí Low perplexity\nBad model ‚Üí High perplexity\n```\n\n### Extrinsic Evaluation: Downstream Tasks\n\n#### Text Generation Quality\n\n**BLEU Score (for Translation):**\n```\nMeasures N-gram overlap between generated and reference text\n\nExample:\nReference: \"The cat is sleeping\"\nGenerated: \"The cat sleeps\"  \nBLEU considers: How many 1-grams, 2-grams, etc. match?\n```\n\n**Problems with BLEU:**\n```\n‚ùå Only surface-level matching\n‚ùå Doesn't understand semantic equivalence\n‚ùå \"The cat sleeps\" vs \"The feline rests\" = low BLEU but same meaning!\n```\n\n**ROUGE (for Summarization):**\n```\nSimilar to BLEU but focuses on recall\n\"Did the summary include the important content?\"\n```\n\n**BERTScore (Modern Approach):**\n```\nUses embeddings to measure semantic similarity\nCan recognize that \"cat\" and \"feline\" are similar!\nMuch better correlation with human judgment\n```\n\n#### Human Evaluation\n\n**What Humans Judge:**\n```\n1. Fluency: Does the text sound natural?\n2. Coherence: Does it make logical sense?\n3. Factual Accuracy: Are the facts correct?\n4. Relevance: Does it answer the question?\n```\n\n**Challenges:**\n```\n‚ùå Expensive and time-consuming\n‚ùå Subjective (humans disagree!)\n‚ùå Hard to scale\n‚ùå But most reliable for quality assessment\n```\n\n### The Evaluation Challenge\n\n#### Why Evaluation is Hard\n```\nLanguage is:\n- Subjective (multiple good answers)\n- Contextual (meaning depends on situation)  \n- Creative (novel combinations are good!)\n- Nuanced (subtle differences matter)\n\nHow do you measure creativity and understanding? ü§î\n```\n\n#### Current Best Practices\n```\n1. Use multiple metrics (no single metric is perfect)\n2. Include human evaluation for final assessment\n3. Task-specific metrics when possible\n4. Consider both automatic and human metrics\n5. Be aware of metric limitations\n```\n\n---\n\n## Putting It All Together: The NLP Pipeline üîÑ\n\n### From Raw Text to Model Predictions\n\n#### Step 1: Text Preprocessing\n```\nRaw text: \"Hello world! How are you today???\"\nCleaned: \"Hello world! How are you today?\"\nNormalized: Remove excessive punctuation, fix encoding\n```\n\n#### Step 2: Tokenization  \n```\nText: \"Hello world! How are you today?\"\nTokens: [\"Hello\", \" world\", \"!\", \" How\", \" are\", \" you\", \" today\", \"?\"]\n```\n\n#### Step 3: Vocabulary Creation\n```\nCollect all unique tokens from training data\nAdd special tokens: [PAD], [UNK], [BOS], [EOS]\nFinal vocabulary size: ~30K-50K tokens\n```\n\n#### Step 4: Embedding\n```\nEach token gets mapped to dense vector:\n\"Hello\" ‚Üí [0.2, -0.1, 0.8, 0.3, ...]\n\" world\" ‚Üí [0.1, 0.5, -0.2, 0.7, ...]\n```\n\n#### Step 5: Model Processing\n```\nEmbeddings ‚Üí Transformer layers ‚Üí Output probabilities\n```\n\n#### Step 6: Decoding\n```\nProbabilities ‚Üí Sample next token ‚Üí Convert back to text\n```\n\n---\n\n## Common Student Questions üôã‚Äç‚ôÄÔ∏è\n\n### Q: \"Why do we need so many different tokenization methods?\"\n**A:** Different methods work better for different languages and tasks. It's like having different tools for different jobs!\n\n### Q: \"How do embeddings actually capture meaning?\"\n**A:** Through training! Words that appear in similar contexts get similar embeddings. The model learns that \"king\" and \"queen\" are similar because they appear in similar situations.\n\n### Q: \"Why is language modeling so important?\"\n**A:** If you can predict what comes next in language really well, you understand grammar, semantics, context, and world knowledge. It's a powerful general task!\n\n### Q: \"Which evaluation metric should I use?\"\n**A:** Depends on your task! For generation: BLEU/ROUGE + human eval. For understanding: task-specific accuracy. Always use multiple metrics!\n\n---\n\n## Key Takeaways üéØ\n\n1. **Tokenization** is crucial - it determines how your model sees language. Subword methods like BPE are usually best.\n\n2. **Word embeddings** capture semantic similarity by mapping words to dense vectors based on distributional similarity.\n\n3. **Language modeling** (predicting next words) is a powerful task that teaches models about language structure and meaning.\n\n4. **Evaluation** is challenging but essential - use multiple metrics and include human judgment when possible.\n\n5. **The NLP pipeline** connects all these pieces to transform raw text into model understanding.\n\n---\n\n## Fun Exercises üéÆ\n\n### Exercise 1: Tokenization Practice\n```\nTry tokenizing this sentence with different methods:\n\"The AI model's performance was extraordinary!\"\n\nWord-level: ?\nCharacter-level: ?  \nSubword (guess): ?\n```\n\n### Exercise 2: Embedding Intuition\n```\nWhich pairs should have similar embeddings?\na) \"king\" and \"queen\"\nb) \"king\" and \"car\"  \nc) \"happy\" and \"joyful\"\nd) \"run\" and \"running\"\n```\n\n### Exercise 3: Language Modeling\n```\nWhat should come next?\n\"The weather today is very ___\"\n\nWhat factors affect this prediction?\n```\n\n---\n\n## What's Next? üìö\n\nIn Chapter 4, we'll dive deep into the Transformer architecture - the breakthrough that made modern LLMs possible!\n\n**Preview:** We'll learn about:\n- The attention mechanism (how models \"focus\")\n- Self-attention (how words relate to each other)\n- The complete transformer architecture\n- Why this was such a revolutionary breakthrough\n\nGet ready to understand the engine that powers ChatGPT! üöÄ\n\n---\n\n## Final Thought üí≠\n\n```\n\"The best way to understand language models is to remember: \nthey're pattern matching machines that learned patterns from massive amounts of text.\nThe patterns they learned happen to correspond to grammar, meaning, and knowledge!\"\n\nThat's both amazing and important to remember for their limitations! üòä\n```\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"output-file":"Chapter_03_NLP_Fundamentals.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":"cosmo","code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}